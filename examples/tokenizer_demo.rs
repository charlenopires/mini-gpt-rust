//! # ğŸ”¤ DemonstraÃ§Ã£o do Sistema de TokenizaÃ§Ã£o BPE
//!
//! Este exemplo demonstra como o algoritmo Byte Pair Encoding (BPE) funciona na prÃ¡tica,
//! desde o treinamento atÃ© a tokenizaÃ§Ã£o e decodificaÃ§Ã£o de texto.
//!
//! ## ğŸ¯ O que vocÃª vai aprender:
//! - Como treinar um tokenizador BPE do zero
//! - VisualizaÃ§Ã£o do processo de merge de pares
//! - ComparaÃ§Ã£o entre diferentes estratÃ©gias de tokenizaÃ§Ã£o
//! - AnÃ¡lise de eficiÃªncia e compressÃ£o
//! - Tratamento de texto multilÃ­ngue

use std::collections::HashMap;
use std::time::Instant;
use anyhow::Result;

// Estrutura simplificada do BPE Tokenizer para demonstraÃ§Ã£o
// Em um projeto real, esta viria de mini_gpt_rust::tokenizer
#[derive(Debug, Clone)]
struct BPETokenizer {
    vocab: HashMap<String, usize>,
    reverse_vocab: HashMap<usize, String>,
    merges: Vec<(String, String)>,
    vocab_size: usize,
}

impl BPETokenizer {
    /// ğŸ—ï¸ Cria um novo tokenizador BPE
    fn new(vocab_size: usize) -> Result<Self> {
        let mut vocab = HashMap::new();
        let mut reverse_vocab = HashMap::new();
        
        // Adiciona tokens especiais
        vocab.insert("<unk>".to_string(), 0);
        vocab.insert("<pad>".to_string(), 1);
        vocab.insert("<eos>".to_string(), 2);
        vocab.insert("<bos>".to_string(), 3);
        
        reverse_vocab.insert(0, "<unk>".to_string());
        reverse_vocab.insert(1, "<pad>".to_string());
        reverse_vocab.insert(2, "<eos>".to_string());
        reverse_vocab.insert(3, "<bos>".to_string());
        
        // Adiciona caracteres ASCII bÃ¡sicos
        let mut next_id = 4;
        for i in 32..127 {
            let ch = char::from(i as u8).to_string();
            vocab.insert(ch.clone(), next_id);
            reverse_vocab.insert(next_id, ch);
            next_id += 1;
        }
        
        Ok(Self {
            vocab,
            reverse_vocab,
            merges: Vec::new(),
            vocab_size,
        })
    }
    
    /// ğŸ“ Treina o tokenizador com texto fornecido
    fn train(&mut self, text: &str) -> Result<()> {
        println!("ğŸ“ Iniciando treinamento do tokenizador BPE...");
        
        // ObtÃ©m frequÃªncias de palavras
        let mut word_freqs = self.get_word_frequencies(text);
        println!("ğŸ“Š Encontradas {} palavras Ãºnicas", word_freqs.len());
        
        let target_merges = self.vocab_size - self.vocab.len();
        println!("ğŸ¯ Realizando {} merges para atingir vocabulÃ¡rio de {}", target_merges, self.vocab_size);
        
        for merge_step in 0..target_merges {
            // Encontra o par mais frequente
            let pair_freqs = self.get_pair_frequencies(&word_freqs);
            
            if pair_freqs.is_empty() {
                println!("âš ï¸  NÃ£o hÃ¡ mais pares para merge. Parando no passo {}", merge_step);
                break;
            }
            
            let best_pair = pair_freqs.iter()
                .max_by_key(|(_, &freq)| freq)
                .map(|(pair, _)| pair.clone())
                .unwrap();
            
            let freq = pair_freqs[&best_pair];
            
            if merge_step < 10 || merge_step % 100 == 0 {
                println!("ğŸ”„ Merge {}: ('{}', '{}') - frequÃªncia: {}", 
                        merge_step + 1, best_pair.0, best_pair.1, freq);
            }
            
            // Aplica o merge
            word_freqs = self.apply_merge(&word_freqs, &best_pair);
            
            // Adiciona ao vocabulÃ¡rio
            let new_token = format!("{}{}", best_pair.0, best_pair.1);
            let new_id = self.vocab.len();
            self.vocab.insert(new_token.clone(), new_id);
            self.reverse_vocab.insert(new_id, new_token);
            
            // Salva o merge
            self.merges.push(best_pair);
        }
        
        println!("âœ… Treinamento concluÃ­do! VocabulÃ¡rio final: {} tokens", self.vocab.len());
        Ok(())
    }
    
    /// ğŸ“Š ObtÃ©m frequÃªncias de palavras no texto
    fn get_word_frequencies(&self, text: &str) -> HashMap<Vec<String>, usize> {
        let mut word_freqs = HashMap::new();
        
        for word in text.split_whitespace() {
            let chars: Vec<String> = word.chars().map(|c| c.to_string()).collect();
            *word_freqs.entry(chars).or_insert(0) += 1;
        }
        
        word_freqs
    }
    
    /// ğŸ” ObtÃ©m frequÃªncias de pares adjacentes
    fn get_pair_frequencies(&self, word_freqs: &HashMap<Vec<String>, usize>) -> HashMap<(String, String), usize> {
        let mut pair_freqs = HashMap::new();
        
        for (word, &freq) in word_freqs {
            for i in 0..word.len().saturating_sub(1) {
                let pair = (word[i].clone(), word[i + 1].clone());
                *pair_freqs.entry(pair).or_insert(0) += freq;
            }
        }
        
        pair_freqs
    }
    
    /// ğŸ”„ Aplica um merge especÃ­fico
    fn apply_merge(&self, word_freqs: &HashMap<Vec<String>, usize>, pair: &(String, String)) -> HashMap<Vec<String>, usize> {
        let mut new_word_freqs = HashMap::new();
        
        for (word, &freq) in word_freqs {
            let new_word = self.apply_merge_to_word(word.clone(), pair);
            *new_word_freqs.entry(new_word).or_insert(0) += freq;
        }
        
        new_word_freqs
    }
    
    /// ğŸ”§ Aplica merge a uma palavra especÃ­fica
    fn apply_merge_to_word(&self, mut word: Vec<String>, merge: &(String, String)) -> Vec<String> {
        let mut i = 0;
        while i < word.len().saturating_sub(1) {
            if word[i] == merge.0 && word[i + 1] == merge.1 {
                let merged = format!("{}{}", merge.0, merge.1);
                word[i] = merged;
                word.remove(i + 1);
            } else {
                i += 1;
            }
        }
        word
    }
    
    /// ğŸ”¢ Codifica texto em IDs de tokens
    fn encode(&self, text: &str) -> Result<Vec<usize>> {
        let mut tokens = Vec::new();
        
        for word in text.split_whitespace() {
            let mut chars: Vec<String> = word.chars().map(|c| c.to_string()).collect();
            
            // Aplica todos os merges na ordem
            for merge in &self.merges {
                chars = self.apply_merge_to_word(chars, merge);
            }
            
            // Converte para IDs
            for token in chars {
                if let Some(&id) = self.vocab.get(&token) {
                    tokens.push(id);
                } else {
                    tokens.push(0); // <unk>
                }
            }
        }
        
        Ok(tokens)
    }
    
    /// ğŸ”¤ Decodifica IDs de tokens em texto
    fn decode(&self, tokens: &[usize]) -> Result<String> {
        let mut result = String::new();
        
        for &token_id in tokens {
            if let Some(token) = self.reverse_vocab.get(&token_id) {
                result.push_str(token);
            } else {
                result.push_str("<unk>");
            }
        }
        
        Ok(result)
    }
    
    /// ğŸ“ Retorna tamanho do vocabulÃ¡rio
    fn vocab_size(&self) -> usize {
        self.vocab.len()
    }
}

/// ğŸ­ Estrutura para demonstrar conceitos de tokenizaÃ§Ã£o
struct TokenizerDemo {
    tokenizer: BPETokenizer,
}

impl TokenizerDemo {
    /// ğŸ—ï¸ Cria uma nova instÃ¢ncia de demonstraÃ§Ã£o
    fn new(vocab_size: usize) -> Result<Self> {
        let tokenizer = BPETokenizer::new(vocab_size)?;
        Ok(Self { tokenizer })
    }
    
    /// ğŸ“š Demonstra treinamento bÃ¡sico com texto simples
    fn demo_basic_training(&mut self) -> Result<()> {
        println!("\nğŸ“š === DEMONSTRAÃ‡ÃƒO DE TREINAMENTO BÃSICO ===");
        
        let training_text = "
            O gato subiu no telhado.
            O gato desceu do telhado.
            O cachorro correu no jardim.
            O cachorro brincou no jardim.
            A crianÃ§a brincou com o gato.
            A crianÃ§a correu com o cachorro.
            O sol brilhou no jardim.
            A lua brilhou no telhado.
        ";
        
        println!("ğŸ“ Texto de treinamento:");
        println!("{}", training_text.trim());
        
        println!("\nğŸ”¢ EstatÃ­sticas do texto:");
        let words: Vec<&str> = training_text.split_whitespace().collect();
        let unique_words: std::collections::HashSet<&str> = words.iter().cloned().collect();
        let chars: Vec<char> = training_text.chars().filter(|c| !c.is_whitespace()).collect();
        let unique_chars: std::collections::HashSet<char> = chars.iter().cloned().collect();
        
        println!("  - Total de palavras: {}", words.len());
        println!("  - Palavras Ãºnicas: {}", unique_words.len());
        println!("  - Total de caracteres: {}", chars.len());
        println!("  - Caracteres Ãºnicos: {}", unique_chars.len());
        
        // Treina o tokenizador
        let start_time = Instant::now();
        self.tokenizer.train(training_text)?;
        let training_duration = start_time.elapsed();
        
        println!("\nâ±ï¸  Tempo de treinamento: {:?}", training_duration);
        println!("ğŸ“Š VocabulÃ¡rio final: {} tokens", self.tokenizer.vocab_size());
        
        Ok(())
    }
    
    /// ğŸ” Demonstra processo de tokenizaÃ§Ã£o passo a passo
    fn demo_tokenization_process(&self) -> Result<()> {
        println!("\nğŸ” === PROCESSO DE TOKENIZAÃ‡ÃƒO ===");
        
        let test_sentences = vec![
            "O gato subiu",
            "programaÃ§Ã£o em Rust",
            "inteligÃªncia artificial",
            "tokenizaÃ§Ã£o avanÃ§ada",
        ];
        
        for sentence in test_sentences {
            println!("\nğŸ“ Frase: '{}'", sentence);
            
            // Tokeniza
            let start_time = Instant::now();
            let tokens = self.tokenizer.encode(sentence)?;
            let encoding_duration = start_time.elapsed();
            
            // Decodifica
            let start_time = Instant::now();
            let decoded = self.tokenizer.decode(&tokens)?;
            let decoding_duration = start_time.elapsed();
            
            println!("ğŸ”¢ Tokens: {:?}", tokens);
            println!("ğŸ”¤ Decodificado: '{}'", decoded);
            println!("â±ï¸  CodificaÃ§Ã£o: {:?}, DecodificaÃ§Ã£o: {:?}", 
                    encoding_duration, decoding_duration);
            
            // Analisa compressÃ£o
            let original_chars = sentence.len();
            let token_count = tokens.len();
            let compression_ratio = original_chars as f32 / token_count as f32;
            
            println!("ğŸ“Š CompressÃ£o: {} chars â†’ {} tokens (ratio: {:.2}x)", 
                    original_chars, token_count, compression_ratio);
            
            // Mostra tokens individuais
            println!("ğŸ§© Breakdown dos tokens:");
            for (i, &token_id) in tokens.iter().enumerate() {
                if let Some(token_str) = self.tokenizer.reverse_vocab.get(&token_id) {
                    println!("  {}: {} â†’ '{}'", i, token_id, token_str);
                }
            }
        }
        
        Ok(())
    }
    
    /// ğŸ“Š Analisa eficiÃªncia do vocabulÃ¡rio
    fn analyze_vocabulary_efficiency(&self) -> Result<()> {
        println!("\nğŸ“Š === ANÃLISE DE EFICIÃŠNCIA DO VOCABULÃRIO ===");
        
        // Analisa distribuiÃ§Ã£o de comprimentos de tokens
        let mut length_distribution: HashMap<usize, usize> = HashMap::new();
        
        for token in self.tokenizer.reverse_vocab.values() {
            let length = token.chars().count();
            *length_distribution.entry(length).or_insert(0) += 1;
        }
        
        println!("\nğŸ“ DistribuiÃ§Ã£o de comprimentos de tokens:");
        let mut lengths: Vec<_> = length_distribution.keys().cloned().collect();
        lengths.sort();
        
        for length in lengths {
            let count = length_distribution[&length];
            let percentage = count as f32 / self.tokenizer.vocab_size() as f32 * 100.0;
            let bar = "â–ˆ".repeat((percentage / 2.0) as usize);
            println!("  {} chars: {:3} tokens ({:5.1}%) {}", 
                    length, count, percentage, bar);
        }
        
        // Analisa tokens mais comuns por categoria
        self.analyze_token_categories()?;
        
        Ok(())
    }
    
    /// ğŸ·ï¸ Analisa categorias de tokens
    fn analyze_token_categories(&self) -> Result<()> {
        println!("\nğŸ·ï¸ Categorias de tokens:");
        
        let mut categories = HashMap::new();
        
        for token in self.tokenizer.reverse_vocab.values() {
            let category = if token.starts_with('<') && token.ends_with('>') {
                "Especiais"
            } else if token.len() == 1 {
                if token.chars().next().unwrap().is_alphabetic() {
                    "Letras"
                } else if token.chars().next().unwrap().is_numeric() {
                    "NÃºmeros"
                } else {
                    "SÃ­mbolos"
                }
            } else if token.chars().all(|c| c.is_alphabetic()) {
                "Palavras/Subpalavras"
            } else {
                "Mistos"
            };
            
            *categories.entry(category).or_insert(0) += 1;
        }
        
        for (category, count) in categories {
            let percentage = count as f32 / self.tokenizer.vocab_size() as f32 * 100.0;
            println!("  {}: {} tokens ({:.1}%)", category, count, percentage);
        }
        
        Ok(())
    }
    
    /// ğŸŒ Demonstra tokenizaÃ§Ã£o multilÃ­ngue
    fn demo_multilingual_tokenization(&self) -> Result<()> {
        println!("\nğŸŒ === TOKENIZAÃ‡ÃƒO MULTILÃNGUE ===");
        
        let multilingual_texts = vec![
            ("PortuguÃªs", "OlÃ¡, como vocÃª estÃ¡?"),
            ("English", "Hello, how are you?"),
            ("EspaÃ±ol", "Hola, Â¿cÃ³mo estÃ¡s?"),
            ("FranÃ§ais", "Salut, comment allez-vous?"),
        ];
        
        println!("\nğŸ” Comparando tokenizaÃ§Ã£o entre idiomas:");
        println!("Idioma     | Texto                    | Tokens | CompressÃ£o");
        println!("-----------|--------------------------|--------|------------");
        
        for (language, text) in multilingual_texts {
            let tokens = self.tokenizer.encode(text)?;
            let compression = text.len() as f32 / tokens.len() as f32;
            
            println!("{:10} | {:24} | {:6} | {:8.2}x", 
                    language, text, tokens.len(), compression);
        }
        
        println!("\nğŸ’¡ ObservaÃ§Ãµes:");
        println!("  - Idiomas com caracteres especiais podem ter compressÃ£o menor");
        println!("  - BPE se adapta aos padrÃµes mais frequentes no treinamento");
        println!("  - VocabulÃ¡rio maior melhora suporte multilÃ­ngue");
        
        Ok(())
    }
    
    /// âš¡ Benchmark de performance
    fn benchmark_performance(&self) -> Result<()> {
        println!("\nâš¡ === BENCHMARK DE PERFORMANCE ===");
        
        let long_text = "Esta Ã© uma frase muito longa que serÃ¡ repetida vÃ¡rias vezes para simular um texto extenso. ".repeat(10);
        let test_texts = vec![
            ("Curto", "OlÃ¡ mundo"),
            ("MÃ©dio", "Esta Ã© uma frase de tamanho mÃ©dio para testar a performance do tokenizador"),
            ("Longo", long_text.as_str()),
        ];
        
        println!("\nâ±ï¸  Resultados do benchmark:");
        println!("Tamanho | Chars | Tokens | Encode (Î¼s) | Decode (Î¼s) | Throughput");
        println!("--------|-------|--------|-------------|-------------|------------");
        
        for (size_name, text) in test_texts {
            let char_count = text.len();
            
            // Benchmark encoding
            let iterations = 1000;
            let start = Instant::now();
            
            for _ in 0..iterations {
                let _ = self.tokenizer.encode(text)?;
            }
            
            let encode_duration = start.elapsed();
            let avg_encode_micros = encode_duration.as_micros() / iterations;
            
            // Benchmark decoding
            let tokens = self.tokenizer.encode(text)?;
            let start = Instant::now();
            
            for _ in 0..iterations {
                let _ = self.tokenizer.decode(&tokens)?;
            }
            
            let decode_duration = start.elapsed();
            let avg_decode_micros = decode_duration.as_micros() / iterations;
            
            let throughput = char_count as f64 / (avg_encode_micros as f64 / 1_000_000.0);
            
            println!("{:7} | {:5} | {:6} | {:11} | {:11} | {:8.0} chars/s", 
                    size_name, char_count, tokens.len(), 
                    avg_encode_micros, avg_decode_micros, throughput);
        }
        
        Ok(())
    }
}

/// ğŸ¯ ExercÃ­cios prÃ¡ticos para aprofundar o entendimento
struct TokenizerExercises;

impl TokenizerExercises {
    /// ğŸ“ ExercÃ­cio 1: ComparaÃ§Ã£o de estratÃ©gias
    fn exercise_tokenization_strategies() {
        println!("\nğŸ“ === EXERCÃCIO 1: ESTRATÃ‰GIAS DE TOKENIZAÃ‡ÃƒO ===");
        println!("\nğŸ¯ Objetivo: Comparar diferentes abordagens de tokenizaÃ§Ã£o");
        
        println!("\nğŸ” EstratÃ©gias para implementar:");
        
        println!("\n1. ğŸ”¤ TokenizaÃ§Ã£o por Caracteres:");
        println!("   - Cada caractere = 1 token");
        println!("   - Vantagem: VocabulÃ¡rio pequeno");
        println!("   - Desvantagem: SequÃªncias muito longas");
        
        println!("\n2. ğŸ“ TokenizaÃ§Ã£o por Palavras:");
        println!("   - Cada palavra = 1 token");
        println!("   - Vantagem: Preserva significado");
        println!("   - Desvantagem: VocabulÃ¡rio gigante");
        
        println!("\n3. ğŸ§© BPE (Atual):");
        println!("   - Subpalavras baseadas em frequÃªncia");
        println!("   - Vantagem: Balanceado");
        println!("   - Desvantagem: Complexidade de treinamento");
        
        println!("\n4. ğŸ¯ SentencePiece:");
        println!("   - Trata texto como sequÃªncia de bytes");
        println!("   - Vantagem: Independente de idioma");
        println!("   - Desvantagem: Pode quebrar caracteres");
        
        println!("\nğŸ’¡ Experimento sugerido:");
        println!("  1. Implemente tokenizador por caracteres");
        println!("  2. Implemente tokenizador por palavras");
        println!("  3. Compare eficiÃªncia em diferentes tipos de texto");
        println!("  4. Analise trade-offs de cada abordagem");
    }
    
    /// ğŸ”¬ ExercÃ­cio 2: OtimizaÃ§Ã£o de vocabulÃ¡rio
    fn exercise_vocabulary_optimization() {
        println!("\nğŸ”¬ === EXERCÃCIO 2: OTIMIZAÃ‡ÃƒO DE VOCABULÃRIO ===");
        println!("\nğŸ¯ Objetivo: Otimizar tamanho e composiÃ§Ã£o do vocabulÃ¡rio");
        
        println!("\nâš¡ TÃ©cnicas de otimizaÃ§Ã£o:");
        
        println!("\n1. ğŸ“Š AnÃ¡lise de FrequÃªncia:");
        println!("   - Identifique tokens subutilizados");
        println!("   - Remova tokens com frequÃªncia < threshold");
        println!("   - Substitua por decomposiÃ§Ã£o em subtokens");
        
        println!("\n2. ğŸ¯ VocabulÃ¡rio Adaptativo:");
        println!("   - Ajuste vocabulÃ¡rio para domÃ­nio especÃ­fico");
        println!("   - Adicione termos tÃ©cnicos relevantes");
        println!("   - Remova tokens irrelevantes");
        
        println!("\n3. ğŸ”„ Re-treinamento Incremental:");
        println!("   - Atualize vocabulÃ¡rio com novos dados");
        println!("   - Mantenha compatibilidade com modelos existentes");
        println!("   - Use tÃ©cnicas de transfer learning");
        
        println!("\n4. ğŸ“ˆ MÃ©tricas de Qualidade:");
        println!("   - Taxa de compressÃ£o");
        println!("   - Cobertura de vocabulÃ¡rio");
        println!("   - EficiÃªncia de encoding/decoding");
        
        println!("\nğŸ’¡ ImplementaÃ§Ã£o sugerida:");
        println!("  1. Analise distribuiÃ§Ã£o de frequÃªncias");
        println!("  2. Implemente pruning de vocabulÃ¡rio");
        println!("  3. Teste em diferentes domÃ­nios");
        println!("  4. MeÃ§a impacto na qualidade do modelo");
    }
    
    /// ğŸŒ ExercÃ­cio 3: Suporte multilÃ­ngue avanÃ§ado
    fn exercise_multilingual_support() {
        println!("\nğŸŒ === EXERCÃCIO 3: SUPORTE MULTILÃNGUE AVANÃ‡ADO ===");
        println!("\nğŸ¯ Objetivo: Melhorar tokenizaÃ§Ã£o para mÃºltiplos idiomas");
        
        println!("\nğŸ” Desafios multilÃ­ngues:");
        
        println!("\n1. ğŸ“ Scripts Diferentes:");
        println!("   - Latino, CirÃ­lico, Ãrabe, CJK");
        println!("   - DireÃ§Ãµes de escrita diferentes");
        println!("   - Sistemas de pontuaÃ§Ã£o variados");
        
        println!("\n2. ğŸ”¤ Morfologia Complexa:");
        println!("   - AglutinaÃ§Ã£o (Turco, FinlandÃªs)");
        println!("   - FlexÃ£o rica (Russo, AlemÃ£o)");
        println!("   - ComposiÃ§Ã£o (AlemÃ£o, HolandÃªs)");
        
        println!("\n3. ğŸ¯ Balanceamento de Idiomas:");
        println!("   - Evitar bias para idiomas dominantes");
        println!("   - Garantir cobertura adequada");
        println!("   - Otimizar para idiomas de baixo recurso");
        
        println!("\nğŸ’¡ SoluÃ§Ãµes propostas:");
        println!("  1. PrÃ©-processamento especÃ­fico por script");
        println!("  2. VocabulÃ¡rio balanceado por idioma");
        println!("  3. NormalizaÃ§Ã£o Unicode consistente");
        println!("  4. Teste em corpora multilÃ­ngues");
    }
}

/// ğŸš€ FunÃ§Ã£o principal que executa todas as demonstraÃ§Ãµes
fn main() -> Result<()> {
    println!("ğŸ”¤ === DEMONSTRAÃ‡ÃƒO DO SISTEMA DE TOKENIZAÃ‡ÃƒO BPE ===");
    println!("Explorando como texto se torna nÃºmeros que modelos entendem");
    
    let mut demo = TokenizerDemo::new(500)?; // VocabulÃ¡rio de 500 tokens
    
    // Executa demonstraÃ§Ãµes bÃ¡sicas
    demo.demo_basic_training()?;
    demo.demo_tokenization_process()?;
    demo.analyze_vocabulary_efficiency()?;
    demo.demo_multilingual_tokenization()?;
    
    // Benchmark de performance
    demo.benchmark_performance()?;
    
    // ExercÃ­cios educacionais
    println!("\n\nğŸ“ === EXERCÃCIOS PRÃTICOS ===");
    TokenizerExercises::exercise_tokenization_strategies();
    TokenizerExercises::exercise_vocabulary_optimization();
    TokenizerExercises::exercise_multilingual_support();
    
    println!("\n\nâœ… === DEMONSTRAÃ‡ÃƒO CONCLUÃDA ===");
    println!("ğŸ¯ PrÃ³ximos passos:");
    println!("  1. Experimente com diferentes tamanhos de vocabulÃ¡rio");
    println!("  2. Teste com textos de domÃ­nios especÃ­ficos");
    println!("  3. Implemente os exercÃ­cios sugeridos");
    println!("  4. Compare com outros algoritmos de tokenizaÃ§Ã£o");
    
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_tokenizer_creation() {
        let tokenizer = BPETokenizer::new(1000).unwrap();
        assert!(tokenizer.vocab_size() >= 100); // Pelo menos caracteres bÃ¡sicos
    }
    
    #[test]
    fn test_basic_encoding_decoding() {
        let mut tokenizer = BPETokenizer::new(200).unwrap();
        let text = "hello world";
        
        // Treina com texto simples
        tokenizer.train(text).unwrap();
        
        // Testa encoding/decoding
        let tokens = tokenizer.encode(text).unwrap();
        let decoded = tokenizer.decode(&tokens).unwrap();
        
        assert!(!tokens.is_empty());
        assert_eq!(decoded.replace(" ", ""), text.replace(" ", ""));
    }
    
    #[test]
    fn test_special_tokens() {
        let tokenizer = BPETokenizer::new(100).unwrap();
        
        // Verifica se tokens especiais existem
        assert!(tokenizer.vocab.contains_key("<unk>"));
        assert!(tokenizer.vocab.contains_key("<pad>"));
        assert!(tokenizer.vocab.contains_key("<eos>"));
        assert!(tokenizer.vocab.contains_key("<bos>"));
    }
    
    #[test]
    fn test_merge_application() {
        let tokenizer = BPETokenizer::new(100).unwrap();
        let word = vec!["h".to_string(), "e".to_string(), "l".to_string(), "l".to_string(), "o".to_string()];
        let merge = ("l".to_string(), "l".to_string());
        
        let result = tokenizer.apply_merge_to_word(word, &merge);
        assert_eq!(result, vec!["h", "e", "ll", "o"]);
    }
}