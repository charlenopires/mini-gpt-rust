//! # üî§ Demonstra√ß√£o do Sistema de Tokeniza√ß√£o BPE
//!
//! Este exemplo demonstra como o algoritmo Byte Pair Encoding (BPE) funciona na pr√°tica,
//! desde o treinamento at√© a tokeniza√ß√£o e decodifica√ß√£o de texto.
//!
//! ## üéØ O que voc√™ vai aprender:
//! - Como treinar um tokenizador BPE do zero
//! - Visualiza√ß√£o do processo de merge de pares
//! - Compara√ß√£o entre diferentes estrat√©gias de tokeniza√ß√£o
//! - An√°lise de efici√™ncia e compress√£o
//! - Tratamento de texto multil√≠ngue

use std::collections::HashMap;
use std::time::Instant;
use anyhow::Result;

// Estrutura simplificada do BPE Tokenizer para demonstra√ß√£o
// Em um projeto real, esta viria de mini_gpt_rust::tokenizer
#[derive(Debug, Clone)]
struct BPETokenizer {
    vocab: HashMap<String, usize>,
    reverse_vocab: HashMap<usize, String>,
    merges: Vec<(String, String)>,
    vocab_size: usize,
}

impl BPETokenizer {
    /// üèóÔ∏è Cria um novo tokenizador BPE
    fn new(vocab_size: usize) -> Result<Self> {
        let mut vocab = HashMap::new();
        let mut reverse_vocab = HashMap::new();
        
        // Adiciona tokens especiais
        vocab.insert("<unk>".to_string(), 0);
        vocab.insert("<pad>".to_string(), 1);
        vocab.insert("<eos>".to_string(), 2);
        vocab.insert("<bos>".to_string(), 3);
        
        reverse_vocab.insert(0, "<unk>".to_string());
        reverse_vocab.insert(1, "<pad>".to_string());
        reverse_vocab.insert(2, "<eos>".to_string());
        reverse_vocab.insert(3, "<bos>".to_string());
        
        // Adiciona caracteres ASCII b√°sicos
        let mut next_id = 4;
        for i in 32..127 {
            let ch = char::from(i as u8).to_string();
            vocab.insert(ch.clone(), next_id);
            reverse_vocab.insert(next_id, ch);
            next_id += 1;
        }
        
        Ok(Self {
            vocab,
            reverse_vocab,
            merges: Vec::new(),
            vocab_size,
        })
    }
    
    /// üéì Treina o tokenizador com texto fornecido
    fn train(&mut self, text: &str) -> Result<()> {
        println!("üéì Iniciando treinamento do tokenizador BPE...");
        
        // Obt√©m frequ√™ncias de palavras
        let mut word_freqs = self.get_word_frequencies(text);
        println!("üìä Encontradas {} palavras √∫nicas", word_freqs.len());
        
        let target_merges = self.vocab_size - self.vocab.len();
        println!("üéØ Realizando {} merges para atingir vocabul√°rio de {}", target_merges, self.vocab_size);
        
        for merge_step in 0..target_merges {
            // Encontra o par mais frequente
            let pair_freqs = self.get_pair_frequencies(&word_freqs);
            
            if pair_freqs.is_empty() {
                println!("‚ö†Ô∏è  N√£o h√° mais pares para merge. Parando no passo {}", merge_step);
                break;
            }
            
            let best_pair = pair_freqs.iter()
                .max_by_key(|(_, &freq)| freq)
                .map(|(pair, _)| pair.clone())
                .unwrap();
            
            let freq = pair_freqs[&best_pair];
            
            if merge_step < 10 || merge_step % 100 == 0 {
                println!("üîÑ Merge {}: ('{}', '{}') - frequ√™ncia: {}", 
                        merge_step + 1, best_pair.0, best_pair.1, freq);
            }
            
            // Aplica o merge
            word_freqs = self.apply_merge(&word_freqs, &best_pair);
            
            // Adiciona ao vocabul√°rio
            let new_token = format!("{}{}", best_pair.0, best_pair.1);
            let new_id = self.vocab.len();
            self.vocab.insert(new_token.clone(), new_id);
            self.reverse_vocab.insert(new_id, new_token);
            
            // Salva o merge
            self.merges.push(best_pair);
        }
        
        println!("‚úÖ Treinamento conclu√≠do! Vocabul√°rio final: {} tokens", self.vocab.len());
        Ok(())
    }
    
    /// üìä Obt√©m frequ√™ncias de palavras no texto
    fn get_word_frequencies(&self, text: &str) -> HashMap<Vec<String>, usize> {
        let mut word_freqs = HashMap::new();
        
        for word in text.split_whitespace() {
            let chars: Vec<String> = word.chars().map(|c| c.to_string()).collect();
            *word_freqs.entry(chars).or_insert(0) += 1;
        }
        
        word_freqs
    }
    
    /// üîç Obt√©m frequ√™ncias de pares adjacentes
    fn get_pair_frequencies(&self, word_freqs: &HashMap<Vec<String>, usize>) -> HashMap<(String, String), usize> {
        let mut pair_freqs = HashMap::new();
        
        for (word, &freq) in word_freqs {
            for i in 0..word.len().saturating_sub(1) {
                let pair = (word[i].clone(), word[i + 1].clone());
                *pair_freqs.entry(pair).or_insert(0) += freq;
            }
        }
        
        pair_freqs
    }
    
    /// üîÑ Aplica um merge espec√≠fico
    fn apply_merge(&self, word_freqs: &HashMap<Vec<String>, usize>, pair: &(String, String)) -> HashMap<Vec<String>, usize> {
        let mut new_word_freqs = HashMap::new();
        
        for (word, &freq) in word_freqs {
            let new_word = self.apply_merge_to_word(word.clone(), pair);
            *new_word_freqs.entry(new_word).or_insert(0) += freq;
        }
        
        new_word_freqs
    }
    
    /// üîß Aplica merge a uma palavra espec√≠fica
    fn apply_merge_to_word(&self, mut word: Vec<String>, merge: &(String, String)) -> Vec<String> {
        let mut i = 0;
        while i < word.len().saturating_sub(1) {
            if word[i] == merge.0 && word[i + 1] == merge.1 {
                let merged = format!("{}{}", merge.0, merge.1);
                word[i] = merged;
                word.remove(i + 1);
            } else {
                i += 1;
            }
        }
        word
    }
    
    /// üî¢ Codifica texto em IDs de tokens
    fn encode(&self, text: &str) -> Result<Vec<usize>> {
        let mut tokens = Vec::new();
        
        for word in text.split_whitespace() {
            let mut chars: Vec<String> = word.chars().map(|c| c.to_string()).collect();
            
            // Aplica todos os merges na ordem
            for merge in &self.merges {
                chars = self.apply_merge_to_word(chars, merge);
            }
            
            // Converte para IDs
            for token in chars {
                if let Some(&id) = self.vocab.get(&token) {
                    tokens.push(id);
                } else {
                    tokens.push(0); // <unk>
                }
            }
        }
        
        Ok(tokens)
    }
    
    /// üî§ Decodifica IDs de tokens em texto
    fn decode(&self, tokens: &[usize]) -> Result<String> {
        let mut result = String::new();
        
        for &token_id in tokens {
            if let Some(token) = self.reverse_vocab.get(&token_id) {
                result.push_str(token);
            } else {
                result.push_str("<unk>");
            }
        }
        
        Ok(result)
    }
    
    /// üìè Retorna tamanho do vocabul√°rio
    fn vocab_size(&self) -> usize {
        self.vocab.len()
    }
}

/// üé≠ Estrutura para demonstrar conceitos de tokeniza√ß√£o
struct TokenizerDemo {
    tokenizer: BPETokenizer,
}

impl TokenizerDemo {
    /// üèóÔ∏è Cria uma nova inst√¢ncia de demonstra√ß√£o
    fn new(vocab_size: usize) -> Result<Self> {
        let tokenizer = BPETokenizer::new(vocab_size)?;
        Ok(Self { tokenizer })
    }
    
    /// üìö Demonstra treinamento b√°sico com texto simples
    fn demo_basic_training(&mut self) -> Result<()> {
        println!("\nüìö === DEMONSTRA√á√ÉO DE TREINAMENTO B√ÅSICO ===");
        
        let training_text = "
            O gato subiu no telhado.
            O gato desceu do telhado.
            O cachorro correu no jardim.
            O cachorro brincou no jardim.
            A crian√ßa brincou com o gato.
            A crian√ßa correu com o cachorro.
            O sol brilhou no jardim.
            A lua brilhou no telhado.
        ";
        
        println!("üìù Texto de treinamento:");
        println!("{}", training_text.trim());
        
        println!("\nüî¢ Estat√≠sticas do texto:");
        let words: Vec<&str> = training_text.split_whitespace().collect();
        let unique_words: std::collections::HashSet<&str> = words.iter().cloned().collect();
        let chars: Vec<char> = training_text.chars().filter(|c| !c.is_whitespace()).collect();
        let unique_chars: std::collections::HashSet<char> = chars.iter().cloned().collect();
        
        println!("  - Total de palavras: {}", words.len());
        println!("  - Palavras √∫nicas: {}", unique_words.len());
        println!("  - Total de caracteres: {}", chars.len());
        println!("  - Caracteres √∫nicos: {}", unique_chars.len());
        
        // Treina o tokenizador
        let start_time = Instant::now();
        self.tokenizer.train(training_text)?;
        let training_duration = start_time.elapsed();
        
        println!("\n‚è±Ô∏è  Tempo de treinamento: {:?}", training_duration);
        println!("üìä Vocabul√°rio final: {} tokens", self.tokenizer.vocab_size());
        
        Ok(())
    }
    
    /// üîç Demonstra processo de tokeniza√ß√£o passo a passo
    fn demo_tokenization_process(&self) -> Result<()> {
        println!("\nüîç === PROCESSO DE TOKENIZA√á√ÉO ===");
        
        let test_sentences = vec![
            "O gato subiu",
            "programa√ß√£o em Rust",
            "intelig√™ncia artificial",
            "tokeniza√ß√£o avan√ßada",
        ];
        
        for sentence in test_sentences {
            println!("\nüìù Frase: '{}'", sentence);
            
            // Tokeniza
            let start_time = Instant::now();
            let tokens = self.tokenizer.encode(sentence)?;
            let encoding_duration = start_time.elapsed();
            
            // Decodifica
            let start_time = Instant::now();
            let decoded = self.tokenizer.decode(&tokens)?;
            let decoding_duration = start_time.elapsed();
            
            println!("üî¢ Tokens: {:?}", tokens);
            println!("üî§ Decodificado: '{}'", decoded);
            println!("‚è±Ô∏è  Codifica√ß√£o: {:?}, Decodifica√ß√£o: {:?}", 
                    encoding_duration, decoding_duration);
            
            // Analisa compress√£o
            let original_chars = sentence.len();
            let token_count = tokens.len();
            let compression_ratio = original_chars as f32 / token_count as f32;
            
            println!("üìä Compress√£o: {} chars ‚Üí {} tokens (ratio: {:.2}x)", 
                    original_chars, token_count, compression_ratio);
            
            // Mostra tokens individuais
            println!("üß© Breakdown dos tokens:");
            for (i, &token_id) in tokens.iter().enumerate() {
                if let Some(token_str) = self.tokenizer.reverse_vocab.get(&token_id) {
                    println!("  {}: {} ‚Üí '{}'", i, token_id, token_str);
                }
            }
        }
        
        Ok(())
    }
    
    /// üìä Analisa efici√™ncia do vocabul√°rio
    fn analyze_vocabulary_efficiency(&self) -> Result<()> {
        println!("\nüìä === AN√ÅLISE DE EFICI√äNCIA DO VOCABUL√ÅRIO ===");
        
        // Analisa distribui√ß√£o de comprimentos de tokens
        let mut length_distribution: HashMap<usize, usize> = HashMap::new();
        
        for token in self.tokenizer.reverse_vocab.values() {
            let length = token.chars().count();
            *length_distribution.entry(length).or_insert(0) += 1;
        }
        
        println!("\nüìè Distribui√ß√£o de comprimentos de tokens:");
        let mut lengths: Vec<_> = length_distribution.keys().cloned().collect();
        lengths.sort();
        
        for length in lengths {
            let count = length_distribution[&length];
            let percentage = count as f32 / self.tokenizer.vocab_size() as f32 * 100.0;
            let bar = "‚ñà".repeat((percentage / 2.0) as usize);
            println!("  {} chars: {:3} tokens ({:5.1}%) {}", 
                    length, count, percentage, bar);
        }
        
        // Analisa tokens mais comuns por categoria
        self.analyze_token_categories()?;
        
        Ok(())
    }
    
    /// üè∑Ô∏è Analisa categorias de tokens
    fn analyze_token_categories(&self) -> Result<()> {
        println!("\nüè∑Ô∏è Categorias de tokens:");
        
        let mut categories = HashMap::new();
        
        for token in self.tokenizer.reverse_vocab.values() {
            let category = if token.starts_with('<') && token.ends_with('>') {
                "Especiais"
            } else if token.len() == 1 {
                if token.chars().next().unwrap().is_alphabetic() {
                    "Letras"
                } else if token.chars().next().unwrap().is_numeric() {
                    "N√∫meros"
                } else {
                    "S√≠mbolos"
                }
            } else if token.chars().all(|c| c.is_alphabetic()) {
                "Palavras/Subpalavras"
            } else {
                "Mistos"
            };
            
            *categories.entry(category).or_insert(0) += 1;
        }
        
        for (category, count) in categories {
            let percentage = count as f32 / self.tokenizer.vocab_size() as f32 * 100.0;
            println!("  {}: {} tokens ({:.1}%)", category, count, percentage);
        }
        
        Ok(())
    }
    
    /// üåç Demonstra tokeniza√ß√£o multil√≠ngue
    fn demo_multilingual_tokenization(&self) -> Result<()> {
        println!("\nüåç === TOKENIZA√á√ÉO MULTIL√çNGUE ===");
        
        let multilingual_texts = vec![
            ("Portugu√™s", "Ol√°, como voc√™ est√°?"),
            ("English", "Hello, how are you?"),
            ("Espa√±ol", "Hola, ¬øc√≥mo est√°s?"),
            ("Fran√ßais", "Salut, comment allez-vous?"),
        ];
        
        println!("\nüîç Comparando tokeniza√ß√£o entre idiomas:");
        println!("Idioma     | Texto                    | Tokens | Compress√£o");
        println!("-----------|--------------------------|--------|------------");
        
        for (language, text) in multilingual_texts {
            let tokens = self.tokenizer.encode(text)?;
            let compression = text.len() as f32 / tokens.len() as f32;
            
            println!("{:10} | {:24} | {:6} | {:8.2}x", 
                    language, text, tokens.len(), compression);
        }
        
        println!("\nüí° Observa√ß√µes:");
        println!("  - Idiomas com caracteres especiais podem ter compress√£o menor");
        println!("  - BPE se adapta aos padr√µes mais frequentes no treinamento");
        println!("  - Vocabul√°rio maior melhora suporte multil√≠ngue");
        
        Ok(())
    }
    
    /// ‚ö° Benchmark de performance
    fn benchmark_performance(&self) -> Result<()> {
        println!("\n‚ö° === BENCHMARK DE PERFORMANCE ===");
        
        let long_text = "Esta √© uma frase muito longa que ser√° repetida v√°rias vezes para simular um texto extenso. ".repeat(10);
        let test_texts = vec![
            ("Curto", "Ol√° mundo"),
            ("M√©dio", "Esta √© uma frase de tamanho m√©dio para testar a performance do tokenizador"),
            ("Longo", long_text.as_str()),
        ];
        
        println!("\n‚è±Ô∏è  Resultados do benchmark:");
        println!("Tamanho | Chars | Tokens | Encode (Œºs) | Decode (Œºs) | Throughput");
        println!("--------|-------|--------|-------------|-------------|------------");
        
        for (size_name, text) in test_texts {
            let char_count = text.len();
            
            // Benchmark encoding
            let iterations = 1000;
            let start = Instant::now();
            
            for _ in 0..iterations {
                let _ = self.tokenizer.encode(text)?;
            }
            
            let encode_duration = start.elapsed();
            let avg_encode_micros = encode_duration.as_micros() / iterations;
            
            // Benchmark decoding
            let tokens = self.tokenizer.encode(text)?;
            let start = Instant::now();
            
            for _ in 0..iterations {
                let _ = self.tokenizer.decode(&tokens)?;
            }
            
            let decode_duration = start.elapsed();
            let avg_decode_micros = decode_duration.as_micros() / iterations;
            
            let throughput = char_count as f64 / (avg_encode_micros as f64 / 1_000_000.0);
            
            println!("{:7} | {:5} | {:6} | {:11} | {:11} | {:8.0} chars/s", 
                    size_name, char_count, tokens.len(), 
                    avg_encode_micros, avg_decode_micros, throughput);
        }
        
        Ok(())
    }
}

/// üéØ Exerc√≠cios pr√°ticos para aprofundar o entendimento
struct TokenizerExercises;

impl TokenizerExercises {
    /// üìù Exerc√≠cio 1: Compara√ß√£o de estrat√©gias
    fn exercise_tokenization_strategies() {
        println!("\nüìù === EXERC√çCIO 1: ESTRAT√âGIAS DE TOKENIZA√á√ÉO ===");
        println!("\nüéØ Objetivo: Comparar diferentes abordagens de tokeniza√ß√£o");
        
        println!("\nüîç Estrat√©gias para implementar:");
        
        println!("\n1. üî§ Tokeniza√ß√£o por Caracteres:");
        println!("   - Cada caractere = 1 token");
        println!("   - Vantagem: Vocabul√°rio pequeno");
        println!("   - Desvantagem: Sequ√™ncias muito longas");
        
        println!("\n2. üìù Tokeniza√ß√£o por Palavras:");
        println!("   - Cada palavra = 1 token");
        println!("   - Vantagem: Preserva significado");
        println!("   - Desvantagem: Vocabul√°rio gigante");
        
        println!("\n3. üß© BPE (Atual):");
        println!("   - Subpalavras baseadas em frequ√™ncia");
        println!("   - Vantagem: Balanceado");
        println!("   - Desvantagem: Complexidade de treinamento");
        
        println!("\n4. üéØ SentencePiece:");
        println!("   - Trata texto como sequ√™ncia de bytes");
        println!("   - Vantagem: Independente de idioma");
        println!("   - Desvantagem: Pode quebrar caracteres");
        
        println!("\nüí° Experimento sugerido:");
        println!("  1. Implemente tokenizador por caracteres");
        println!("  2. Implemente tokenizador por palavras");
        println!("  3. Compare efici√™ncia em diferentes tipos de texto");
        println!("  4. Analise trade-offs de cada abordagem");
    }
    
    /// üî¨ Exerc√≠cio 2: Otimiza√ß√£o de vocabul√°rio
    fn exercise_vocabulary_optimization() {
        println!("\nüî¨ === EXERC√çCIO 2: OTIMIZA√á√ÉO DE VOCABUL√ÅRIO ===");
        println!("\nüéØ Objetivo: Otimizar tamanho e composi√ß√£o do vocabul√°rio");
        
        println!("\n‚ö° T√©cnicas de otimiza√ß√£o:");
        
        println!("\n1. üìä An√°lise de Frequ√™ncia:");
        println!("   - Identifique tokens subutilizados");
        println!("   - Remova tokens com frequ√™ncia < threshold");
        println!("   - Substitua por decomposi√ß√£o em subtokens");
        
        println!("\n2. üéØ Vocabul√°rio Adaptativo:");
        println!("   - Ajuste vocabul√°rio para dom√≠nio espec√≠fico");
        println!("   - Adicione termos t√©cnicos relevantes");
        println!("   - Remova tokens irrelevantes");
        
        println!("\n3. üîÑ Re-treinamento Incremental:");
        println!("   - Atualize vocabul√°rio com novos dados");
        println!("   - Mantenha compatibilidade com modelos existentes");
        println!("   - Use t√©cnicas de transfer learning");
        
        println!("\n4. üìà M√©tricas de Qualidade:");
        println!("   - Taxa de compress√£o");
        println!("   - Cobertura de vocabul√°rio");
        println!("   - Efici√™ncia de encoding/decoding");
        
        println!("\nüí° Implementa√ß√£o sugerida:");
        println!("  1. Analise distribui√ß√£o de frequ√™ncias");
        println!("  2. Implemente pruning de vocabul√°rio");
        println!("  3. Teste em diferentes dom√≠nios");
        println!("  4. Me√ßa impacto na qualidade do modelo");
    }
    
    /// üåê Exerc√≠cio 3: Suporte multil√≠ngue avan√ßado
    fn exercise_multilingual_support() {
        println!("\nüåê === EXERC√çCIO 3: SUPORTE MULTIL√çNGUE AVAN√áADO ===");
        println!("\nüéØ Objetivo: Melhorar tokeniza√ß√£o para m√∫ltiplos idiomas");
        
        println!("\nüîç Desafios multil√≠ngues:");
        
        println!("\n1. üìù Scripts Diferentes:");
        println!("   - Latino, Cir√≠lico, √Årabe, CJK");
        println!("   - Dire√ß√µes de escrita diferentes");
        println!("   - Sistemas de pontua√ß√£o variados");
        
        println!("\n2. üî§ Morfologia Complexa:");
        println!("   - Aglutina√ß√£o (Turco, Finland√™s)");
        println!("   - Flex√£o rica (Russo, Alem√£o)");
        println!("   - Composi√ß√£o (Alem√£o, Holand√™s)");
        
        println!("\n3. üéØ Balanceamento de Idiomas:");
        println!("   - Evitar bias para idiomas dominantes");
        println!("   - Garantir cobertura adequada");
        println!("   - Otimizar para idiomas de baixo recurso");
        
        println!("\nüí° Solu√ß√µes propostas:");
        println!("  1. Pr√©-processamento espec√≠fico por script");
        println!("  2. Vocabul√°rio balanceado por idioma");
        println!("  3. Normaliza√ß√£o Unicode consistente");
        println!("  4. Teste em corpora multil√≠ngues");
    }
}

/// üöÄ Fun√ß√£o principal que executa todas as demonstra√ß√µes
fn main() -> Result<()> {
    println!("üî§ === DEMONSTRA√á√ÉO DO SISTEMA DE TOKENIZA√á√ÉO BPE ===");
    println!("Explorando como texto se torna n√∫meros que modelos entendem");
    
    let mut demo = TokenizerDemo::new(500)?; // Vocabul√°rio de 500 tokens
    
    // Executa demonstra√ß√µes b√°sicas
    demo.demo_basic_training()?;
    demo.demo_tokenization_process()?;
    demo.analyze_vocabulary_efficiency()?;
    demo.demo_multilingual_tokenization()?;
    
    // Benchmark de performance
    demo.benchmark_performance()?;
    
    // Exerc√≠cios educacionais
    println!("\n\nüéì === EXERC√çCIOS PR√ÅTICOS ===");
    TokenizerExercises::exercise_tokenization_strategies();
    TokenizerExercises::exercise_vocabulary_optimization();
    TokenizerExercises::exercise_multilingual_support();
    
    println!("\n\n‚úÖ === DEMONSTRA√á√ÉO CONCLU√çDA ===");
    println!("üéØ Pr√≥ximos passos:");
    println!("  1. Experimente com diferentes tamanhos de vocabul√°rio");
    println!("  2. Teste com textos de dom√≠nios espec√≠ficos");
    println!("  3. Implemente os exerc√≠cios sugeridos");
    println!("  4. Compare com outros algoritmos de tokeniza√ß√£o");
    
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_tokenizer_creation() {
        let tokenizer = BPETokenizer::new(1000).unwrap();
        assert!(tokenizer.vocab_size() >= 100); // Pelo menos caracteres b√°sicos
    }
    
    #[test]
    fn test_basic_encoding_decoding() {
        let mut tokenizer = BPETokenizer::new(200).unwrap();
        let text = "hello world";
        
        // Treina com texto simples
        tokenizer.train(text).unwrap();
        
        // Testa encoding/decoding
        let tokens = tokenizer.encode(text).unwrap();
        let decoded = tokenizer.decode(&tokens).unwrap();
        
        assert!(!tokens.is_empty());
        assert_eq!(decoded.replace(" ", ""), text.replace(" ", ""));
    }
    
    #[test]
    fn test_special_tokens() {
        let tokenizer = BPETokenizer::new(100).unwrap();
        
        // Verifica se tokens especiais existem
        assert!(tokenizer.vocab.contains_key("<unk>"));
        assert!(tokenizer.vocab.contains_key("<pad>"));
        assert!(tokenizer.vocab.contains_key("<eos>"));
        assert!(tokenizer.vocab.contains_key("<bos>"));
    }
    
    #[test]
    fn test_merge_application() {
        let tokenizer = BPETokenizer::new(100).unwrap();
        let word = vec!["h".to_string(), "e".to_string(), "l".to_string(), "l".to_string(), "o".to_string()];
        let merge = ("l".to_string(), "l".to_string());
        
        let result = tokenizer.apply_merge_to_word(word, &merge);
        assert_eq!(result, vec!["h", "e", "ll", "o"]);
    }
}