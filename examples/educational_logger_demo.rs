//! # ğŸ“š **DEMONSTRAÃ‡ÃƒO: SISTEMA DE LOGGING EDUCACIONAL AVANÃ‡ADO**
//!
//! Este exemplo demonstra o sistema completo de logging educacional que torna
//! visÃ­vel todo o ciclo de vida de um Large Language Model (LLM), desde a
//! inicializaÃ§Ã£o atÃ© a geraÃ§Ã£o de texto.
//!
//! ## ğŸ¯ **O QUE VOCÃŠ VAI APRENDER:**
//!
//! 1. **Logging de InicializaÃ§Ã£o** - Como o modelo Ã© construÃ­do
//! 2. **Logging de Treinamento** - Como o modelo aprende
//! 3. **Logging de InferÃªncia** - Como o modelo gera texto
//! 4. **AnÃ¡lise de Performance** - MÃ©tricas e otimizaÃ§Ãµes
//! 5. **VisualizaÃ§Ã£o de AtenÃ§Ã£o** - Como o modelo "presta atenÃ§Ã£o"
//! 6. **Debugging AvanÃ§ado** - DetecÃ§Ã£o de problemas

use std::collections::HashMap;
use std::time::{Duration, Instant};

// ============================================================================
// ğŸ—ï¸ ESTRUTURAS SIMPLIFICADAS PARA DEMONSTRAÃ‡ÃƒO
// ============================================================================

#[derive(Debug, Clone, PartialEq)]
pub enum ModelPhase {
    /// ğŸ—ï¸ Inicializando arquitetura e pesos
    Initialization,
    /// ğŸ“Š Preparando dados de entrada
    Preprocessing,
    /// ğŸ“ Treinando o modelo
    Training { epoch: u32, batch: u32 },
    /// ğŸ”® Gerando texto
    Inference { step: u32 },
    /// ğŸ“ˆ Avaliando performance
    Evaluation,
}

#[derive(Debug, Clone)]
pub struct TrainingEpochMetrics {
    /// ğŸ“Š NÃºmero da Ã©poca
    pub epoch: u32,
    /// ğŸ“‰ Loss mÃ©dia da Ã©poca
    pub avg_loss: f32,
    /// ğŸ“‰ Menor loss da Ã©poca
    pub min_loss: f32,
    /// ğŸ“ˆ Maior loss da Ã©poca
    pub max_loss: f32,
    /// â±ï¸ DuraÃ§Ã£o da Ã©poca em ms
    pub duration_ms: u64,
    /// ğŸ¯ AcurÃ¡cia (opcional)
    pub accuracy: Option<f32>,
    /// ğŸ“ Norma dos gradientes
    pub gradient_norm: f32,
    /// ğŸ“š Taxa de aprendizado
    pub learning_rate: f32,
    /// ğŸ’¾ Uso de memÃ³ria em MB
    pub memory_usage_mb: f32,
}

#[derive(Debug, Clone)]
pub struct LogEntry {
    /// â° Timestamp da operaÃ§Ã£o
    pub timestamp: Instant,
    /// ğŸ·ï¸ Tipo da operaÃ§Ã£o
    pub operation_type: OperationType,
    /// ğŸ“ DescriÃ§Ã£o da operaÃ§Ã£o
    pub description: String,
    /// ğŸ“Š Dados associados
    pub data: LogData,
    /// â­ ImportÃ¢ncia (0-3)
    pub importance: u8,
}

#[derive(Debug, Clone)]
pub enum OperationType {
    /// ğŸ—ï¸ InicializaÃ§Ã£o de componentes
    Initialization(String),
    /// â¡ï¸ Forward pass
    Forward(String),
    /// â¬…ï¸ Backward pass
    Backward(String),
    /// ğŸ‘ï¸ OperaÃ§Ãµes de atenÃ§Ã£o
    Attention(String),
    /// ğŸ§® OperaÃ§Ãµes matemÃ¡ticas
    Mathematical(String),
    /// ğŸ“Š MÃ©tricas
    Metric(String),
    /// âš ï¸ Avisos
    Warning(String),
}

#[derive(Debug, Clone)]
pub enum LogData {
    /// ğŸ“Š InformaÃ§Ãµes sobre tensors
    TensorInfo {
        shape: Vec<usize>,
        dtype: String,
        device: String,
        memory_mb: f32,
    },
    /// ğŸ§® Resultado de operaÃ§Ã£o matemÃ¡tica
    MathResult {
        input_shapes: Vec<Vec<usize>>,
        output_shape: Vec<usize>,
        operation: String,
        flops: u64,
    },
    /// ğŸ“ˆ MÃ©trica
    Metric {
        name: String,
        value: f32,
        unit: String,
    },
    /// âš¡ Performance
    Performance {
        duration_ms: f64,
        memory_delta_mb: f32,
        cpu_usage: f32,
    },
    /// ğŸ“ Texto simples
    Text(String),
}

#[derive(Debug, Clone, Default)]
pub struct LogStats {
    /// ğŸ”¢ Total de operaÃ§Ãµes
    pub total_operations: u64,
    /// â±ï¸ Tempo total em ms
    pub total_time_ms: f64,
    /// ğŸ’¾ Pico de memÃ³ria em MB
    pub peak_memory_mb: f32,
    /// ğŸ§® Total de FLOPs
    pub total_flops: u64,
    /// ğŸ“Š DistribuiÃ§Ã£o de operaÃ§Ãµes
    pub operation_distribution: HashMap<String, u64>,
}

#[derive(Debug, Clone)]
pub struct Matrix {
    pub data: Vec<f32>,
    pub rows: usize,
    pub cols: usize,
}

impl Matrix {
    pub fn new(rows: usize, cols: usize) -> Self {
        Self {
            data: vec![0.0; rows * cols],
            rows,
            cols,
        }
    }
    
    pub fn random(rows: usize, cols: usize) -> Self {
        let mut matrix = Self::new(rows, cols);
        for i in 0..matrix.data.len() {
            matrix.data[i] = (i as f32 * 0.1) % 1.0;
        }
        matrix
    }
    
    pub fn shape(&self) -> Vec<usize> {
        vec![self.rows, self.cols]
    }
    
    pub fn memory_mb(&self) -> f32 {
        (self.data.len() * 4) as f32 / 1024.0 / 1024.0
    }
}

// ============================================================================
// ğŸ“š SISTEMA DE LOGGING EDUCACIONAL
// ============================================================================

#[derive(Debug, Clone)]
pub struct EducationalLogger {
    /// ğŸ“š HistÃ³rico completo de todas as operaÃ§Ãµes
    pub operations: Vec<LogEntry>,
    /// â±ï¸ MediÃ§Ã£o de tempos para anÃ¡lise de performance
    pub timestamps: HashMap<String, Instant>,
    /// ğŸ“Š EstatÃ­sticas acumuladas durante execuÃ§Ã£o
    pub stats: LogStats,
    /// ğŸ¯ NÃ­vel de detalhamento (0=mÃ­nimo, 3=mÃ¡ximo)
    pub verbosity: u8,
    /// ğŸ—ï¸ Fase atual do modelo
    pub current_phase: ModelPhase,
    /// ğŸ“ˆ MÃ©tricas de treinamento por Ã©poca
    pub training_metrics: Vec<TrainingEpochMetrics>,
    /// ğŸ§® Contador de operaÃ§Ãµes matemÃ¡ticas
    pub operation_counts: HashMap<String, u64>,
    /// ğŸ”§ ConfiguraÃ§Ãµes de visualizaÃ§Ã£o
    pub show_tensors: bool,
    pub show_attention: bool,
}

impl EducationalLogger {
    pub fn new(verbosity: u8) -> Self {
        Self {
            operations: Vec::new(),
            timestamps: HashMap::new(),
            stats: LogStats::default(),
            verbosity,
            current_phase: ModelPhase::Initialization,
            training_metrics: Vec::new(),
            operation_counts: HashMap::new(),
            show_tensors: true,
            show_attention: true,
        }
    }
    
    pub fn new_simple() -> Self {
        Self::new(1)
    }
    
    pub fn with_verbosity(mut self, verbosity: u8) -> Self {
        self.verbosity = verbosity;
        self
    }
    
    pub fn with_tensor_info(mut self, show_tensors: bool) -> Self {
        self.show_tensors = show_tensors;
        self
    }
    
    pub fn with_attention_maps(mut self, show_attention: bool) -> Self {
        self.show_attention = show_attention;
        self
    }
    
    /// ğŸ—ï¸ Iniciar uma nova fase do modelo
    pub fn start_phase(&mut self, phase: ModelPhase) {
        let phase_name = match &phase {
            ModelPhase::Initialization => "ğŸ—ï¸ INICIALIZAÃ‡ÃƒO",
            ModelPhase::Preprocessing => "ğŸ“Š PRÃ‰-PROCESSAMENTO",
            ModelPhase::Training { epoch, batch } => {
                println!("ğŸ“ TREINAMENTO - Ã‰poca: {}, Batch: {}", epoch, batch);
                "ğŸ“ TREINAMENTO"
            },
            ModelPhase::Inference { step } => {
                println!("ğŸ”® INFERÃŠNCIA - Passo: {}", step);
                "ğŸ”® INFERÃŠNCIA"
            },
            ModelPhase::Evaluation => "ğŸ“ˆ AVALIAÃ‡ÃƒO",
        };
        
        if self.verbosity >= 1 {
            println!("\nğŸš€ ============================================");
            println!("ğŸ“ MUDANÃ‡A DE FASE: {}", phase_name);
            println!("============================================");
        }
        
        self.current_phase = phase;
        self.log_operation(
            OperationType::Initialization("phase_change".to_string()),
            format!("Iniciando fase: {}", phase_name),
            LogData::Text(format!("TransiÃ§Ã£o para: {:?}", self.current_phase)),
            2,
        );
    }
    
    /// â±ï¸ Iniciar cronÃ´metro para uma operaÃ§Ã£o
    pub fn start_timer(&mut self, operation: &str) {
        self.timestamps.insert(operation.to_string(), Instant::now());
        
        if self.verbosity >= 2 {
            println!("â±ï¸  Iniciando timer: {}", operation);
        }
    }
    
    /// â¹ï¸ Parar cronÃ´metro e retornar duraÃ§Ã£o
    pub fn end_timer(&mut self, operation: &str) -> f64 {
        if let Some(start_time) = self.timestamps.remove(operation) {
            let duration = start_time.elapsed().as_secs_f64() * 1000.0;
            self.stats.total_time_ms += duration;
            
            if self.verbosity >= 2 {
                println!("â¹ï¸  Timer finalizado: {} ({:.2}ms)", operation, duration);
            }
            
            duration
        } else {
            0.0
        }
    }
    
    /// ğŸ§® Log de operaÃ§Ã£o matemÃ¡tica
    pub fn log_math_operation(
        &mut self,
        operation_name: &str,
        input_shapes: Vec<Vec<usize>>,
        output_shape: Vec<usize>,
        explanation: &str,
    ) {
        let flops = self.estimate_flops(operation_name, &input_shapes, &output_shape);
        self.stats.total_flops += flops;
        
        *self.operation_counts.entry(operation_name.to_string()).or_insert(0) += 1;
        
        if self.verbosity >= 2 {
            println!("ğŸ§® OperaÃ§Ã£o: {} â†’ {}", operation_name, explanation);
            println!("   ğŸ“Š Input shapes: {:?}", input_shapes);
            println!("   ğŸ“Š Output shape: {:?}", output_shape);
            println!("   âš¡ FLOPs: {}", self.format_flops(flops));
        }
        
        self.log_operation(
            OperationType::Mathematical(operation_name.to_string()),
            explanation.to_string(),
            LogData::MathResult {
                input_shapes,
                output_shape,
                operation: operation_name.to_string(),
                flops,
            },
            1,
        );
    }
    
    /// ğŸ‘ï¸ Log de operaÃ§Ã£o de atenÃ§Ã£o
    pub fn log_attention_operation(
        &mut self,
        layer: usize,
        head: usize,
        seq_len: usize,
        attention_scores: Option<&Matrix>,
    ) {
        if self.verbosity >= 1 {
            println!("ğŸ‘ï¸  AtenÃ§Ã£o - Camada: {}, CabeÃ§a: {}, Seq: {}", layer, head, seq_len);
        }
        
        if let Some(scores) = attention_scores {
            if self.show_attention && self.verbosity >= 2 {
                self.visualize_attention_pattern(scores, seq_len);
            }
        }
        
        self.log_operation(
            OperationType::Attention(format!("layer_{}_head_{}", layer, head)),
            format!("AtenÃ§Ã£o na camada {} cabeÃ§a {} (seq_len={})", layer, head, seq_len),
            LogData::Text(format!("Processando atenÃ§Ã£o {}x{}", seq_len, seq_len)),
            2,
        );
    }
    
    /// ğŸ“ Log de Ã©poca de treinamento
    pub fn log_training_epoch(
        &mut self,
        epoch: u32,
        avg_loss: f32,
        min_loss: f32,
        max_loss: f32,
        gradient_norm: f32,
        learning_rate: f32,
        duration_ms: u64,
    ) {
        let metrics = TrainingEpochMetrics {
            epoch,
            avg_loss,
            min_loss,
            max_loss,
            duration_ms,
            accuracy: None,
            gradient_norm,
            learning_rate,
            memory_usage_mb: self.stats.peak_memory_mb,
        };
        
        if self.verbosity >= 1 {
            println!("\nğŸ“ Ã‰POCA {} CONCLUÃDA:", epoch);
            println!("   ğŸ“‰ Loss: {:.4} (min: {:.4}, max: {:.4})", avg_loss, min_loss, max_loss);
            println!("   ğŸ“ Gradient Norm: {:.4}", gradient_norm);
            println!("   ğŸ“š Learning Rate: {:.6}", learning_rate);
            println!("   â±ï¸ DuraÃ§Ã£o: {}ms", duration_ms);
            println!("   ğŸ’¾ MemÃ³ria: {:.1}MB", metrics.memory_usage_mb);
        }
        
        self.analyze_training_progress(&metrics);
        self.training_metrics.push(metrics);
    }
    
    /// ğŸ“Š Analisar progresso do treinamento
    fn analyze_training_progress(&self, current: &TrainingEpochMetrics) {
        if let Some(previous) = self.training_metrics.last() {
            let loss_change = current.avg_loss - previous.avg_loss;
            let loss_change_percent = (loss_change / previous.avg_loss) * 100.0;
            
            if self.verbosity >= 1 {
                if loss_change < 0.0 {
                    println!("   âœ… Melhoria: {:.2}% ({:+.4})", loss_change_percent.abs(), loss_change);
                } else {
                    println!("   âš ï¸  Piora: {:.2}% ({:+.4})", loss_change_percent, loss_change);
                }
                
                // Detectar possÃ­veis problemas
                if current.gradient_norm < 1e-6 {
                    println!("   ğŸš¨ AVISO: Gradientes muito pequenos (vanishing gradients?)");
                } else if current.gradient_norm > 10.0 {
                    println!("   ğŸš¨ AVISO: Gradientes muito grandes (exploding gradients?)");
                }
            }
        }
    }
    
    /// ğŸ§® Estimar FLOPs de uma operaÃ§Ã£o
    fn estimate_flops(&self, operation: &str, inputs: &[Vec<usize>], output: &[usize]) -> u64 {
        match operation {
            "matmul" | "linear" => {
                if inputs.len() >= 2 {
                    let m = inputs[0][0] as u64;
                    let k = inputs[0][1] as u64;
                    let n = inputs[1][1] as u64;
                    2 * m * k * n // 2 FLOPs por multiplicaÃ§Ã£o-adiÃ§Ã£o
                } else {
                    0
                }
            },
            "attention" => {
                if !output.is_empty() {
                    let seq_len = output[0] as u64;
                    4 * seq_len * seq_len * seq_len // Q*K^T + Softmax + *V
                } else {
                    0
                }
            },
            "softmax" | "gelu" | "layernorm" => {
                output.iter().product::<usize>() as u64 * 5 // ~5 ops por elemento
            },
            _ => output.iter().product::<usize>() as u64,
        }
    }
    
    /// ğŸ“Š Formatar FLOPs para exibiÃ§Ã£o
    fn format_flops(&self, flops: u64) -> String {
        if flops >= 1_000_000_000 {
            format!("{:.2}G FLOPs", flops as f64 / 1_000_000_000.0)
        } else if flops >= 1_000_000 {
            format!("{:.2}M FLOPs", flops as f64 / 1_000_000.0)
        } else if flops >= 1_000 {
            format!("{:.2}K FLOPs", flops as f64 / 1_000.0)
        } else {
            format!("{} FLOPs", flops)
        }
    }
    
    /// ğŸ‘ï¸ Visualizar padrÃ£o de atenÃ§Ã£o
    fn visualize_attention_pattern(&self, attention_scores: &Matrix, seq_len: usize) {
        if self.verbosity >= 2 {
            println!("\nğŸ‘ï¸  MAPA DE ATENÃ‡ÃƒO ({}x{}):", seq_len, seq_len);
            
            // CabeÃ§alho
            print!("     ");
            for j in 0..seq_len.min(8) {
                print!("{:6}", j);
            }
            println!();
            
            // Linhas da matriz
            for i in 0..seq_len.min(8) {
                print!("{:3}: ", i);
                for j in 0..seq_len.min(8) {
                    let idx = i * attention_scores.cols + j;
                    if idx < attention_scores.data.len() {
                        let value = attention_scores.data[idx];
                        let symbol = if value > 0.5 {
                            "â–ˆâ–ˆ"
                        } else if value > 0.3 {
                            "â–“â–“"
                        } else if value > 0.1 {
                            "â–‘â–‘"
                        } else {
                            "  "
                        };
                        print!("{:>4}", symbol);
                    } else {
                        print!("    ");
                    }
                }
                println!();
            }
            
            if seq_len > 8 {
                println!("   ... (mostrando apenas 8x8 primeiros elementos)");
            }
            println!();
        }
    }
    
    /// ğŸ“ Log genÃ©rico de operaÃ§Ã£o
    fn log_operation(
        &mut self,
        operation_type: OperationType,
        description: String,
        data: LogData,
        importance: u8,
    ) {
        let entry = LogEntry {
            timestamp: Instant::now(),
            operation_type,
            description,
            data,
            importance,
        };
        
        self.operations.push(entry);
        self.stats.total_operations += 1;
    }
    
    /// ğŸ“Š Log de tokenizaÃ§Ã£o
    pub fn log_tokenization(&self, text: &str, tokens: &[usize]) {
        if self.verbosity >= 1 {
            println!("\nğŸ”¤ TOKENIZAÃ‡ÃƒO:");
            println!("   ğŸ“ Texto original: \"{}\"", text);
            println!("   ğŸ”¢ Tokens: {:?}", tokens);
            println!("   ğŸ“Š Comprimento: {} â†’ {} tokens", text.len(), tokens.len());
            
            if self.verbosity >= 2 {
                let compression_ratio = text.len() as f32 / tokens.len() as f32;
                println!("   ğŸ“ˆ Taxa de compressÃ£o: {:.2} chars/token", compression_ratio);
            }
        }
    }
    
    /// ğŸ§® Log de anÃ¡lise de embeddings
    pub fn log_embedding_analysis(&self, tokens: &[usize], token_embeddings: &Matrix) {
        if self.verbosity >= 1 {
            println!("\nğŸ§® ANÃLISE DE EMBEDDINGS:");
            println!("   ğŸ”¢ Tokens: {} elementos", tokens.len());
            println!("   ğŸ“Š Embedding shape: {:?}", token_embeddings.shape());
            println!("   ğŸ’¾ MemÃ³ria: {:.2}MB", token_embeddings.memory_mb());
            
            if self.verbosity >= 2 {
                // Calcular estatÃ­sticas bÃ¡sicas
                let mean = token_embeddings.data.iter().sum::<f32>() / token_embeddings.data.len() as f32;
                let variance = token_embeddings.data.iter()
                    .map(|x| (x - mean).powi(2))
                    .sum::<f32>() / token_embeddings.data.len() as f32;
                let std_dev = variance.sqrt();
                
                println!("   ğŸ“ˆ EstatÃ­sticas:");
                println!("      â€¢ MÃ©dia: {:.4}", mean);
                println!("      â€¢ Desvio padrÃ£o: {:.4}", std_dev);
                println!("      â€¢ VariÃ¢ncia: {:.4}", variance);
            }
        }
    }
    
    /// ğŸ”„ Log de processamento do transformer
    pub fn log_transformer_processing(&self, layer: usize, input_shape: &[usize], output_shape: &[usize]) {
        if self.verbosity >= 1 {
            println!("\nğŸ”„ TRANSFORMER LAYER {}:", layer);
            println!("   ğŸ“Š Input shape: {:?}", input_shape);
            println!("   ğŸ“Š Output shape: {:?}", output_shape);
            
            if self.verbosity >= 2 {
                println!("   ğŸ§® Componentes processados:");
                println!("      1. ğŸ‘ï¸  Multi-Head Attention");
                println!("      2. â• Residual Connection + LayerNorm");
                println!("      3. ğŸ§  Feed-Forward Network");
                println!("      4. â• Residual Connection + LayerNorm");
            }
        }
    }
    
    /// ğŸ”® Log de prediÃ§Ã£o
    pub fn log_prediction(&self, logits: &Matrix, predicted_token: usize, top_k: usize) {
        if self.verbosity >= 1 {
            println!("\nğŸ”® PREDIÃ‡ÃƒO:");
            println!("   ğŸ“Š Logits shape: {:?}", logits.shape());
            println!("   ğŸ¯ Token predito: {}", predicted_token);
            
            if self.verbosity >= 2 && top_k > 0 {
                println!("   ğŸ“ˆ Top-{} candidatos:", top_k);
                
                // Simular top-k (em implementaÃ§Ã£o real, seria sorted)
                let mut top_indices: Vec<usize> = (0..logits.data.len().min(top_k)).collect();
                top_indices.sort_by(|&a, &b| {
                    logits.data[b].partial_cmp(&logits.data[a]).unwrap_or(std::cmp::Ordering::Equal)
                });
                
                for (rank, &idx) in top_indices.iter().enumerate() {
                    let prob = logits.data[idx];
                    println!("      {}. Token {}: {:.4}", rank + 1, idx, prob);
                }
            }
        }
    }
    
    /// ğŸ“‹ Log de resumo do processo
    pub fn log_process_summary(&self, input_text: &str, output_text: &str, total_tokens: usize, processing_time: f32) {
        if self.verbosity >= 1 {
            println!("\nğŸ“‹ RESUMO DO PROCESSO:");
            println!("   ğŸ“ Input: \"{}\"", input_text);
            println!("   ğŸ“ Output: \"{}\"", output_text);
            println!("   ğŸ”¢ Total de tokens processados: {}", total_tokens);
            println!("   â±ï¸ Tempo total: {:.2}ms", processing_time);
            println!("   âš¡ Tokens/segundo: {:.1}", total_tokens as f32 / (processing_time / 1000.0));
            
            if self.verbosity >= 2 {
                println!("   ğŸ“Š EstatÃ­sticas detalhadas:");
                println!("      â€¢ Total de operaÃ§Ãµes: {}", self.stats.total_operations);
                println!("      â€¢ Total de FLOPs: {}", self.format_flops(self.stats.total_flops));
                println!("      â€¢ Pico de memÃ³ria: {:.1}MB", self.stats.peak_memory_mb);
            }
        }
    }
    
    /// ğŸ“Š Gerar relatÃ³rio final
    pub fn generate_final_report(&self) {
        println!("\nğŸ“Š ============================================");
        println!("ğŸ“ˆ RELATÃ“RIO FINAL DE PERFORMANCE");
        println!("============================================\n");
        
        // EstatÃ­sticas gerais
        println!("ğŸ”¢ ESTATÃSTICAS GERAIS:");
        println!("   â€¢ Total de operaÃ§Ãµes: {}", self.stats.total_operations);
        println!("   â€¢ Tempo total: {:.2}ms", self.stats.total_time_ms);
        println!("   â€¢ Total de FLOPs: {}", self.format_flops(self.stats.total_flops));
        println!("   â€¢ Pico de memÃ³ria: {:.1}MB", self.stats.peak_memory_mb);
        
        // DistribuiÃ§Ã£o de operaÃ§Ãµes
        if !self.operation_counts.is_empty() {
            println!("\nğŸ“Š DISTRIBUIÃ‡ÃƒO DE OPERAÃ‡Ã•ES:");
            let mut ops: Vec<_> = self.operation_counts.iter().collect();
            ops.sort_by(|a, b| b.1.cmp(a.1));
            
            for (op, count) in ops.iter().take(10) {
                let percentage = **count as f64 / self.stats.total_operations as f64 * 100.0;
                println!("   â€¢ {}: {} ({:.1}%)", op, count, percentage);
            }
        }
        
        // MÃ©tricas de treinamento
        if !self.training_metrics.is_empty() {
            println!("\nğŸ“ PROGRESSO DO TREINAMENTO:");
            let first = &self.training_metrics[0];
            let last = &self.training_metrics[self.training_metrics.len() - 1];
            
            let loss_improvement = (first.avg_loss - last.avg_loss) / first.avg_loss * 100.0;
            println!("   â€¢ Ã‰pocas: {}", self.training_metrics.len());
            println!("   â€¢ Loss inicial: {:.4}", first.avg_loss);
            println!("   â€¢ Loss final: {:.4}", last.avg_loss);
            println!("   â€¢ Melhoria: {:.1}%", loss_improvement);
            
            let total_training_time: u64 = self.training_metrics.iter().map(|m| m.duration_ms).sum();
            println!("   â€¢ Tempo total de treinamento: {:.2}s", total_training_time as f64 / 1000.0);
        }
        
        // RecomendaÃ§Ãµes
        println!("\nğŸ’¡ RECOMENDAÃ‡Ã•ES:");
        if self.stats.peak_memory_mb > 1000.0 {
            println!("   â€¢ âš ï¸  Alto uso de memÃ³ria - considere batch size menor");
        }
        if self.stats.total_time_ms > 10000.0 {
            println!("   â€¢ âš ï¸  Processamento lento - considere otimizaÃ§Ãµes");
        }
        if !self.training_metrics.is_empty() {
            let last = &self.training_metrics[self.training_metrics.len() - 1];
            if last.gradient_norm < 1e-6 {
                println!("   â€¢ âš ï¸  Gradientes muito pequenos - ajuste learning rate");
            }
        }
        println!("   â€¢ âœ… Use kernel fusion para melhor performance");
        println!("   â€¢ âœ… Monitore mÃ©tricas regularmente");
        println!("   â€¢ âœ… Implemente early stopping se necessÃ¡rio");
    }
}

impl Default for EducationalLogger {
    fn default() -> Self {
        Self::new(1)
    }
}

// ============================================================================
// ğŸ¯ DEMONSTRAÃ‡Ã•ES EDUCACIONAIS
// ============================================================================

fn demonstrate_initialization_logging() {
    println!("\nğŸ¯ ============================================");
    println!("ğŸ—ï¸ DEMONSTRAÃ‡ÃƒO: LOGGING DE INICIALIZAÃ‡ÃƒO");
    println!("============================================\n");
    
    let mut logger = EducationalLogger::new(2)
        .with_tensor_info(true)
        .with_attention_maps(true);
    
    logger.start_phase(ModelPhase::Initialization);
    
    // Simular inicializaÃ§Ã£o de componentes
    println!("ğŸ”§ Inicializando componentes do modelo...");
    
    logger.start_timer("model_init");
    
    // Embeddings
    let vocab_size = 50000;
    let d_model = 768;
    let token_embeddings = Matrix::random(vocab_size, d_model);
    
    logger.log_operation(
        OperationType::Initialization("embeddings".to_string()),
        "Inicializando token embeddings".to_string(),
        LogData::TensorInfo {
            shape: token_embeddings.shape(),
            dtype: "f32".to_string(),
            device: "cpu".to_string(),
            memory_mb: token_embeddings.memory_mb(),
        },
        3,
    );
    
    // Position embeddings
    let max_seq_len = 2048;
    let pos_embeddings = Matrix::random(max_seq_len, d_model);
    
    logger.log_operation(
        OperationType::Initialization("position_embeddings".to_string()),
        "Inicializando position embeddings".to_string(),
        LogData::TensorInfo {
            shape: pos_embeddings.shape(),
            dtype: "f32".to_string(),
            device: "cpu".to_string(),
            memory_mb: pos_embeddings.memory_mb(),
        },
        3,
    );
    
    // Transformer layers
    let num_layers = 12;
    for layer in 0..num_layers {
        let layer_params = d_model * d_model * 4; // AproximaÃ§Ã£o
        let layer_memory = (layer_params * 4) as f32 / 1024.0 / 1024.0;
        
        logger.log_operation(
            OperationType::Initialization(format!("layer_{}", layer)),
            format!("Inicializando Transformer layer {}", layer),
            LogData::TensorInfo {
                shape: vec![d_model, d_model],
                dtype: "f32".to_string(),
                device: "cpu".to_string(),
                memory_mb: layer_memory,
            },
            2,
        );
        
        // Simular tempo de inicializaÃ§Ã£o
        std::thread::sleep(Duration::from_millis(10));
    }
    
    let init_time = logger.end_timer("model_init");
    
    // Calcular estatÃ­sticas do modelo
    let total_params = vocab_size * d_model + max_seq_len * d_model + num_layers * d_model * d_model * 4;
    let total_memory = (total_params * 4) as f32 / 1024.0 / 1024.0;
    
    logger.stats.peak_memory_mb = total_memory;
    
    println!("\nğŸ“Š ESTATÃSTICAS DE INICIALIZAÃ‡ÃƒO:");
    println!("   â€¢ Total de parÃ¢metros: {:.1}M", total_params as f32 / 1_000_000.0);
    println!("   â€¢ MemÃ³ria total: {:.1}MB", total_memory);
    println!("   â€¢ Tempo de inicializaÃ§Ã£o: {:.2}ms", init_time);
    println!("   â€¢ Camadas Transformer: {}", num_layers);
    println!("   â€¢ DimensÃ£o do modelo: {}", d_model);
    println!("   â€¢ Tamanho do vocabulÃ¡rio: {}", vocab_size);
}

fn demonstrate_training_logging() {
    println!("\nğŸ¯ ============================================");
    println!("ğŸ“ DEMONSTRAÃ‡ÃƒO: LOGGING DE TREINAMENTO");
    println!("============================================\n");
    
    let mut logger = EducationalLogger::new(2);
    
    // Simular vÃ¡rias Ã©pocas de treinamento
    let num_epochs = 5;
    let mut current_loss = 4.5;
    
    for epoch in 1..=num_epochs {
        logger.start_phase(ModelPhase::Training { epoch, batch: 0 });
        logger.start_timer(&format!("epoch_{}", epoch));
        
        // Simular batches
        let num_batches = 10;
        let mut epoch_losses = Vec::new();
        
        for batch in 1..=num_batches {
            logger.start_phase(ModelPhase::Training { epoch, batch });
            
            // Simular forward pass
            logger.log_math_operation(
                "forward_pass",
                vec![vec![32, 512], vec![512, 768]], // batch_size, seq_len, d_model
                vec![32, 512, 768],
                "Forward pass atravÃ©s do modelo",
            );
            
            // Simular cÃ¡lculo de loss
            let batch_loss = current_loss + (rand::random::<f32>() - 0.5) * 0.2;
            epoch_losses.push(batch_loss);
            
            logger.log_operation(
                OperationType::Metric("loss".to_string()),
                format!("Loss do batch {}", batch),
                LogData::Metric {
                    name: "cross_entropy_loss".to_string(),
                    value: batch_loss,
                    unit: "nats".to_string(),
                },
                2,
            );
            
            // Simular backward pass
            logger.log_math_operation(
                "backward_pass",
                vec![vec![32, 512, 768]],
                vec![32, 512, 768],
                "Backward pass - cÃ¡lculo de gradientes",
            );
            
            // Simular tempo de processamento
            std::thread::sleep(Duration::from_millis(20));
        }
        
        let epoch_time = logger.end_timer(&format!("epoch_{}", epoch)) as u64;
        
        // Calcular estatÃ­sticas da Ã©poca
        let avg_loss = epoch_losses.iter().sum::<f32>() / epoch_losses.len() as f32;
        let min_loss = epoch_losses.iter().fold(f32::INFINITY, |a, &b| a.min(b));
        let max_loss = epoch_losses.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b));
        
        // Simular gradiente norm e learning rate
        let gradient_norm = 0.5 + rand::random::<f32>() * 0.3;
        let learning_rate = 0.001 * (0.95_f32).powi(epoch as i32 - 1);
        
        logger.log_training_epoch(
            epoch,
            avg_loss,
            min_loss,
            max_loss,
            gradient_norm,
            learning_rate,
            epoch_time,
        );
        
        // Reduzir loss gradualmente (simulando aprendizado)
        current_loss *= 0.85;
    }
    
    println!("\nğŸ“ Treinamento concluÃ­do! Veja o progresso acima.");
}

fn demonstrate_inference_logging() {
    println!("\nğŸ¯ ============================================");
    println!("ğŸ”® DEMONSTRAÃ‡ÃƒO: LOGGING DE INFERÃŠNCIA");
    println!("============================================\n");
    
    let mut logger = EducationalLogger::new(2)
        .with_attention_maps(true);
    
    let input_text = "O futuro da inteligÃªncia artificial";
    let tokens = vec![15, 2847, 18, 9284, 7834]; // Simulado
    
    logger.start_phase(ModelPhase::Preprocessing);
    
    // Log de tokenizaÃ§Ã£o
    logger.log_tokenization(input_text, &tokens);
    
    // Log de embeddings
    let d_model = 768;
    let token_embeddings = Matrix::random(tokens.len(), d_model);
    logger.log_embedding_analysis(&tokens, &token_embeddings);
    
    logger.start_phase(ModelPhase::Inference { step: 0 });
    logger.start_timer("inference");
    
    // Simular processamento atravÃ©s das camadas
    let num_layers = 12;
    for layer in 0..num_layers {
        logger.log_transformer_processing(
            layer,
            &[tokens.len(), d_model],
            &[tokens.len(), d_model],
        );
        
        // Simular atenÃ§Ã£o
        let attention_scores = Matrix::random(tokens.len(), tokens.len());
        logger.log_attention_operation(
            layer,
            0, // head 0
            tokens.len(),
            Some(&attention_scores),
        );
        
        std::thread::sleep(Duration::from_millis(5));
    }
    
    // Simular prediÃ§Ã£o final
    let vocab_size = 50000;
    let logits = Matrix::random(1, vocab_size);
    let predicted_token = 1234; // Simulado
    
    logger.log_prediction(&logits, predicted_token, 5);
    
    let inference_time = logger.end_timer("inference");
    
    // Log de resumo
    let output_text = "O futuro da inteligÃªncia artificial serÃ¡";
    logger.log_process_summary(
        input_text,
        output_text,
        tokens.len() + 1,
        inference_time as f32,
    );
}

fn demonstrate_performance_analysis() {
    println!("\nğŸ¯ ============================================");
    println!("ğŸ“Š DEMONSTRAÃ‡ÃƒO: ANÃLISE DE PERFORMANCE");
    println!("============================================\n");
    
    let mut logger = EducationalLogger::new(3);
    
    // Simular vÃ¡rias operaÃ§Ãµes com diferentes complexidades
    let operations = vec![
        ("embedding_lookup", vec![vec![32, 512]], vec![32, 512, 768]),
        ("attention", vec![vec![32, 512, 768]], vec![32, 512, 768]),
        ("feedforward", vec![vec![32, 512, 768]], vec![32, 512, 768]),
        ("layernorm", vec![vec![32, 512, 768]], vec![32, 512, 768]),
        ("linear", vec![vec![32, 512, 768], vec![768, 3072]], vec![32, 512, 3072]),
    ];
    
    for (op_name, inputs, output) in operations {
        logger.start_timer(op_name);
        
        // Simular processamento
        std::thread::sleep(Duration::from_millis(rand::random_u64() % 50 + 10));
        
        logger.log_math_operation(
            op_name,
            inputs,
            output,
            &format!("Executando operaÃ§Ã£o {}", op_name),
        );
        
        logger.end_timer(op_name);
    }
    
    // Simular uso de memÃ³ria
    logger.stats.peak_memory_mb = 1024.0 + rand::random::<f32>() * 512.0;
    
    println!("\nğŸ“ˆ AnÃ¡lise de performance concluÃ­da!");
    println!("Veja as estatÃ­sticas detalhadas no relatÃ³rio final.");
}

fn demonstrate_debugging_features() {
    println!("\nğŸ¯ ============================================");
    println!("ğŸ” DEMONSTRAÃ‡ÃƒO: RECURSOS DE DEBUGGING");
    println!("============================================\n");
    
    let mut logger = EducationalLogger::new(3);
    
    // Simular problemas comuns e como o logger os detecta
    
    // 1. Gradientes explodindo
    println!("ğŸš¨ Simulando gradientes explodindo...");
    logger.log_training_epoch(
        1,
        2.5,
        2.3,
        2.8,
        15.7, // Gradient norm muito alto
        0.001,
        5000,
    );
    
    // 2. Gradientes desaparecendo
    println!("\nğŸš¨ Simulando gradientes desaparecendo...");
    logger.log_training_epoch(
        2,
        2.4,
        2.35,
        2.45,
        1e-8, // Gradient norm muito baixo
        0.001,
        5000,
    );
    
    // 3. Alto uso de memÃ³ria
    logger.stats.peak_memory_mb = 8192.0; // 8GB
    
    // 4. OperaÃ§Ãµes lentas
    logger.start_timer("slow_operation");
    std::thread::sleep(Duration::from_millis(100));
    logger.end_timer("slow_operation");
    
    println!("\nğŸ” Recursos de debugging demonstrados!");
    println!("O logger detectou automaticamente vÃ¡rios problemas potenciais.");
}

// ============================================================================
// ğŸ“ EXERCÃCIOS PRÃTICOS
// ============================================================================

fn practical_exercises() {
    println!("\nğŸ¯ ============================================");
    println!("ğŸ“ EXERCÃCIOS PRÃTICOS");
    println!("============================================\n");
    
    println!("ğŸ“š EXERCÃCIO 1: Logger Customizado");
    println!("   Tarefa: Criar um logger especÃ­fico para debugging de atenÃ§Ã£o");
    println!("   Dica: Foque em visualizar padrÃµes de atenÃ§Ã£o anÃ´malos");
    println!("   Meta: Detectar heads que nÃ£o aprendem\n");
    
    println!("ğŸ“š EXERCÃCIO 2: MÃ©tricas AvanÃ§adas");
    println!("   Tarefa: Implementar tracking de diversidade de gradientes");
    println!("   Dica: MeÃ§a a variÃ¢ncia dos gradientes entre camadas");
    println!("   Meta: Detectar layers que nÃ£o contribuem\n");
    
    println!("ğŸ“š EXERCÃCIO 3: Profiling AutomÃ¡tico");
    println!("   Tarefa: Criar sistema de profiling que identifica gargalos");
    println!("   Dica: Compare tempos esperados vs reais");
    println!("   Meta: Sugerir otimizaÃ§Ãµes automaticamente\n");
    
    println!("ğŸ“š EXERCÃCIO 4: VisualizaÃ§Ã£o Interativa");
    println!("   Tarefa: Criar dashboard web para mÃ©tricas em tempo real");
    println!("   Dica: Use WebSockets para updates em tempo real");
    println!("   Meta: Monitoramento visual durante treinamento\n");
    
    println!("ğŸ“š EXERCÃCIO 5: AnÃ¡lise de ConvergÃªncia");
    println!("   Tarefa: Implementar detecÃ§Ã£o automÃ¡tica de convergÃªncia");
    println!("   Dica: Analise tendÃªncias de loss e gradientes");
    println!("   Meta: Early stopping inteligente\n");
    
    println!("ğŸ† DESAFIO AVANÃ‡ADO: Logger DistribuÃ­do");
    println!("   Implemente logging para treinamento distribuÃ­do");
    println!("   Meta: Sincronizar mÃ©tricas entre mÃºltiplos workers");
}

// ============================================================================
// ğŸš€ FUNÃ‡ÃƒO PRINCIPAL
// ============================================================================

fn main() {
    println!("ğŸ“š ============================================");
    println!("ğŸš€ MINI GPT RUST - EDUCATIONAL LOGGER DEMO");
    println!("============================================");
    println!("Sistema completo de logging educacional que torna");
    println!("visÃ­vel todo o ciclo de vida de um LLM! ğŸ¦€ğŸ“Š");
    
    // 1. DemonstraÃ§Ã£o de inicializaÃ§Ã£o
    demonstrate_initialization_logging();
    
    // 2. DemonstraÃ§Ã£o de treinamento
    demonstrate_training_logging();
    
    // 3. DemonstraÃ§Ã£o de inferÃªncia
    demonstrate_inference_logging();
    
    // 4. AnÃ¡lise de performance
    demonstrate_performance_analysis();
    
    // 5. Recursos de debugging
    demonstrate_debugging_features();
    
    // 6. RelatÃ³rio final
    let logger = EducationalLogger::new(1);
    logger.generate_final_report();
    
    // 7. ExercÃ­cios prÃ¡ticos
    practical_exercises();
    
    println!("\nğŸ‰ ============================================");
    println!("âœ¨ DEMONSTRAÃ‡ÃƒO CONCLUÃDA!");
    println!("============================================");
    println!("Agora vocÃª entende como o sistema de logging");
    println!("educacional torna visÃ­vel cada aspecto do LLM! ğŸ“š");
    println!("\nğŸ’¡ PrÃ³ximos passos:");
    println!("   â€¢ Integre o logger em seus prÃ³prios modelos");
    println!("   â€¢ Customize os nÃ­veis de verbosidade");
    println!("   â€¢ Implemente mÃ©tricas especÃ­ficas do seu domÃ­nio");
    println!("   â€¢ Crie visualizaÃ§Ãµes personalizadas");
    println!("   â€¢ Contribua com melhorias para o projeto!");
}

// MÃ³dulo para simular nÃºmeros aleatÃ³rios simples
mod rand {
    use std::cell::Cell;
    
    thread_local! {
        static SEED: Cell<u64> = Cell::new(1);
    }
    
    pub fn random<T>() -> T
    where
        T: From<f32>,
    {
        SEED.with(|seed| {
            let current = seed.get();
            let next = current.wrapping_mul(1103515245).wrapping_add(12345);
            seed.set(next);
            let normalized = (next as f32) / (u64::MAX as f32);
            T::from(normalized)
        })
    }
    
    // FunÃ§Ã£o especÃ­fica para u64
    pub fn random_u64() -> u64 {
        SEED.with(|seed| {
            let current = seed.get();
            let next = current.wrapping_mul(1103515245).wrapping_add(12345);
            seed.set(next);
            next % 100 // Retorna valor entre 0-99
        })
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_logger_initialization() {
        let logger = EducationalLogger::new(1);
        assert_eq!(logger.verbosity, 1);
        assert_eq!(logger.current_phase, ModelPhase::Initialization);
        assert!(logger.operations.is_empty());
    }
    
    #[test]
    fn test_timer_functionality() {
        let mut logger = EducationalLogger::new(0);
        
        logger.start_timer("test_op");
        std::thread::sleep(Duration::from_millis(10));
        let duration = logger.end_timer("test_op");
        
        assert!(duration >= 10.0); // Pelo menos 10ms
        assert!(duration < 100.0);  // Menos que 100ms (margem de seguranÃ§a)
    }
    
    #[test]
    fn test_math_operation_logging() {
        let mut logger = EducationalLogger::new(0);
        
        logger.log_math_operation(
            "matmul",
            vec![vec![32, 768], vec![768, 3072]],
            vec![32, 3072],
            "Matrix multiplication test",
        );
        
        assert_eq!(logger.operations.len(), 1);
        assert_eq!(logger.stats.total_operations, 1);
        assert!(logger.stats.total_flops > 0);
    }
    
    #[test]
    fn test_training_metrics() {
        let mut logger = EducationalLogger::new(0);
        
        logger.log_training_epoch(1, 2.5, 2.3, 2.7, 0.5, 0.001, 5000);
        
        assert_eq!(logger.training_metrics.len(), 1);
        assert_eq!(logger.training_metrics[0].epoch, 1);
        assert_eq!(logger.training_metrics[0].avg_loss, 2.5);
    }
    
    #[test]
    fn test_phase_transitions() {
        let mut logger = EducationalLogger::new(0);
        
        logger.start_phase(ModelPhase::Training { epoch: 1, batch: 0 });
        assert!(matches!(logger.current_phase, ModelPhase::Training { epoch: 1, batch: 0 }));
        
        logger.start_phase(ModelPhase::Inference { step: 5 });
        assert!(matches!(logger.current_phase, ModelPhase::Inference { step: 5 }));
    }
    
    #[test]
    fn test_flops_estimation() {
        let logger = EducationalLogger::new(0);
        
        // Test matrix multiplication FLOPs
        let flops = logger.estimate_flops(
            "matmul",
            &[vec![32, 768], vec![768, 3072]],
            &[32, 3072],
        );
        
        // Expected: 2 * 32 * 768 * 3072 = 150,994,944
        let expected = 2 * 32 * 768 * 3072;
        assert_eq!(flops, expected);
    }
}