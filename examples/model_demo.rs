//! # üß† Demonstra√ß√£o do Modelo Mini-GPT
//!
//! Este exemplo demonstra como construir, configurar e usar um modelo GPT completo,
//! desde a inicializa√ß√£o at√© a gera√ß√£o de texto.
//!
//! ## üéØ O que voc√™ vai aprender:
//! - Como configurar um modelo GPT com diferentes tamanhos
//! - Processo de forward pass e c√°lculo de loss
//! - Gera√ß√£o de texto com diferentes estrat√©gias
//! - An√°lise de par√¢metros e complexidade computacional
//! - Salvamento e carregamento de checkpoints
//! - Otimiza√ß√µes de mem√≥ria e performance

use std::collections::HashMap;
use std::time::Instant;
use anyhow::Result;

// Estruturas simplificadas para demonstra√ß√£o
// Em um projeto real, estas viriam de mini_gpt_rust::model

#[derive(Debug, Clone)]
struct GPTConfig {
    pub vocab_size: usize,   // üìö Tamanho do vocabul√°rio
    pub n_embd: usize,       // üßÆ Dimens√£o dos embeddings
    pub n_head: usize,       // üëÅÔ∏è N√∫mero de cabe√ßas de aten√ß√£o
    pub n_layer: usize,      // üèóÔ∏è N√∫mero de blocos transformer
    pub block_size: usize,   // üìè Tamanho m√°ximo do contexto
    pub dropout: f32,        // üé≤ Taxa de dropout
}

impl GPTConfig {
    /// üè† Configura√ß√£o para modelo pequeno (para testes)
    fn tiny() -> Self {
        Self {
            vocab_size: 1000,
            n_embd: 128,
            n_head: 4,
            n_layer: 4,
            block_size: 64,
            dropout: 0.1,
        }
    }
    
    /// üè¢ Configura√ß√£o para modelo pequeno (experimentos)
    fn small() -> Self {
        Self {
            vocab_size: 5000,
            n_embd: 256,
            n_head: 8,
            n_layer: 6,
            block_size: 128,
            dropout: 0.1,
        }
    }
    
    /// üè≠ Configura√ß√£o para modelo m√©dio (produ√ß√£o leve)
    fn medium() -> Self {
        Self {
            vocab_size: 10000,
            n_embd: 512,
            n_head: 8,
            n_layer: 12,
            block_size: 256,
            dropout: 0.1,
        }
    }
    
    /// üöÄ Configura√ß√£o para modelo grande (alta performance)
    fn large() -> Self {
        Self {
            vocab_size: 50000,
            n_embd: 768,
            n_head: 12,
            n_layer: 24,
            block_size: 512,
            dropout: 0.1,
        }
    }
    
    /// üìä Calcula n√∫mero total de par√¢metros
    fn num_parameters(&self) -> usize {
        // Token embeddings: vocab_size √ó n_embd
        let token_emb = self.vocab_size * self.n_embd;
        
        // Position embeddings: block_size √ó n_embd
        let pos_emb = self.block_size * self.n_embd;
        
        // Transformer blocks
        let mut transformer_params = 0;
        for _ in 0..self.n_layer {
            // Multi-head attention
            let qkv_proj = 3 * self.n_embd * self.n_embd; // Q, K, V projections
            let attn_out = self.n_embd * self.n_embd;     // Output projection
            let attn_total = qkv_proj + attn_out;
            
            // Feed-forward network
            let ff_hidden = 4 * self.n_embd; // Typical expansion factor
            let ff_up = self.n_embd * ff_hidden;
            let ff_down = ff_hidden * self.n_embd;
            let ff_total = ff_up + ff_down;
            
            // Layer norms (2 per block)
            let ln_params = 2 * self.n_embd * 2; // weight + bias
            
            transformer_params += attn_total + ff_total + ln_params;
        }
        
        // Final layer norm
        let final_ln = self.n_embd * 2;
        
        // Language model head
        let lm_head = self.n_embd * self.vocab_size;
        
        token_emb + pos_emb + transformer_params + final_ln + lm_head
    }
    
    /// üíæ Estima uso de mem√≥ria em MB
    fn memory_usage_mb(&self) -> f32 {
        let params = self.num_parameters();
        let bytes_per_param = 4; // float32
        let total_bytes = params * bytes_per_param;
        
        // Adiciona overhead para ativa√ß√µes (estimativa)
        let activation_overhead = 1.5;
        
        (total_bytes as f32 * activation_overhead) / (1024.0 * 1024.0)
    }
    
    /// ‚ö° Estima FLOPs por token
    fn flops_per_token(&self) -> usize {
        let mut total_flops = 0;
        
        // Embedding lookup (neglig√≠vel)
        
        // Transformer blocks
        for _ in 0..self.n_layer {
            // Attention: Q@K^T
            let qk_flops = self.block_size * self.n_embd * self.block_size;
            
            // Attention: softmax@V
            let av_flops = self.block_size * self.block_size * self.n_embd;
            
            // Feed-forward
            let ff_flops = 2 * self.n_embd * (4 * self.n_embd); // up + down projections
            
            total_flops += qk_flops + av_flops + ff_flops;
        }
        
        // Final projection
        total_flops += self.n_embd * self.vocab_size;
        
        total_flops
    }
}

/// üé≠ Estrutura principal do modelo (simplificada para demonstra√ß√£o)
#[derive(Debug)]
struct MiniGPT {
    config: GPTConfig,
    parameters: HashMap<String, Vec<f32>>, // Simula√ß√£o de par√¢metros
    training_step: usize,
    device: String,
}

impl MiniGPT {
    /// üèóÔ∏è Cria um novo modelo com configura√ß√£o especificada
    fn new(config: GPTConfig, device: &str) -> Result<Self> {
        println!("üèóÔ∏è Inicializando modelo Mini-GPT...");
        println!("üìä Configura√ß√£o:");
        println!("  - Vocabul√°rio: {} tokens", config.vocab_size);
        println!("  - Embedding: {} dimens√µes", config.n_embd);
        println!("  - Aten√ß√£o: {} cabe√ßas", config.n_head);
        println!("  - Camadas: {} blocos", config.n_layer);
        println!("  - Contexto: {} tokens", config.block_size);
        println!("  - Dropout: {:.1}%", config.dropout * 100.0);
        
        let num_params = config.num_parameters();
        let memory_mb = config.memory_usage_mb();
        
        println!("\nüíæ Recursos computacionais:");
        println!("  - Par√¢metros: {:.2}M", num_params as f32 / 1_000_000.0);
        println!("  - Mem√≥ria: {:.1} MB", memory_mb);
        println!("  - FLOPs/token: {:.2}M", config.flops_per_token() as f32 / 1_000_000.0);
        
        // Simula inicializa√ß√£o de par√¢metros
        let mut parameters = HashMap::new();
        
        // Token embeddings
        parameters.insert(
            "token_embedding".to_string(),
            vec![0.0; config.vocab_size * config.n_embd]
        );
        
        // Position embeddings
        parameters.insert(
            "position_embedding".to_string(),
            vec![0.0; config.block_size * config.n_embd]
        );
        
        // Transformer blocks (simplificado)
        for i in 0..config.n_layer {
            parameters.insert(
                format!("block_{}_attention", i),
                vec![0.0; config.n_embd * config.n_embd * 4] // Q, K, V, O
            );
            parameters.insert(
                format!("block_{}_ffn", i),
                vec![0.0; config.n_embd * config.n_embd * 8] // up + down
            );
        }
        
        // Language model head
        parameters.insert(
            "lm_head".to_string(),
            vec![0.0; config.n_embd * config.vocab_size]
        );
        
        println!("‚úÖ Modelo inicializado com sucesso!");
        
        Ok(Self {
            config,
            parameters,
            training_step: 0,
            device: device.to_string(),
        })
    }
    
    /// üîÑ Simula forward pass do modelo
    fn forward(&self, input_ids: &[usize], targets: Option<&[usize]>) -> Result<(Vec<f32>, Option<f32>)> {
        let batch_size = 1;
        let seq_len = input_ids.len();
        
        println!("üîÑ Executando forward pass...");
        println!("  - Input shape: [{}]", seq_len);
        println!("  - Batch size: {}", batch_size);
        
        // Simula processamento atrav√©s das camadas
        let start_time = Instant::now();
        
        // 1. Token + Position Embeddings
        println!("  üìö Aplicando embeddings...");
        let mut hidden_states = vec![vec![0.0; self.config.n_embd]; seq_len];
        
        // 2. Transformer Blocks
        for layer in 0..self.config.n_layer {
            println!("  üß† Processando bloco {}...", layer + 1);
            
            // Simula aten√ß√£o multi-cabe√ßa
            self.simulate_attention(&mut hidden_states, layer)?;
            
            // Simula feed-forward
            self.simulate_feedforward(&mut hidden_states, layer)?;
        }
        
        // 3. Final Layer Norm + LM Head
        println!("  üé™ Aplicando proje√ß√£o final...");
        let logits = self.simulate_lm_head(&hidden_states)?;
        
        let forward_time = start_time.elapsed();
        println!("  ‚è±Ô∏è Tempo de forward: {:?}", forward_time);
        
        // Calcula loss se targets fornecidos
        let loss = if let Some(targets) = targets {
            Some(self.compute_loss(&logits, targets)?)
        } else {
            None
        };
        
        Ok((logits, loss))
    }
    
    /// üëÅÔ∏è Simula camada de aten√ß√£o multi-cabe√ßa
    fn simulate_attention(&self, hidden_states: &mut Vec<Vec<f32>>, layer: usize) -> Result<()> {
        let seq_len = hidden_states.len();
        let head_dim = self.config.n_embd / self.config.n_head;
        
        println!("    üëÅÔ∏è Aten√ß√£o: {} cabe√ßas, {} dim/cabe√ßa", self.config.n_head, head_dim);
        
        // Simula c√°lculo de aten√ß√£o para cada cabe√ßa
        for head in 0..self.config.n_head {
            // Q @ K^T / sqrt(d_k)
            let attention_scores = self.compute_attention_scores(seq_len, head_dim)?;
            
            // Softmax
            let attention_weights = self.softmax(&attention_scores)?;
            
            // Attention @ V
            self.apply_attention_weights(&attention_weights, hidden_states, head)?;
        }
        
        Ok(())
    }
    
    /// üßÆ Simula c√°lculo de scores de aten√ß√£o
    fn compute_attention_scores(&self, seq_len: usize, head_dim: usize) -> Result<Vec<Vec<f32>>> {
        let mut scores = vec![vec![0.0; seq_len]; seq_len];
        
        // Simula Q @ K^T
        for i in 0..seq_len {
            for j in 0..seq_len {
                // Simula produto escalar normalizado
                let score = if j <= i { // Causal mask
                    (i as f32 - j as f32).exp() / (head_dim as f32).sqrt()
                } else {
                    f32::NEG_INFINITY
                };
                scores[i][j] = score;
            }
        }
        
        Ok(scores)
    }
    
    /// üéØ Aplica softmax aos scores de aten√ß√£o
    fn softmax(&self, scores: &[Vec<f32>]) -> Result<Vec<Vec<f32>>> {
        let mut weights = vec![vec![0.0; scores[0].len()]; scores.len()];
        
        for i in 0..scores.len() {
            let mut sum = 0.0;
            let mut max_score = f32::NEG_INFINITY;
            
            // Encontra m√°ximo para estabilidade num√©rica
            for &score in &scores[i] {
                if score > max_score {
                    max_score = score;
                }
            }
            
            // Calcula exponenciais
            for j in 0..scores[i].len() {
                if scores[i][j] != f32::NEG_INFINITY {
                    weights[i][j] = (scores[i][j] - max_score).exp();
                    sum += weights[i][j];
                }
            }
            
            // Normaliza
            for j in 0..weights[i].len() {
                weights[i][j] /= sum;
            }
        }
        
        Ok(weights)
    }
    
    /// üîó Aplica pesos de aten√ß√£o aos valores
    fn apply_attention_weights(
        &self,
        weights: &[Vec<f32>],
        hidden_states: &mut Vec<Vec<f32>>,
        head: usize
    ) -> Result<()> {
        // Simula aplica√ß√£o dos pesos de aten√ß√£o
        // Em implementa√ß√£o real, isso modificaria hidden_states
        let _head_offset = head * (self.config.n_embd / self.config.n_head);
        
        // Placeholder para demonstra√ß√£o
        for i in 0..hidden_states.len() {
            for j in 0..hidden_states[i].len() {
                hidden_states[i][j] += weights[i].iter().sum::<f32>() * 0.01;
            }
        }
        
        Ok(())
    }
    
    /// ‚ö° Simula rede feed-forward
    fn simulate_feedforward(&self, hidden_states: &mut Vec<Vec<f32>>, layer: usize) -> Result<()> {
        let ff_dim = self.config.n_embd * 4; // Expans√£o t√≠pica
        
        println!("    ‚ö° Feed-forward: {} ‚Üí {} ‚Üí {}", 
                self.config.n_embd, ff_dim, self.config.n_embd);
        
        // Simula transforma√ß√£o linear + ativa√ß√£o + proje√ß√£o
        for i in 0..hidden_states.len() {
            for j in 0..hidden_states[i].len() {
                // Simula: Linear -> GELU -> Linear
                let x = hidden_states[i][j];
                let expanded = x * 2.0; // Simula expans√£o
                let activated = self.gelu(expanded); // GELU activation
                hidden_states[i][j] = activated * 0.5; // Simula proje√ß√£o de volta
            }
        }
        
        Ok(())
    }
    
    /// üåä Fun√ß√£o de ativa√ß√£o GELU
    fn gelu(&self, x: f32) -> f32 {
        0.5 * x * (1.0 + ((2.0 / std::f32::consts::PI).sqrt() * (x + 0.044715 * x.powi(3))).tanh())
    }
    
    /// üé™ Simula proje√ß√£o final para vocabul√°rio
    fn simulate_lm_head(&self, hidden_states: &[Vec<f32>]) -> Result<Vec<f32>> {
        let seq_len = hidden_states.len();
        let mut logits = vec![0.0; self.config.vocab_size];
        
        // Simula proje√ß√£o linear: hidden_states @ W_lm_head
        for i in 0..self.config.vocab_size {
            let mut sum = 0.0;
            for j in 0..self.config.n_embd {
                // Usa √∫ltimo token para predi√ß√£o
                sum += hidden_states[seq_len - 1][j] * (i as f32 * 0.01);
            }
            logits[i] = sum;
        }
        
        Ok(logits)
    }
    
    /// üìâ Calcula cross-entropy loss
    fn compute_loss(&self, logits: &[f32], targets: &[usize]) -> Result<f32> {
        if targets.is_empty() {
            return Ok(0.0);
        }
        
        let target = targets[targets.len() - 1]; // √öltimo token como target
        
        // Softmax
        let max_logit = logits.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b));
        let mut exp_sum = 0.0;
        
        for &logit in logits {
            exp_sum += (logit - max_logit).exp();
        }
        
        let log_prob = logits[target] - max_logit - exp_sum.ln();
        let loss = -log_prob;
        
        println!("  üìâ Loss: {:.4}", loss);
        
        Ok(loss)
    }
    
    /// üé≤ Gera texto usando sampling
    fn generate(
        &self,
        prompt_ids: &[usize],
        max_tokens: usize,
        temperature: f32
    ) -> Result<Vec<usize>> {
        println!("\nüé≤ Gerando {} tokens com temperatura {:.2}...", max_tokens, temperature);
        
        let mut generated = prompt_ids.to_vec();
        
        for step in 0..max_tokens {
            // Limita contexto ao block_size
            let context_start = if generated.len() > self.config.block_size {
                generated.len() - self.config.block_size
            } else {
                0
            };
            
            let context = &generated[context_start..];
            
            // Forward pass
            let (logits, _) = self.forward(context, None)?;
            
            // Aplica temperatura
            let scaled_logits: Vec<f32> = logits.iter().map(|&x| x / temperature).collect();
            
            // Sample pr√≥ximo token
            let next_token = self.sample_from_logits(&scaled_logits)?;
            generated.push(next_token);
            
            if step < 10 || step % 10 == 0 {
                println!("  Token {}: {} (logit: {:.3})", 
                        step + 1, next_token, logits[next_token]);
            }
        }
        
        println!("‚úÖ Gera√ß√£o conclu√≠da!");
        Ok(generated)
    }
    
    /// üéØ Faz sampling dos logits
    fn sample_from_logits(&self, logits: &[f32]) -> Result<usize> {
        // Softmax
        let max_logit = logits.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b));
        let mut probs = Vec::new();
        let mut sum = 0.0;
        
        for &logit in logits {
            let prob = (logit - max_logit).exp();
            probs.push(prob);
            sum += prob;
        }
        
        // Normaliza
        for prob in &mut probs {
            *prob /= sum;
        }
        
        // Sampling multinomial (simplificado)
        let mut cumsum = 0.0;
        let random_val = 0.5; // Simula√ß√£o de random
        
        for (i, &prob) in probs.iter().enumerate() {
            cumsum += prob;
            if random_val <= cumsum {
                return Ok(i);
            }
        }
        
        Ok(0) // Fallback
    }
    
    /// üíæ Simula salvamento de checkpoint
    fn save_checkpoint(&self, path: &str, description: Option<String>) -> Result<()> {
        println!("üíæ Salvando checkpoint em '{}'...", path);
        
        let metadata = CheckpointMetadata {
            config: self.config.clone(),
            training_step: self.training_step,
            timestamp: chrono::Utc::now().to_rfc3339(),
            description,
            model_size_mb: self.config.memory_usage_mb(),
            parameter_count: self.config.num_parameters(),
        };
        
        println!("üìä Metadados do checkpoint:");
        println!("  - Passo de treinamento: {}", metadata.training_step);
        println!("  - Par√¢metros: {:.2}M", metadata.parameter_count as f32 / 1_000_000.0);
        println!("  - Tamanho: {:.1} MB", metadata.model_size_mb);
        
        if let Some(desc) = &metadata.description {
            println!("  - Descri√ß√£o: {}", desc);
        }
        
        // Simula salvamento dos par√¢metros
        let total_params: usize = self.parameters.values().map(|v| v.len()).sum();
        println!("  - Salvando {} par√¢metros...", total_params);
        
        println!("‚úÖ Checkpoint salvo com sucesso!");
        Ok(())
    }
    
    /// üìä Retorna estat√≠sticas do modelo
    fn get_stats(&self) -> ModelStats {
        ModelStats {
            config: self.config.clone(),
            parameter_count: self.config.num_parameters(),
            memory_usage_mb: self.config.memory_usage_mb(),
            flops_per_token: self.config.flops_per_token(),
            training_step: self.training_step,
            device: self.device.clone(),
        }
    }
}

/// üìä Estrutura para metadados de checkpoint
#[derive(Debug, Clone)]
struct CheckpointMetadata {
    config: GPTConfig,
    training_step: usize,
    timestamp: String,
    description: Option<String>,
    model_size_mb: f32,
    parameter_count: usize,
}

/// üìà Estrutura para estat√≠sticas do modelo
#[derive(Debug)]
struct ModelStats {
    config: GPTConfig,
    parameter_count: usize,
    memory_usage_mb: f32,
    flops_per_token: usize,
    training_step: usize,
    device: String,
}

/// üé≠ Estrutura para demonstrar conceitos do modelo
struct ModelDemo;

impl ModelDemo {
    /// üèóÔ∏è Demonstra cria√ß√£o de modelos com diferentes tamanhos
    fn demo_model_sizes() -> Result<()> {
        println!("\nüèóÔ∏è === DEMONSTRA√á√ÉO DE TAMANHOS DE MODELO ===");
        
        let configs = vec![
            ("Tiny", GPTConfig::tiny()),
            ("Small", GPTConfig::small()),
            ("Medium", GPTConfig::medium()),
            ("Large", GPTConfig::large()),
        ];
        
        println!("\nüìä Compara√ß√£o de configura√ß√µes:");
        println!("Tamanho | Params | Memory | Layers | Heads | Context | FLOPs/token");
        println!("--------|--------|--------|--------|-------|---------|------------");
        
        for (name, config) in configs {
            let params_m = config.num_parameters() as f32 / 1_000_000.0;
            let memory_mb = config.memory_usage_mb();
            let flops_m = config.flops_per_token() as f32 / 1_000_000.0;
            
            println!("{:7} | {:6.1}M | {:6.1}MB | {:6} | {:5} | {:7} | {:8.1}M",
                    name, params_m, memory_mb, config.n_layer, 
                    config.n_head, config.block_size, flops_m);
        }
        
        println!("\nüí° Observa√ß√µes:");
        println!("  - Modelos maiores = mais par√¢metros = mais capacidade");
        println!("  - Mais camadas = representa√ß√µes mais abstratas");
        println!("  - Mais cabe√ßas = diferentes tipos de aten√ß√£o");
        println!("  - Contexto maior = mem√≥ria de longo prazo");
        
        Ok(())
    }
    
    /// üîÑ Demonstra forward pass detalhado
    fn demo_forward_pass() -> Result<()> {
        println!("\nüîÑ === DEMONSTRA√á√ÉO DE FORWARD PASS ===");
        
        let config = GPTConfig::small();
        let model = MiniGPT::new(config, "cpu")?;
        
        // Simula sequ√™ncia de entrada
        let input_ids = vec![1, 15, 42, 128, 7]; // IDs de tokens
        let targets = vec![15, 42, 128, 7, 99];  // Targets para loss
        
        println!("\nüìù Entrada:");
        println!("  Input IDs: {:?}", input_ids);
        println!("  Targets: {:?}", targets);
        
        // Executa forward pass
        let (logits, loss) = model.forward(&input_ids, Some(&targets))?;
        
        println!("\nüìä Sa√≠da:");
        println!("  Logits shape: [{}]", logits.len());
        println!("  Loss: {:?}", loss);
        
        // Analisa distribui√ß√£o dos logits
        let max_logit = logits.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b));
        let min_logit = logits.iter().fold(f32::INFINITY, |a, &b| a.min(b));
        let mean_logit = logits.iter().sum::<f32>() / logits.len() as f32;
        
        println!("\nüìà Estat√≠sticas dos logits:");
        println!("  M√°ximo: {:.4}", max_logit);
        println!("  M√≠nimo: {:.4}", min_logit);
        println!("  M√©dia: {:.4}", mean_logit);
        
        // Mostra top-5 predi√ß√µes
        let mut indexed_logits: Vec<(usize, f32)> = logits.iter().enumerate().map(|(i, &v)| (i, v)).collect();
        indexed_logits.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        
        println!("\nüèÜ Top-5 predi√ß√µes:");
        for (rank, (token_id, logit)) in indexed_logits.iter().take(5).enumerate() {
            println!("  {}: Token {} (logit: {:.4})", rank + 1, token_id, logit);
        }
        
        Ok(())
    }
    
    /// üé≤ Demonstra gera√ß√£o de texto
    fn demo_text_generation() -> Result<()> {
        println!("\nüé≤ === DEMONSTRA√á√ÉO DE GERA√á√ÉO DE TEXTO ===");
        
        let config = GPTConfig::tiny(); // Modelo pequeno para demo r√°pida
        let model = MiniGPT::new(config, "cpu")?;
        
        let prompt_ids = vec![1, 15, 42]; // Prompt inicial
        
        println!("\nüéØ Testando diferentes temperaturas:");
        
        let temperatures = vec![0.1, 0.7, 1.0, 1.5];
        
        for temp in temperatures {
            println!("\nüå°Ô∏è Temperatura: {:.1}", temp);
            
            let generated = model.generate(&prompt_ids, 10, temp)?;
            
            println!("  Prompt: {:?}", &generated[..prompt_ids.len()]);
            println!("  Gerado: {:?}", &generated[prompt_ids.len()..]);
            
            // Analisa diversidade
            let unique_tokens: std::collections::HashSet<_> = generated[prompt_ids.len()..].iter().collect();
            let diversity = unique_tokens.len() as f32 / (generated.len() - prompt_ids.len()) as f32;
            
            println!("  Diversidade: {:.2} ({} tokens √∫nicos)", diversity, unique_tokens.len());
        }
        
        println!("\nüí° Efeitos da temperatura:");
        println!("  - Baixa (0.1): Mais determin√≠stica, menos criativa");
        println!("  - M√©dia (0.7): Balanceada, boa para texto geral");
        println!("  - Alta (1.0+): Mais criativa, potencialmente incoerente");
        
        Ok(())
    }
    
    /// üíæ Demonstra sistema de checkpoints
    fn demo_checkpoint_system() -> Result<()> {
        println!("\nüíæ === DEMONSTRA√á√ÉO DE SISTEMA DE CHECKPOINTS ===");
        
        let config = GPTConfig::small();
        let mut model = MiniGPT::new(config, "cpu")?;
        
        // Simula progresso de treinamento
        let training_steps = vec![100, 500, 1000, 2000];
        
        for &step in &training_steps {
            model.training_step = step;
            
            let description = match step {
                100 => Some("Checkpoint inicial - modelo come√ßando a convergir".to_string()),
                500 => Some("Checkpoint intermedi√°rio - loss estabilizando".to_string()),
                1000 => Some("Checkpoint avan√ßado - boa performance".to_string()),
                2000 => Some("Checkpoint final - modelo treinado".to_string()),
                _ => None,
            };
            
            let checkpoint_path = format!("model_step_{}.safetensors", step);
            model.save_checkpoint(&checkpoint_path, description)?;
            
            println!();
        }
        
        println!("üí° Boas pr√°ticas para checkpoints:");
        println!("  - Salve regularmente durante o treinamento");
        println!("  - Inclua metadados (loss, learning rate, etc.)");
        println!("  - Use formato SafeTensors para seguran√ßa");
        println!("  - Mantenha m√∫ltiplas vers√µes para rollback");
        
        Ok(())
    }
    
    /// üìä Demonstra an√°lise de performance
    fn demo_performance_analysis() -> Result<()> {
        println!("\nüìä === AN√ÅLISE DE PERFORMANCE ===");
        
        let configs = vec![
            ("Tiny", GPTConfig::tiny()),
            ("Small", GPTConfig::small()),
            ("Medium", GPTConfig::medium()),
        ];
        
        println!("\n‚ö° Benchmark de forward pass:");
        println!("Modelo | Seq Len | Tempo (ms) | Throughput (tokens/s) | Memory (MB)");
        println!("-------|---------|------------|----------------------|------------");
        
        for (name, config) in configs {
            let model = MiniGPT::new(config, "cpu")?;
            
            let sequence_lengths = vec![16, 64, 128];
            
            for seq_len in sequence_lengths {
                let input_ids: Vec<usize> = (0..seq_len).collect();
                
                let start_time = Instant::now();
                let _ = model.forward(&input_ids, None)?;
                let duration = start_time.elapsed();
                
                let duration_ms = duration.as_millis() as f32;
                let throughput = seq_len as f32 / (duration_ms / 1000.0);
                let memory_mb = config.memory_usage_mb();
                
                println!("{:6} | {:7} | {:10.1} | {:20.1} | {:10.1}",
                        name, seq_len, duration_ms, throughput, memory_mb);
            }
        }
        
        println!("\nüîç Fatores que afetam performance:");
        println!("  - Tamanho do modelo (par√¢metros)");
        println!("  - Comprimento da sequ√™ncia (quadr√°tico na aten√ß√£o)");
        println!("  - Tamanho do batch (paraleliza√ß√£o)");
        println!("  - Dispositivo (CPU vs GPU vs TPU)");
        println!("  - Precis√£o (FP32 vs FP16 vs INT8)");
        
        Ok(())
    }
}

/// üéØ Exerc√≠cios pr√°ticos para aprofundar o entendimento
struct ModelExercises;

impl ModelExercises {
    /// üìù Exerc√≠cio 1: Otimiza√ß√£o de arquitetura
    fn exercise_architecture_optimization() {
        println!("\nüìù === EXERC√çCIO 1: OTIMIZA√á√ÉO DE ARQUITETURA ===");
        println!("\nüéØ Objetivo: Encontrar configura√ß√£o √≥tima para seu caso de uso");
        
        println!("\nüîç Par√¢metros para experimentar:");
        
        println!("\n1. üìè Tamanho do Modelo:");
        println!("   - n_embd: 128, 256, 512, 768, 1024");
        println!("   - n_layer: 4, 6, 12, 24, 48");
        println!("   - n_head: 4, 8, 12, 16");
        
        println!("\n2. üìù Contexto e Vocabul√°rio:");
        println!("   - block_size: 128, 256, 512, 1024, 2048");
        println!("   - vocab_size: 1K, 5K, 10K, 50K, 100K");
        
        println!("\n3. üéõÔ∏è Regulariza√ß√£o:");
        println!("   - dropout: 0.0, 0.1, 0.2, 0.3");
        println!("   - weight_decay: 0.01, 0.1, 0.3");
        
        println!("\nüí° Experimentos sugeridos:");
        println!("  1. Teste diferentes ratios n_embd/n_head");
        println!("  2. Compare modelos largos vs profundos");
        println!("  3. Analise trade-off mem√≥ria vs performance");
        println!("  4. Me√ßa impacto do tamanho do contexto");
    }
    
    /// üî¨ Exerc√≠cio 2: An√°lise de aten√ß√£o
    fn exercise_attention_analysis() {
        println!("\nüî¨ === EXERC√çCIO 2: AN√ÅLISE DE ATEN√á√ÉO ===");
        println!("\nüéØ Objetivo: Entender como o modelo 'presta aten√ß√£o'");
        
        println!("\nüîç T√©cnicas de an√°lise:");
        
        println!("\n1. üé® Visualiza√ß√£o de Mapas de Aten√ß√£o:");
        println!("   - Extraia pesos de aten√ß√£o de cada cabe√ßa");
        println!("   - Crie heatmaps token-to-token");
        println!("   - Identifique padr√µes (local, global, sint√°tico)");
        
        println!("\n2. üìä An√°lise Estat√≠stica:");
        println!("   - Entropia dos pesos de aten√ß√£o");
        println!("   - Dist√¢ncia m√©dia de aten√ß√£o");
        println!("   - Especializa√ß√£o por cabe√ßa");
        
        println!("\n3. üß™ Experimentos de Abla√ß√£o:");
        println!("   - Remova cabe√ßas espec√≠ficas");
        println!("   - Teste com diferentes n√∫meros de cabe√ßas");
        println!("   - Compare aten√ß√£o local vs global");
        
        println!("\nüí° Implementa√ß√£o sugerida:");
        println!("  1. Adicione hooks para capturar aten√ß√£o");
        println!("  2. Implemente visualiza√ß√µes interativas");
        println!("  3. Analise em diferentes tipos de texto");
        println!("  4. Compare com modelos de diferentes tamanhos");
    }
    
    /// üöÄ Exerc√≠cio 3: Otimiza√ß√£o de infer√™ncia
    fn exercise_inference_optimization() {
        println!("\nüöÄ === EXERC√çCIO 3: OTIMIZA√á√ÉO DE INFER√äNCIA ===");
        println!("\nüéØ Objetivo: Acelerar gera√ß√£o de texto em produ√ß√£o");
        
        println!("\n‚ö° T√©cnicas de otimiza√ß√£o:");
        
        println!("\n1. üß† KV-Cache:");
        println!("   - Cache keys e values computados");
        println!("   - Evita recomputa√ß√£o em gera√ß√£o autoregressiva");
        println!("   - Reduz complexidade de O(n¬≤) para O(n)");
        
        println!("\n2. üî¢ Quantiza√ß√£o:");
        println!("   - FP32 ‚Üí FP16 (2x speedup)");
        println!("   - FP16 ‚Üí INT8 (4x speedup)");
        println!("   - Quantiza√ß√£o din√¢mica vs est√°tica");
        
        println!("\n3. üì¶ Batching Inteligente:");
        println!("   - Agrupe sequ√™ncias de tamanhos similares");
        println!("   - Use padding m√≠nimo");
        println!("   - Implemente continuous batching");
        
        println!("\n4. üîß Otimiza√ß√µes de Kernel:");
        println!("   - Fused attention kernels");
        println!("   - Flash Attention");
        println!("   - Opera√ß√µes in-place");
        
        println!("\nüí° M√©tricas para medir:");
        println!("  - Lat√™ncia (tempo por token)");
        println!("  - Throughput (tokens por segundo)");
        println!("  - Uso de mem√≥ria");
        println!("  - Qualidade da sa√≠da (perplexity)");
    }
}

/// üöÄ Fun√ß√£o principal que executa todas as demonstra√ß√µes
fn main() -> Result<()> {
    println!("üß† === DEMONSTRA√á√ÉO DO MODELO MINI-GPT ===");
    println!("Explorando como construir e usar um Large Language Model");
    
    // Demonstra√ß√µes b√°sicas
    ModelDemo::demo_model_sizes()?;
    ModelDemo::demo_forward_pass()?;
    ModelDemo::demo_text_generation()?;
    ModelDemo::demo_checkpoint_system()?;
    ModelDemo::demo_performance_analysis()?;
    
    // Exerc√≠cios educacionais
    println!("\n\nüéì === EXERC√çCIOS PR√ÅTICOS ===");
    ModelExercises::exercise_architecture_optimization();
    ModelExercises::exercise_attention_analysis();
    ModelExercises::exercise_inference_optimization();
    
    println!("\n\n‚úÖ === DEMONSTRA√á√ÉO CONCLU√çDA ===");
    println!("üéØ Pr√≥ximos passos:");
    println!("  1. Experimente com diferentes configura√ß√µes");
    println!("  2. Implemente os exerc√≠cios sugeridos");
    println!("  3. Teste com dados reais");
    println!("  4. Explore t√©cnicas avan√ßadas de otimiza√ß√£o");
    
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_config_parameter_calculation() {
        let config = GPTConfig::tiny();
        let params = config.num_parameters();
        assert!(params > 0);
        assert!(params < 10_000_000); // Modelo tiny deve ser pequeno
    }
    
    #[test]
    fn test_model_creation() {
        let config = GPTConfig::tiny();
        let model = MiniGPT::new(config, "cpu").unwrap();
        assert_eq!(model.device, "cpu");
        assert_eq!(model.training_step, 0);
    }
    
    #[test]
    fn test_forward_pass() {
        let config = GPTConfig::tiny();
        let model = MiniGPT::new(config, "cpu").unwrap();
        
        let input_ids = vec![1, 2, 3];
        let (logits, loss) = model.forward(&input_ids, None).unwrap();
        
        assert_eq!(logits.len(), model.config.vocab_size);
        assert!(loss.is_none());
    }
    
    #[test]
    fn test_generation() {
        let config = GPTConfig::tiny();
        let model = MiniGPT::new(config, "cpu").unwrap();
        
        let prompt = vec![1, 2];
        let generated = model.generate(&prompt, 5, 1.0).unwrap();
        
        assert_eq!(generated.len(), prompt.len() + 5);
        assert_eq!(&generated[..prompt.len()], &prompt[..]);
    }
    
    #[test]
    fn test_gelu_activation() {
        let config = GPTConfig::tiny();
        let model = MiniGPT::new(config, "cpu").unwrap();
        
        // Testa propriedades da GELU
        assert_eq!(model.gelu(0.0), 0.0);
        assert!(model.gelu(1.0) > 0.0);
        assert!(model.gelu(-1.0) < 0.0);
    }
}