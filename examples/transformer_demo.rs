//! # üèóÔ∏è Demonstra√ß√£o da Arquitetura Transformer
//!
//! Este exemplo demonstra como funcionam os blocos Transformer,
//! os componentes fundamentais dos modelos de linguagem modernos.
//!
//! ## üéØ O que voc√™ aprender√°:
//! - Como funciona um bloco Transformer completo
//! - Intera√ß√£o entre Multi-Head Attention e Feed-Forward
//! - Papel das Residual Connections e Layer Normalization
//! - An√°lise de performance e otimiza√ß√µes
//! - Compara√ß√£o entre diferentes configura√ß√µes

use std::time::Instant;
use std::collections::HashMap;

// Estruturas simplificadas para demonstra√ß√£o
#[derive(Debug, Clone)]
pub struct TransformerConfig {
    pub n_embd: usize,     // Dimens√£o dos embeddings
    pub n_head: usize,     // N√∫mero de cabe√ßas de aten√ß√£o
    pub n_layer: usize,    // N√∫mero de camadas
    pub dropout: f32,      // Taxa de dropout
    pub vocab_size: usize, // Tamanho do vocabul√°rio
    pub max_seq_len: usize, // Comprimento m√°ximo da sequ√™ncia
}

#[derive(Debug)]
pub struct Matrix {
    pub data: Vec<Vec<f32>>,
    pub rows: usize,
    pub cols: usize,
}

impl Matrix {
    pub fn new(rows: usize, cols: usize) -> Self {
        Self {
            data: vec![vec![0.0; cols]; rows],
            rows,
            cols,
        }
    }
    
    pub fn random(rows: usize, cols: usize) -> Self {
        let mut matrix = Self::new(rows, cols);
        for i in 0..rows {
            for j in 0..cols {
                matrix.data[i][j] = (i as f32 * 0.1 + j as f32 * 0.01) % 1.0;
            }
        }
        matrix
    }
    
    pub fn add(&self, other: &Matrix) -> Matrix {
        let mut result = Matrix::new(self.rows, self.cols);
        for i in 0..self.rows {
            for j in 0..self.cols {
                result.data[i][j] = self.data[i][j] + other.data[i][j];
            }
        }
        result
    }
    
    pub fn layer_norm(&self) -> Matrix {
        let mut result = Matrix::new(self.rows, self.cols);
        for i in 0..self.rows {
            let mean: f32 = self.data[i].iter().sum::<f32>() / self.cols as f32;
            let variance: f32 = self.data[i].iter()
                .map(|x| (x - mean).powi(2))
                .sum::<f32>() / self.cols as f32;
            let std_dev = (variance + 1e-5).sqrt();
            
            for j in 0..self.cols {
                result.data[i][j] = (self.data[i][j] - mean) / std_dev;
            }
        }
        result
    }
}

#[derive(Debug)]
pub struct MultiHeadAttention {
    pub n_head: usize,
    pub head_dim: usize,
    pub n_embd: usize,
}

impl MultiHeadAttention {
    pub fn new(n_embd: usize, n_head: usize) -> Self {
        Self {
            n_head,
            head_dim: n_embd / n_head,
            n_embd,
        }
    }
    
    pub fn forward(&self, input: &Matrix) -> Matrix {
        // Simula√ß√£o simplificada do Multi-Head Attention
        let mut output = Matrix::new(input.rows, input.cols);
        
        // Para cada cabe√ßa de aten√ß√£o
        for head in 0..self.n_head {
            let start_dim = head * self.head_dim;
            let end_dim = start_dim + self.head_dim;
            
            // Simula Q, K, V projections e attention
            for i in 0..input.rows {
                for j in start_dim..end_dim {
                    if j < input.cols {
                        // Simula√ß√£o de aten√ß√£o: combina informa√ß√µes de diferentes posi√ß√µes
                        let mut attention_sum = 0.0;
                        for k in 0..input.rows {
                            let attention_weight = (1.0 / (1.0 + (i as f32 - k as f32).abs()));
                            attention_sum += input.data[k][j] * attention_weight;
                        }
                        output.data[i][j] += attention_sum / input.rows as f32;
                    }
                }
            }
        }
        
        output
    }
}

#[derive(Debug)]
pub struct FeedForward {
    pub n_embd: usize,
    pub hidden_dim: usize,
}

impl FeedForward {
    pub fn new(n_embd: usize) -> Self {
        Self {
            n_embd,
            hidden_dim: n_embd * 4, // Expans√£o t√≠pica 4x
        }
    }
    
    pub fn forward(&self, input: &Matrix) -> Matrix {
        let mut output = Matrix::new(input.rows, input.cols);
        
        // Simula√ß√£o de Feed-Forward: Linear -> GELU -> Linear
        for i in 0..input.rows {
            for j in 0..input.cols {
                // Primeira camada linear (expans√£o)
                let expanded = input.data[i][j] * 2.0; // Simula√ß√£o
                
                // GELU activation (aproxima√ß√£o)
                let gelu_output = expanded * 0.5 * (1.0 + (expanded * 0.7978845608).tanh());
                
                // Segunda camada linear (contra√ß√£o)
                output.data[i][j] = gelu_output * 0.5; // Simula√ß√£o
            }
        }
        
        output
    }
}

#[derive(Debug)]
pub struct TransformerBlock {
    pub attention: MultiHeadAttention,
    pub feed_forward: FeedForward,
    pub config: TransformerConfig,
}

impl TransformerBlock {
    pub fn new(config: TransformerConfig) -> Self {
        Self {
            attention: MultiHeadAttention::new(config.n_embd, config.n_head),
            feed_forward: FeedForward::new(config.n_embd),
            config,
        }
    }
    
    pub fn forward(&self, input: &Matrix) -> Matrix {
        // 1. Layer Norm + Multi-Head Attention + Residual
        let normed1 = input.layer_norm();
        let attention_out = self.attention.forward(&normed1);
        let residual1 = input.add(&attention_out);
        
        // 2. Layer Norm + Feed-Forward + Residual
        let normed2 = residual1.layer_norm();
        let ff_out = self.feed_forward.forward(&normed2);
        let residual2 = residual1.add(&ff_out);
        
        residual2
    }
}

// === DEMONSTRA√á√ïES ===

fn demo_transformer_basics() {
    println!("\nüèóÔ∏è === DEMONSTRA√á√ÉO: FUNDAMENTOS DO TRANSFORMER ===");
    
    let config = TransformerConfig {
        n_embd: 64,
        n_head: 8,
        n_layer: 6,
        dropout: 0.1,
        vocab_size: 1000,
        max_seq_len: 128,
    };
    
    println!("üìä Configura√ß√£o do Transformer:");
    println!("   ‚Ä¢ Dimens√£o dos embeddings: {}", config.n_embd);
    println!("   ‚Ä¢ N√∫mero de cabe√ßas: {}", config.n_head);
    println!("   ‚Ä¢ Dimens√£o por cabe√ßa: {}", config.n_embd / config.n_head);
    println!("   ‚Ä¢ N√∫mero de camadas: {}", config.n_layer);
    println!("   ‚Ä¢ Tamanho do vocabul√°rio: {}", config.vocab_size);
    
    // Simula uma sequ√™ncia de entrada
    let seq_len = 10;
    let input = Matrix::random(seq_len, config.n_embd);
    
    println!("\nüî§ Entrada: sequ√™ncia de {} tokens, {} dimens√µes", seq_len, config.n_embd);
    println!("   Formato da matriz: {}x{}", input.rows, input.cols);
    
    // Cria um bloco Transformer
    let transformer_block = TransformerBlock::new(config);
    
    // Processa atrav√©s do bloco
    let start = Instant::now();
    let output = transformer_block.forward(&input);
    let duration = start.elapsed();
    
    println!("\n‚ö° Processamento conclu√≠do em {:?}", duration);
    println!("   Sa√≠da: {}x{} (mesma forma da entrada)", output.rows, output.cols);
    
    // Mostra algumas estat√≠sticas
    let input_sum: f32 = input.data.iter().flatten().sum();
    let output_sum: f32 = output.data.iter().flatten().sum();
    
    println!("\nüìà Estat√≠sticas:");
    println!("   ‚Ä¢ Soma da entrada: {:.4}", input_sum);
    println!("   ‚Ä¢ Soma da sa√≠da: {:.4}", output_sum);
    println!("   ‚Ä¢ Diferen√ßa: {:.4}", (output_sum - input_sum).abs());
}

fn demo_attention_vs_feedforward() {
    println!("\nüéØ === DEMONSTRA√á√ÉO: ATEN√á√ÉO vs FEED-FORWARD ===");
    
    let config = TransformerConfig {
        n_embd: 32,
        n_head: 4,
        n_layer: 1,
        dropout: 0.0,
        vocab_size: 100,
        max_seq_len: 64,
    };
    
    let seq_len = 8;
    let input = Matrix::random(seq_len, config.n_embd);
    
    let attention = MultiHeadAttention::new(config.n_embd, config.n_head);
    let feed_forward = FeedForward::new(config.n_embd);
    
    println!("üîç Comparando componentes do Transformer:");
    
    // Testa Multi-Head Attention
    let start = Instant::now();
    let attention_out = attention.forward(&input);
    let attention_time = start.elapsed();
    
    // Testa Feed-Forward
    let start = Instant::now();
    let ff_out = feed_forward.forward(&input);
    let ff_time = start.elapsed();
    
    println!("\n‚è±Ô∏è Performance:");
    println!("   ‚Ä¢ Multi-Head Attention: {:?}", attention_time);
    println!("   ‚Ä¢ Feed-Forward Network: {:?}", ff_time);
    
    // Analisa as sa√≠das
    let attention_variance = calculate_variance(&attention_out);
    let ff_variance = calculate_variance(&ff_out);
    
    println!("\nüìä An√°lise das sa√≠das:");
    println!("   ‚Ä¢ Vari√¢ncia da Aten√ß√£o: {:.6}", attention_variance);
    println!("   ‚Ä¢ Vari√¢ncia do Feed-Forward: {:.6}", ff_variance);
    
    println!("\nüß† Interpreta√ß√£o:");
    println!("   ‚Ä¢ Aten√ß√£o: Mistura informa√ß√µes entre posi√ß√µes");
    println!("   ‚Ä¢ Feed-Forward: Processa cada posi√ß√£o independentemente");
    println!("   ‚Ä¢ Juntos: Capturam padr√µes locais e globais");
}

fn demo_residual_connections() {
    println!("\nüîó === DEMONSTRA√á√ÉO: RESIDUAL CONNECTIONS ===");
    
    let config = TransformerConfig {
        n_embd: 16,
        n_head: 2,
        n_layer: 1,
        dropout: 0.0,
        vocab_size: 50,
        max_seq_len: 32,
    };
    
    let seq_len = 5;
    let input = Matrix::random(seq_len, config.n_embd);
    
    println!("üîÑ Demonstrando o papel das Residual Connections:");
    
    // Sem residual connections
    let attention = MultiHeadAttention::new(config.n_embd, config.n_head);
    let attention_only = attention.forward(&input);
    
    // Com residual connections
    let with_residual = input.add(&attention_only);
    
    // Calcula magnitudes
    let input_magnitude = calculate_magnitude(&input);
    let attention_magnitude = calculate_magnitude(&attention_only);
    let residual_magnitude = calculate_magnitude(&with_residual);
    
    println!("\nüìè Magnitudes dos vetores:");
    println!("   ‚Ä¢ Entrada original: {:.4}", input_magnitude);
    println!("   ‚Ä¢ Apenas aten√ß√£o: {:.4}", attention_magnitude);
    println!("   ‚Ä¢ Com residual: {:.4}", residual_magnitude);
    
    println!("\n‚úÖ Benef√≠cios das Residual Connections:");
    println!("   ‚Ä¢ Preservam informa√ß√£o original");
    println!("   ‚Ä¢ Facilitam o fluxo de gradientes");
    println!("   ‚Ä¢ Permitem redes mais profundas");
    println!("   ‚Ä¢ Estabilizam o treinamento");
}

fn demo_layer_normalization() {
    println!("\nüìè === DEMONSTRA√á√ÉO: LAYER NORMALIZATION ===");
    
    let input = Matrix::random(4, 8);
    
    println!("üî¢ Demonstrando Layer Normalization:");
    
    // Antes da normaliza√ß√£o
    println!("\nüìä Antes da normaliza√ß√£o:");
    for i in 0..input.rows {
        let mean: f32 = input.data[i].iter().sum::<f32>() / input.cols as f32;
        let variance: f32 = input.data[i].iter()
            .map(|x| (x - mean).powi(2))
            .sum::<f32>() / input.cols as f32;
        println!("   Token {}: m√©dia={:.4}, vari√¢ncia={:.4}", i, mean, variance);
    }
    
    // Ap√≥s a normaliza√ß√£o
    let normalized = input.layer_norm();
    
    println!("\nüìä Ap√≥s a normaliza√ß√£o:");
    for i in 0..normalized.rows {
        let mean: f32 = normalized.data[i].iter().sum::<f32>() / normalized.cols as f32;
        let variance: f32 = normalized.data[i].iter()
            .map(|x| (x - mean).powi(2))
            .sum::<f32>() / normalized.cols as f32;
        println!("   Token {}: m√©dia={:.4}, vari√¢ncia={:.4}", i, mean, variance);
    }
    
    println!("\n‚úÖ Efeitos da Layer Normalization:");
    println!("   ‚Ä¢ M√©dia ‚âà 0 para cada token");
    println!("   ‚Ä¢ Vari√¢ncia ‚âà 1 para cada token");
    println!("   ‚Ä¢ Estabiliza o treinamento");
    println!("   ‚Ä¢ Acelera a converg√™ncia");
}

fn demo_scaling_analysis() {
    println!("\nüìà === DEMONSTRA√á√ÉO: AN√ÅLISE DE ESCALABILIDADE ===");
    
    let configs = vec![
        ("Pequeno", 64, 4, 4),
        ("M√©dio", 128, 8, 6),
        ("Grande", 256, 16, 12),
    ];
    
    println!("üîç Analisando diferentes tamanhos de modelo:");
    
    for (name, n_embd, n_head, n_layer) in configs {
        let config = TransformerConfig {
            n_embd,
            n_head,
            n_layer,
            dropout: 0.1,
            vocab_size: 1000,
            max_seq_len: 128,
        };
        
        // Calcula par√¢metros aproximados
        let attention_params = n_embd * n_embd * 4; // Q, K, V, O projections
        let ff_params = n_embd * (n_embd * 4) * 2; // Duas camadas lineares
        let layer_params = attention_params + ff_params;
        let total_params = layer_params * n_layer;
        
        // Calcula complexidade computacional (aproximada)
        let seq_len = 128;
        let attention_flops = seq_len * seq_len * n_embd * n_head;
        let ff_flops = seq_len * n_embd * (n_embd * 4) * 2;
        let layer_flops = attention_flops + ff_flops;
        let total_flops = layer_flops * n_layer;
        
        println!("\nüèóÔ∏è Modelo {}:", name);
        println!("   ‚Ä¢ Dimens√£o: {}, Cabe√ßas: {}, Camadas: {}", n_embd, n_head, n_layer);
        println!("   ‚Ä¢ Par√¢metros: ~{:.1}M", total_params as f32 / 1_000_000.0);
        println!("   ‚Ä¢ FLOPs (seq={}): ~{:.1}M", seq_len, total_flops as f32 / 1_000_000.0);
        
        // Testa performance
        let input = Matrix::random(seq_len, n_embd);
        let block = TransformerBlock::new(config);
        
        let start = Instant::now();
        let _output = block.forward(&input);
        let duration = start.elapsed();
        
        println!("   ‚Ä¢ Tempo de execu√ß√£o: {:?}", duration);
    }
}

fn demo_optimization_techniques() {
    println!("\n‚ö° === DEMONSTRA√á√ÉO: T√âCNICAS DE OTIMIZA√á√ÉO ===");
    
    println!("üöÄ Principais otimiza√ß√µes em Transformers:");
    
    println!("\n1. üî• **Kernel Fusion**:");
    println!("   ‚Ä¢ Combina opera√ß√µes em um √∫nico kernel");
    println!("   ‚Ä¢ Reduz transfer√™ncias de mem√≥ria");
    println!("   ‚Ä¢ Exemplo: LayerNorm + Attention fusionados");
    
    println!("\n2. üíæ **Memory Optimization**:");
    println!("   ‚Ä¢ Gradient Checkpointing");
    println!("   ‚Ä¢ Mixed Precision (FP16/BF16)");
    println!("   ‚Ä¢ Activation Recomputation");
    
    println!("\n3. üéØ **Attention Optimization**:");
    println!("   ‚Ä¢ Flash Attention (reduz complexidade de mem√≥ria)");
    println!("   ‚Ä¢ Sparse Attention (padr√µes de aten√ß√£o limitados)");
    println!("   ‚Ä¢ Multi-Query Attention (compartilha K,V)");
    
    println!("\n4. üîÑ **Parallelization**:");
    println!("   ‚Ä¢ Data Parallelism (batch splitting)");
    println!("   ‚Ä¢ Model Parallelism (layer splitting)");
    println!("   ‚Ä¢ Pipeline Parallelism (stage splitting)");
    
    // Simula diferentes estrat√©gias
    let configs = vec![
        ("Padr√£o", 128, 8, false),
        ("Multi-Query", 128, 8, true),
    ];
    
    println!("\nüìä Compara√ß√£o de estrat√©gias:");
    
    for (name, n_embd, n_head, multi_query) in configs {
        let kv_heads = if multi_query { 1 } else { n_head };
        let memory_reduction = if multi_query {
            1.0 - (kv_heads as f32 / n_head as f32)
        } else {
            0.0
        };
        
        println!("\nüîß Estrat√©gia {}:", name);
        println!("   ‚Ä¢ Cabe√ßas Q: {}", n_head);
        println!("   ‚Ä¢ Cabe√ßas K,V: {}", kv_heads);
        println!("   ‚Ä¢ Redu√ß√£o de mem√≥ria: {:.1}%", memory_reduction * 100.0);
    }
}

// === EXERC√çCIOS PR√ÅTICOS ===

fn practical_exercises() {
    println!("\nüéì === EXERC√çCIOS PR√ÅTICOS ===");
    
    println!("\nüìù **Exerc√≠cio 1: An√°lise de Aten√ß√£o**");
    println!("   Implemente uma fun√ß√£o que visualiza os padr√µes de aten√ß√£o");
    println!("   entre diferentes tokens em uma sequ√™ncia.");
    
    println!("\nüìù **Exerc√≠cio 2: Otimiza√ß√£o de Mem√≥ria**");
    println!("   Compare o uso de mem√≥ria entre diferentes configura√ß√µes");
    println!("   de cabe√ßas de aten√ß√£o e dimens√µes de embedding.");
    
    println!("\nüìù **Exerc√≠cio 3: An√°lise de Gradientes**");
    println!("   Implemente gradient clipping e analise como as");
    println!("   residual connections afetam o fluxo de gradientes.");
    
    println!("\nüìù **Exerc√≠cio 4: Compara√ß√£o de Arquiteturas**");
    println!("   Compare Transformer com RNN/LSTM em termos de:");
    println!("   - Paraleliza√ß√£o");
    println!("   - Mem√≥ria de longo prazo");
    println!("   - Complexidade computacional");
    
    println!("\nüìù **Exerc√≠cio 5: Implementa√ß√£o Avan√ßada**");
    println!("   Implemente Flash Attention ou outra otimiza√ß√£o");
    println!("   e me√ßa o impacto na performance.");
}

// === FUN√á√ïES AUXILIARES ===

fn calculate_variance(matrix: &Matrix) -> f32 {
    let values: Vec<f32> = matrix.data.iter().flatten().cloned().collect();
    let mean: f32 = values.iter().sum::<f32>() / values.len() as f32;
    values.iter().map(|x| (x - mean).powi(2)).sum::<f32>() / values.len() as f32
}

fn calculate_magnitude(matrix: &Matrix) -> f32 {
    matrix.data.iter()
        .flatten()
        .map(|x| x.powi(2))
        .sum::<f32>()
        .sqrt()
}

fn benchmark_transformer() {
    println!("\n‚ö° === BENCHMARK DE PERFORMANCE ===");
    
    let configs = vec![
        ("Mini", 64, 4, 2),
        ("Pequeno", 128, 8, 4),
        ("M√©dio", 256, 16, 6),
    ];
    
    let sequence_lengths = vec![32, 64, 128];
    
    println!("üèÉ Testando performance com diferentes configura√ß√µes:");
    
    for (name, n_embd, n_head, n_layer) in configs {
        println!("\nüîß Modelo {}: {}d, {}h, {}l", name, n_embd, n_head, n_layer);
        
        for &seq_len in &sequence_lengths {
            let config = TransformerConfig {
                n_embd,
                n_head,
                n_layer,
                dropout: 0.0,
                vocab_size: 1000,
                max_seq_len: seq_len,
            };
            
            let input = Matrix::random(seq_len, n_embd);
            let block = TransformerBlock::new(config);
            
            // Aquecimento
            let _ = block.forward(&input);
            
            // Benchmark
            let iterations = 10;
            let start = Instant::now();
            
            for _ in 0..iterations {
                let _ = block.forward(&input);
            }
            
            let total_time = start.elapsed();
            let avg_time = total_time / iterations;
            
            println!("   üìè Seq={}: {:?}/iter", seq_len, avg_time);
        }
    }
}

fn main() {
    println!("üèóÔ∏è === DEMONSTRA√á√ÉO DA ARQUITETURA TRANSFORMER ===");
    println!("\nEste exemplo explora os componentes fundamentais dos Transformers,");
    println!("a arquitetura que revolucionou a IA e possibilitou modelos como GPT.");
    
    demo_transformer_basics();
    demo_attention_vs_feedforward();
    demo_residual_connections();
    demo_layer_normalization();
    demo_scaling_analysis();
    demo_optimization_techniques();
    benchmark_transformer();
    practical_exercises();
    
    println!("\nüéâ === DEMONSTRA√á√ÉO CONCLU√çDA ===");
    println!("\nüöÄ **Pr√≥ximos Passos:**");
    println!("   ‚Ä¢ Experimente com diferentes configura√ß√µes");
    println!("   ‚Ä¢ Implemente otimiza√ß√µes avan√ßadas");
    println!("   ‚Ä¢ Analise padr√µes de aten√ß√£o reais");
    println!("   ‚Ä¢ Compare com outras arquiteturas");
    
    println!("\nüìö **Recursos Adicionais:**");
    println!("   ‚Ä¢ Paper original: 'Attention is All You Need'");
    println!("   ‚Ä¢ Implementa√ß√£o completa em src/transformer.rs");
    println!("   ‚Ä¢ Exemplos educacionais em examples/educational/");
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_transformer_block_forward() {
        let config = TransformerConfig {
            n_embd: 32,
            n_head: 4,
            n_layer: 1,
            dropout: 0.0,
            vocab_size: 100,
            max_seq_len: 64,
        };
        
        let input = Matrix::random(8, 32);
        let block = TransformerBlock::new(config);
        let output = block.forward(&input);
        
        assert_eq!(output.rows, input.rows);
        assert_eq!(output.cols, input.cols);
    }
    
    #[test]
    fn test_layer_normalization() {
        let input = Matrix::random(4, 8);
        let normalized = input.layer_norm();
        
        // Verifica que a normaliza√ß√£o preserva as dimens√µes
        assert_eq!(normalized.rows, input.rows);
        assert_eq!(normalized.cols, input.cols);
        
        // Verifica que a m√©dia est√° pr√≥xima de zero
        for i in 0..normalized.rows {
            let mean: f32 = normalized.data[i].iter().sum::<f32>() / normalized.cols as f32;
            assert!((mean.abs()) < 1e-5);
        }
    }
    
    #[test]
    fn test_residual_connections() {
        let input = Matrix::random(4, 16);
        let attention = MultiHeadAttention::new(16, 4);
        let attention_out = attention.forward(&input);
        let with_residual = input.add(&attention_out);
        
        // Verifica que as dimens√µes s√£o preservadas
        assert_eq!(with_residual.rows, input.rows);
        assert_eq!(with_residual.cols, input.cols);
    }
}