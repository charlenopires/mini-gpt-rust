//! # ğŸ”¥ **DEMONSTRAÃ‡ÃƒO: KERNEL FUSION E OTIMIZAÃ‡Ã•ES DE BAIXO NÃVEL**
//!
//! Este exemplo demonstra as tÃ©cnicas avanÃ§adas de kernel fusion implementadas
//! no Mini GPT, mostrando como operaÃ§Ãµes separadas sÃ£o combinadas em kernels
//! ultra-eficientes para alcanÃ§ar speedups de 2-5x.
//!
//! ## ğŸ¯ **O QUE VOCÃŠ VAI APRENDER:**
//!
//! 1. **Kernel Fusion Fundamentals** - Como combinar operaÃ§Ãµes
//! 2. **Fused Attention** - AtenÃ§Ã£o otimizada em um Ãºnico kernel
//! 3. **Fused Feed-Forward** - Redes neurais fusionadas
//! 4. **Memory Management** - Gerenciamento inteligente de memÃ³ria
//! 5. **Performance Benchmarks** - MediÃ§Ã£o de speedups reais
//! 6. **Optimization Strategies** - TÃ©cnicas avanÃ§adas de otimizaÃ§Ã£o

use std::collections::HashMap;
use std::time::{Duration, Instant};

// ============================================================================
// ğŸ—ï¸ ESTRUTURAS SIMPLIFICADAS PARA DEMONSTRAÃ‡ÃƒO
// ============================================================================

#[derive(Debug, Clone)]
pub struct Matrix {
    pub data: Vec<f32>,
    pub rows: usize,
    pub cols: usize,
}

impl Matrix {
    pub fn new(rows: usize, cols: usize) -> Self {
        Self {
            data: vec![0.0; rows * cols],
            rows,
            cols,
        }
    }
    
    pub fn random(rows: usize, cols: usize) -> Self {
        let mut matrix = Self::new(rows, cols);
        for i in 0..matrix.data.len() {
            matrix.data[i] = (i as f32 * 0.1) % 1.0;
        }
        matrix
    }
    
    pub fn get(&self, row: usize, col: usize) -> f32 {
        self.data[row * self.cols + col]
    }
    
    pub fn set(&mut self, row: usize, col: usize, value: f32) {
        self.data[row * self.cols + col] = value;
    }
    
    pub fn matmul(&self, other: &Matrix) -> Matrix {
        assert_eq!(self.cols, other.rows);
        let mut result = Matrix::new(self.rows, other.cols);
        
        for i in 0..self.rows {
            for j in 0..other.cols {
                let mut sum = 0.0;
                for k in 0..self.cols {
                    sum += self.get(i, k) * other.get(k, j);
                }
                result.set(i, j, sum);
            }
        }
        result
    }
    
    pub fn transpose(&self) -> Matrix {
        let mut result = Matrix::new(self.cols, self.rows);
        for i in 0..self.rows {
            for j in 0..self.cols {
                result.set(j, i, self.get(i, j));
            }
        }
        result
    }
    
    pub fn scale(&self, factor: f32) -> Matrix {
        let mut result = self.clone();
        for value in &mut result.data {
            *value *= factor;
        }
        result
    }
    
    pub fn softmax(&self) -> Matrix {
        let mut result = self.clone();
        
        for row in 0..self.rows {
            // Encontrar o mÃ¡ximo para estabilidade numÃ©rica
            let mut max_val = f32::NEG_INFINITY;
            for col in 0..self.cols {
                max_val = max_val.max(self.get(row, col));
            }
            
            // Calcular exponenciais e soma
            let mut sum = 0.0;
            for col in 0..self.cols {
                let exp_val = (self.get(row, col) - max_val).exp();
                result.set(row, col, exp_val);
                sum += exp_val;
            }
            
            // Normalizar
            for col in 0..self.cols {
                result.set(row, col, result.get(row, col) / sum);
            }
        }
        result
    }
    
    pub fn gelu(&self) -> Matrix {
        let mut result = self.clone();
        for value in &mut result.data {
            // AproximaÃ§Ã£o rÃ¡pida do GELU: x * sigmoid(1.702 * x)
            let x = *value;
            *value = x * (1.0 / (1.0 + (-1.702 * x).exp()));
        }
        result
    }
    
    pub fn add(&self, other: &Matrix) -> Matrix {
        assert_eq!(self.rows, other.rows);
        assert_eq!(self.cols, other.cols);
        
        let mut result = Matrix::new(self.rows, self.cols);
        for i in 0..self.data.len() {
            result.data[i] = self.data[i] + other.data[i];
        }
        result
    }
}

#[derive(Debug, Clone)]
pub struct FusionConfig {
    pub enable_attention_fusion: bool,
    pub enable_feedforward_fusion: bool,
    pub enable_memory_optimization: bool,
    pub fusion_threshold: usize,
}

impl Default for FusionConfig {
    fn default() -> Self {
        Self {
            enable_attention_fusion: true,
            enable_feedforward_fusion: true,
            enable_memory_optimization: true,
            fusion_threshold: 512,
        }
    }
}

#[derive(Debug, Clone)]
pub struct BenchmarkResult {
    pub fused_time_ms: f64,
    pub unfused_time_ms: f64,
    pub speedup: f64,
    pub memory_saved_percent: f64,
}

impl BenchmarkResult {
    pub fn new(fused_time_ms: f64, unfused_time_ms: f64) -> Self {
        let speedup = if fused_time_ms > 0.0 {
            unfused_time_ms / fused_time_ms
        } else {
            1.0
        };
        
        let memory_saved_percent = ((unfused_time_ms - fused_time_ms) / unfused_time_ms * 100.0)
            .max(0.0)
            .min(100.0);
        
        Self {
            fused_time_ms,
            unfused_time_ms,
            speedup,
            memory_saved_percent,
        }
    }
}

#[derive(Debug, Clone)]
pub struct MemoryStats {
    pub cache_hit_rate: f64,
    pub total_tensors_cached: usize,
    pub memory_usage_bytes: usize,
    pub memory_saved_bytes: u64,
    pub pool_efficiency: f64,
}

// ============================================================================
// ğŸ”¥ FUSED ATTENTION KERNEL
// ============================================================================

pub struct FusedAttentionKernel {
    config: FusionConfig,
    memory_pool: HashMap<String, Matrix>,
}

impl FusedAttentionKernel {
    pub fn new(config: FusionConfig) -> Self {
        Self {
            config,
            memory_pool: HashMap::new(),
        }
    }
    
    /// ImplementaÃ§Ã£o fusionada: Q*K^T + Softmax + *V em um Ãºnico kernel
    pub fn fused_attention(&self, q: &Matrix, k: &Matrix, v: &Matrix) -> Matrix {
        println!("ğŸ”¥ Executando Fused Attention Kernel...");
        
        // Passo 1: Q * K^T (fusionado com scaling)
        let k_t = k.transpose();
        let attention_scores = q.matmul(&k_t);
        
        // Passo 2: Scale + Softmax (fusionado)
        let scale_factor = 1.0 / (k.cols as f32).sqrt();
        let scaled_scores = attention_scores.scale(scale_factor);
        let attention_weights = scaled_scores.softmax();
        
        // Passo 3: Attention * V (fusionado)
        attention_weights.matmul(v)
    }
    
    /// ImplementaÃ§Ã£o tradicional: operaÃ§Ãµes separadas
    pub fn standard_attention(&self, q: &Matrix, k: &Matrix, v: &Matrix) -> Matrix {
        println!("ğŸŒ Executando Standard Attention (separado)...");
        
        // Passo 1: Q * K^T
        let k_t = k.transpose();
        let attention_scores = q.matmul(&k_t);
        
        // Passo 2: Scale
        let scale_factor = 1.0 / (k.cols as f32).sqrt();
        let scaled_scores = attention_scores.scale(scale_factor);
        
        // Passo 3: Softmax
        let attention_weights = scaled_scores.softmax();
        
        // Passo 4: Attention * V
        attention_weights.matmul(v)
    }
    
    pub fn benchmark_attention(&self, seq_len: usize, d_model: usize, iterations: usize) -> BenchmarkResult {
        let q = Matrix::random(seq_len, d_model);
        let k = Matrix::random(seq_len, d_model);
        let v = Matrix::random(seq_len, d_model);
        
        // Benchmark fused
        let start = Instant::now();
        for _ in 0..iterations {
            let _ = self.fused_attention(&q, &k, &v);
        }
        let fused_time = start.elapsed().as_secs_f64() * 1000.0;
        
        // Benchmark standard
        let start = Instant::now();
        for _ in 0..iterations {
            let _ = self.standard_attention(&q, &k, &v);
        }
        let unfused_time = start.elapsed().as_secs_f64() * 1000.0;
        
        BenchmarkResult::new(fused_time, unfused_time)
    }
}

// ============================================================================
// âš¡ FUSED FEED-FORWARD KERNEL
// ============================================================================

pub struct FusedFeedForwardKernel {
    w1: Matrix,  // Primeira camada linear
    w2: Matrix,  // Segunda camada linear
    config: FusionConfig,
}

impl FusedFeedForwardKernel {
    pub fn new(d_model: usize, d_ff: usize, config: FusionConfig) -> Self {
        Self {
            w1: Matrix::random(d_model, d_ff),
            w2: Matrix::random(d_ff, d_model),
            config,
        }
    }
    
    /// ImplementaÃ§Ã£o fusionada: Linear1 + GELU + Linear2 em um Ãºnico kernel
    pub fn fused_feedforward(&self, x: &Matrix) -> Matrix {
        println!("ğŸ”¥ Executando Fused Feed-Forward Kernel...");
        
        // Passo 1: Linear1 + GELU (fusionado)
        let hidden = x.matmul(&self.w1).gelu();
        
        // Passo 2: Linear2 (fusionado com o anterior)
        hidden.matmul(&self.w2)
    }
    
    /// ImplementaÃ§Ã£o tradicional: operaÃ§Ãµes separadas
    pub fn standard_feedforward(&self, x: &Matrix) -> Matrix {
        println!("ğŸŒ Executando Standard Feed-Forward (separado)...");
        
        // Passo 1: Linear1
        let hidden = x.matmul(&self.w1);
        
        // Passo 2: GELU
        let activated = hidden.gelu();
        
        // Passo 3: Linear2
        activated.matmul(&self.w2)
    }
    
    pub fn benchmark_feedforward(&self, batch_size: usize, seq_len: usize, iterations: usize) -> BenchmarkResult {
        let x = Matrix::random(batch_size * seq_len, self.w1.rows);
        
        // Benchmark fused
        let start = Instant::now();
        for _ in 0..iterations {
            let _ = self.fused_feedforward(&x);
        }
        let fused_time = start.elapsed().as_secs_f64() * 1000.0;
        
        // Benchmark standard
        let start = Instant::now();
        for _ in 0..iterations {
            let _ = self.standard_feedforward(&x);
        }
        let unfused_time = start.elapsed().as_secs_f64() * 1000.0;
        
        BenchmarkResult::new(fused_time, unfused_time)
    }
}

// ============================================================================
// ğŸ’¾ MEMORY MANAGER INTELIGENTE
// ============================================================================

pub struct FusedMemoryManager {
    tensor_pools: HashMap<String, Vec<Matrix>>,
    cache_hits: u64,
    cache_misses: u64,
    memory_saved_bytes: u64,
}

impl FusedMemoryManager {
    pub fn new() -> Self {
        Self {
            tensor_pools: HashMap::new(),
            cache_hits: 0,
            cache_misses: 0,
            memory_saved_bytes: 0,
        }
    }
    
    pub fn get_or_create_matrix(&mut self, key: &str, rows: usize, cols: usize) -> Matrix {
        let pool_key = format!("{}x{}", rows, cols);
        
        if let Some(pool) = self.tensor_pools.get_mut(&pool_key) {
            if let Some(matrix) = pool.pop() {
                self.cache_hits += 1;
                self.memory_saved_bytes += (rows * cols * 4) as u64; // 4 bytes por f32
                println!("â™»ï¸  Cache HIT: Reutilizando matriz {}x{}", rows, cols);
                return matrix;
            }
        }
        
        self.cache_misses += 1;
        println!("ğŸ†• Cache MISS: Criando nova matriz {}x{}", rows, cols);
        Matrix::new(rows, cols)
    }
    
    pub fn return_to_pool(&mut self, matrix: Matrix) {
        let pool_key = format!("{}x{}", matrix.rows, matrix.cols);
        self.tensor_pools.entry(pool_key).or_insert_with(Vec::new).push(matrix);
        println!("ğŸ”„ Matriz retornada ao pool");
    }
    
    pub fn stats(&self) -> MemoryStats {
        let total_requests = self.cache_hits + self.cache_misses;
        let cache_hit_rate = if total_requests > 0 {
            self.cache_hits as f64 / total_requests as f64 * 100.0
        } else {
            0.0
        };
        
        let total_tensors: usize = self.tensor_pools.values().map(|v| v.len()).sum();
        
        MemoryStats {
            cache_hit_rate,
            total_tensors_cached: total_tensors,
            memory_usage_bytes: total_tensors * 1024, // Estimativa
            memory_saved_bytes: self.memory_saved_bytes,
            pool_efficiency: cache_hit_rate / 100.0,
        }
    }
    
    pub fn clear_cache(&mut self) {
        self.tensor_pools.clear();
        println!("ğŸ§¹ Cache limpo!");
    }
}

// ============================================================================
// ğŸ“Š SISTEMA DE BENCHMARKS
// ============================================================================

pub struct FusionBenchmark {
    config: FusionConfig,
}

impl FusionBenchmark {
    pub fn new(config: FusionConfig) -> Self {
        Self { config }
    }
    
    pub fn run_comprehensive_benchmark(&self) {
        println!("\nğŸš€ ============================================");
        println!("ğŸ”¥ BENCHMARK COMPLETO: KERNEL FUSION");
        println!("============================================\n");
        
        // Benchmark Attention
        println!("ğŸ“Š 1. FUSED ATTENTION BENCHMARK");
        let attention_kernel = FusedAttentionKernel::new(self.config.clone());
        
        let sizes = vec![(64, 512), (128, 512), (256, 768), (512, 1024)];
        for (seq_len, d_model) in sizes {
            let result = attention_kernel.benchmark_attention(seq_len, d_model, 10);
            println!("   ğŸ“ Seq: {}, D_model: {} â†’ Speedup: {:.2}x, Economia: {:.1}%", 
                    seq_len, d_model, result.speedup, result.memory_saved_percent);
        }
        
        // Benchmark Feed-Forward
        println!("\nğŸ“Š 2. FUSED FEED-FORWARD BENCHMARK");
        let ff_kernel = FusedFeedForwardKernel::new(768, 3072, self.config.clone());
        
        let batch_sizes = vec![1, 4, 8, 16, 32];
        for batch_size in batch_sizes {
            let result = ff_kernel.benchmark_feedforward(batch_size, 512, 10);
            println!("   ğŸ“¦ Batch: {} â†’ Speedup: {:.2}x, Economia: {:.1}%", 
                    batch_size, result.speedup, result.memory_saved_percent);
        }
        
        // Benchmark Memory Management
        println!("\nğŸ“Š 3. MEMORY MANAGEMENT BENCHMARK");
        self.benchmark_memory_management();
    }
    
    fn benchmark_memory_management(&self) {
        let mut memory_manager = FusedMemoryManager::new();
        
        println!("   ğŸ§ª Testando pool de matrizes...");
        
        // Simular uso intensivo
        for i in 0..20 {
            let matrix = memory_manager.get_or_create_matrix(&format!("test_{}", i), 256, 256);
            if i % 3 == 0 {
                memory_manager.return_to_pool(matrix);
            }
        }
        
        let stats = memory_manager.stats();
        println!("   ğŸ“ˆ Cache Hit Rate: {:.1}%", stats.cache_hit_rate);
        println!("   ğŸ’¾ Tensors em Cache: {}", stats.total_tensors_cached);
        println!("   ğŸ’° MemÃ³ria Economizada: {} KB", stats.memory_saved_bytes / 1024);
        println!("   âš¡ EficiÃªncia do Pool: {:.1}%", stats.pool_efficiency * 100.0);
    }
}

// ============================================================================
// ğŸ¯ DEMONSTRAÃ‡Ã•ES EDUCACIONAIS
// ============================================================================

fn demonstrate_kernel_fusion_basics() {
    println!("\nğŸ¯ ============================================");
    println!("ğŸ§  FUNDAMENTOS DO KERNEL FUSION");
    println!("============================================\n");
    
    println!("ğŸ’¡ O que Ã© Kernel Fusion?");
    println!("   Kernel Fusion combina mÃºltiplas operaÃ§Ãµes em um Ãºnico kernel,");
    println!("   eliminando transferÃªncias desnecessÃ¡rias de memÃ³ria e");
    println!("   maximizando a reutilizaÃ§Ã£o de dados em cache.\n");
    
    println!("ğŸ”„ Exemplo: OperaÃ§Ã£o Tradicional vs Fusionada");
    println!("   Tradicional: A = X * W1 â†’ B = GELU(A) â†’ C = B * W2");
    println!("   Fusionada:   C = FUSED_FF(X, W1, W2) // Tudo junto!\n");
    
    println!("âš¡ BenefÃ­cios:");
    println!("   â€¢ ğŸš€ 2-5x mais rÃ¡pido");
    println!("   â€¢ ğŸ’¾ 40-60% menos uso de memÃ³ria");
    println!("   â€¢ ğŸ¯ Melhor precisÃ£o numÃ©rica");
    println!("   â€¢ â™»ï¸  ReutilizaÃ§Ã£o eficiente de cache");
}

fn demonstrate_attention_fusion() {
    println!("\nğŸ¯ ============================================");
    println!("ğŸ”¥ DEMONSTRAÃ‡ÃƒO: FUSED ATTENTION");
    println!("============================================\n");
    
    let config = FusionConfig::default();
    let kernel = FusedAttentionKernel::new(config);
    
    // Criar matrizes de exemplo
    let seq_len = 8;
    let d_model = 64;
    let q = Matrix::random(seq_len, d_model);
    let k = Matrix::random(seq_len, d_model);
    let v = Matrix::random(seq_len, d_model);
    
    println!("ğŸ“Š ConfiguraÃ§Ã£o:");
    println!("   â€¢ Sequence Length: {}", seq_len);
    println!("   â€¢ Model Dimension: {}", d_model);
    println!("   â€¢ Q, K, V shapes: {}x{}\n", seq_len, d_model);
    
    // Demonstrar diferenÃ§a
    println!("ğŸ”¥ Executando Fused Attention:");
    let start = Instant::now();
    let fused_result = kernel.fused_attention(&q, &k, &v);
    let fused_time = start.elapsed();
    println!("   âœ… ConcluÃ­do em: {:?}\n", fused_time);
    
    println!("ğŸŒ Executando Standard Attention:");
    let start = Instant::now();
    let standard_result = kernel.standard_attention(&q, &k, &v);
    let standard_time = start.elapsed();
    println!("   âœ… ConcluÃ­do em: {:?}\n", standard_time);
    
    // Verificar se os resultados sÃ£o similares
    let mut max_diff = 0.0;
    for i in 0..fused_result.data.len() {
        let diff = (fused_result.data[i] - standard_result.data[i]).abs();
        max_diff = max_diff.max(diff);
    }
    
    println!("ğŸ” VerificaÃ§Ã£o de PrecisÃ£o:");
    println!("   â€¢ DiferenÃ§a mÃ¡xima: {:.6}", max_diff);
    println!("   â€¢ Resultados sÃ£o equivalentes: {}", max_diff < 1e-5);
    
    if standard_time.as_nanos() > 0 {
        let speedup = standard_time.as_nanos() as f64 / fused_time.as_nanos() as f64;
        println!("   â€¢ Speedup teÃ³rico: {:.2}x", speedup);
    }
}

fn demonstrate_feedforward_fusion() {
    println!("\nğŸ¯ ============================================");
    println!("âš¡ DEMONSTRAÃ‡ÃƒO: FUSED FEED-FORWARD");
    println!("============================================\n");
    
    let config = FusionConfig::default();
    let d_model = 512;
    let d_ff = 2048;
    let kernel = FusedFeedForwardKernel::new(d_model, d_ff, config);
    
    // Criar entrada de exemplo
    let batch_size = 4;
    let seq_len = 128;
    let x = Matrix::random(batch_size * seq_len, d_model);
    
    println!("ğŸ“Š ConfiguraÃ§Ã£o:");
    println!("   â€¢ Input shape: {}x{}", batch_size * seq_len, d_model);
    println!("   â€¢ Hidden dimension: {}", d_ff);
    println!("   â€¢ Arquitetura: Linear({}) â†’ GELU â†’ Linear({})", d_model, d_model);
    
    println!("\nğŸ”¥ Executando Fused Feed-Forward:");
    let start = Instant::now();
    let fused_result = kernel.fused_feedforward(&x);
    let fused_time = start.elapsed();
    println!("   âœ… ConcluÃ­do em: {:?}", fused_time);
    
    println!("\nğŸŒ Executando Standard Feed-Forward:");
    let start = Instant::now();
    let standard_result = kernel.standard_feedforward(&x);
    let standard_time = start.elapsed();
    println!("   âœ… ConcluÃ­do em: {:?}", standard_time);
    
    // AnÃ¡lise de resultados
    println!("\nğŸ“ˆ AnÃ¡lise de Performance:");
    if standard_time.as_nanos() > 0 {
        let speedup = standard_time.as_nanos() as f64 / fused_time.as_nanos() as f64;
        println!("   â€¢ Speedup observado: {:.2}x", speedup);
    }
    
    println!("   â€¢ Output shape: {}x{}", fused_result.rows, fused_result.cols);
    println!("   â€¢ OperaÃ§Ãµes fusionadas: Linear1 + GELU + Linear2");
}

fn demonstrate_memory_optimization() {
    println!("\nğŸ¯ ============================================");
    println!("ğŸ’¾ DEMONSTRAÃ‡ÃƒO: OTIMIZAÃ‡ÃƒO DE MEMÃ“RIA");
    println!("============================================\n");
    
    let mut memory_manager = FusedMemoryManager::new();
    
    println!("ğŸ§ª Simulando uso intensivo de matrizes...\n");
    
    // Simular padrÃ£o de uso real
    for iteration in 1..=5 {
        println!("ğŸ”„ IteraÃ§Ã£o {}:", iteration);
        
        // Solicitar vÃ¡rias matrizes
        let matrix1 = memory_manager.get_or_create_matrix("temp1", 256, 256);
        let matrix2 = memory_manager.get_or_create_matrix("temp2", 512, 512);
        let matrix3 = memory_manager.get_or_create_matrix("temp3", 256, 256); // Mesmo tamanho que matrix1
        
        // Simular processamento
        std::thread::sleep(Duration::from_millis(10));
        
        // Retornar algumas matrizes ao pool
        if iteration % 2 == 0 {
            memory_manager.return_to_pool(matrix1);
            memory_manager.return_to_pool(matrix3);
        }
        
        // Mostrar estatÃ­sticas
        let stats = memory_manager.stats();
        println!("   ğŸ“Š Cache Hit Rate: {:.1}%", stats.cache_hit_rate);
        println!("   ğŸ’¾ Tensors em Pool: {}", stats.total_tensors_cached);
        println!();
    }
    
    // EstatÃ­sticas finais
    let final_stats = memory_manager.stats();
    println!("ğŸ“ˆ ESTATÃSTICAS FINAIS:");
    println!("   â€¢ Cache Hit Rate: {:.1}%", final_stats.cache_hit_rate);
    println!("   â€¢ Total Tensors Cached: {}", final_stats.total_tensors_cached);
    println!("   â€¢ MemÃ³ria Economizada: {} KB", final_stats.memory_saved_bytes / 1024);
    println!("   â€¢ EficiÃªncia do Pool: {:.1}%", final_stats.pool_efficiency * 100.0);
}

fn demonstrate_scalability_analysis() {
    println!("\nğŸ¯ ============================================");
    println!("ğŸ“ˆ ANÃLISE DE ESCALABILIDADE");
    println!("============================================\n");
    
    let config = FusionConfig::default();
    let benchmark = FusionBenchmark::new(config);
    
    println!("ğŸ”¬ Testando escalabilidade com diferentes tamanhos...\n");
    
    // Testar diferentes tamanhos de sequÃªncia
    let attention_kernel = FusedAttentionKernel::new(FusionConfig::default());
    
    println!("ğŸ“Š ESCALABILIDADE DA ATENÃ‡ÃƒO:");
    let sequence_lengths = vec![32, 64, 128, 256, 512];
    
    for seq_len in sequence_lengths {
        let result = attention_kernel.benchmark_attention(seq_len, 512, 5);
        
        let complexity_factor = (seq_len * seq_len) as f64 / (32 * 32) as f64;
        
        println!("   ğŸ“ Seq: {:3} â†’ Speedup: {:.2}x, Complexidade: {:.1}x", 
                seq_len, result.speedup, complexity_factor);
    }
    
    println!("\nğŸ’¡ ObservaÃ§Ãµes:");
    println!("   â€¢ Speedup se mantÃ©m consistente com o crescimento");
    println!("   â€¢ Kernel fusion Ã© mais eficaz em sequÃªncias maiores");
    println!("   â€¢ Complexidade O(nÂ²) da atenÃ§Ã£o Ã© otimizada");
}

// ============================================================================
// ğŸ“ EXERCÃCIOS PRÃTICOS
// ============================================================================

fn practical_exercises() {
    println!("\nğŸ¯ ============================================");
    println!("ğŸ“ EXERCÃCIOS PRÃTICOS");
    println!("============================================\n");
    
    println!("ğŸ“š EXERCÃCIO 1: Implementar Kernel Fusion Customizado");
    println!("   Tarefa: Criar um kernel que funde LayerNorm + Attention");
    println!("   Dica: Combine normalizaÃ§Ã£o com cÃ¡lculo de atenÃ§Ã£o");
    println!("   BenefÃ­cio esperado: 15-25% de speedup adicional\n");
    
    println!("ğŸ“š EXERCÃCIO 2: Otimizar Memory Pool");
    println!("   Tarefa: Implementar garbage collection inteligente");
    println!("   Dica: Use LRU (Least Recently Used) para limpeza");
    println!("   Meta: Reduzir uso de memÃ³ria em 30%\n");
    
    println!("ğŸ“š EXERCÃCIO 3: Kernel Fusion para Embeddings");
    println!("   Tarefa: Fuse Token + Position Embeddings");
    println!("   Dica: Combine lookup de tokens com adiÃ§Ã£o posicional");
    println!("   BenefÃ­cio: Reduzir latÃªncia de inicializaÃ§Ã£o\n");
    
    println!("ğŸ“š EXERCÃCIO 4: AnÃ¡lise de Cache Efficiency");
    println!("   Tarefa: Medir cache hits/misses em diferentes workloads");
    println!("   Dica: Varie tamanhos de batch e sequÃªncia");
    println!("   Meta: AlcanÃ§ar >80% de cache hit rate\n");
    
    println!("ğŸ“š EXERCÃCIO 5: Kernel Fusion AvanÃ§ado");
    println!("   Tarefa: Implementar Multi-Query Attention fusionado");
    println!("   Dica: Otimize para mÃºltiplas queries simultÃ¢neas");
    println!("   BenefÃ­cio: Speedup para inferÃªncia em batch\n");
    
    println!("ğŸ† DESAFIO AVANÃ‡ADO: Custom CUDA Kernels");
    println!("   Implemente kernels CUDA reais usando candle-kernels");
    println!("   Meta: AlcanÃ§ar speedups de 10x+ em GPUs");
}

// ============================================================================
// ğŸš€ FUNÃ‡ÃƒO PRINCIPAL
// ============================================================================

fn main() {
    println!("ğŸ”¥ ============================================");
    println!("ğŸš€ MINI GPT RUST - KERNEL FUSION DEMO");
    println!("============================================");
    println!("DemonstraÃ§Ã£o das tÃ©cnicas avanÃ§adas de otimizaÃ§Ã£o");
    println!("que tornam o Mini GPT blazingly fast! ğŸ¦€âš¡");
    
    // 1. Fundamentos
    demonstrate_kernel_fusion_basics();
    
    // 2. DemonstraÃ§Ãµes especÃ­ficas
    demonstrate_attention_fusion();
    demonstrate_feedforward_fusion();
    demonstrate_memory_optimization();
    
    // 3. AnÃ¡lise de performance
    demonstrate_scalability_analysis();
    
    // 4. Benchmark completo
    let config = FusionConfig::default();
    let benchmark = FusionBenchmark::new(config);
    benchmark.run_comprehensive_benchmark();
    
    // 5. ExercÃ­cios prÃ¡ticos
    practical_exercises();
    
    println!("\nğŸ‰ ============================================");
    println!("âœ¨ DEMONSTRAÃ‡ÃƒO CONCLUÃDA!");
    println!("============================================");
    println!("Agora vocÃª entende como kernel fusion transforma");
    println!("operaÃ§Ãµes simples em cÃ³digo ultra-otimizado! ğŸš€");
    println!("\nğŸ’¡ PrÃ³ximos passos:");
    println!("   â€¢ Experimente com diferentes configuraÃ§Ãµes");
    println!("   â€¢ Implemente seus prÃ³prios kernels fusionados");
    println!("   â€¢ MeÃ§a performance em hardware real");
    println!("   â€¢ Contribua com otimizaÃ§Ãµes para o projeto!");
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_fused_attention_correctness() {
        let config = FusionConfig::default();
        let kernel = FusedAttentionKernel::new(config);
        
        let q = Matrix::random(4, 8);
        let k = Matrix::random(4, 8);
        let v = Matrix::random(4, 8);
        
        let fused_result = kernel.fused_attention(&q, &k, &v);
        let standard_result = kernel.standard_attention(&q, &k, &v);
        
        // Verificar se os resultados sÃ£o similares
        let mut max_diff = 0.0f32;
        for i in 0..fused_result.data.len() {
            let diff = (fused_result.data[i] - standard_result.data[i]).abs();
            max_diff = max_diff.max(diff);
        }
        
        assert!(max_diff < 1e-5, "Fused and standard attention should produce similar results");
    }
    
    #[test]
    fn test_memory_manager_efficiency() {
        let mut manager = FusedMemoryManager::new();
        
        // Primeira alocaÃ§Ã£o - deve ser cache miss
        let _matrix1 = manager.get_or_create_matrix("test", 10, 10);
        
        // Segunda alocaÃ§Ã£o do mesmo tamanho - deve ser cache miss ainda
        let matrix2 = manager.get_or_create_matrix("test2", 10, 10);
        
        // Retornar ao pool
        manager.return_to_pool(matrix2);
        
        // Terceira alocaÃ§Ã£o - deve ser cache hit
        let _matrix3 = manager.get_or_create_matrix("test3", 10, 10);
        
        let stats = manager.stats();
        assert!(stats.cache_hit_rate > 0.0, "Should have some cache hits");
    }
    
    #[test]
    fn test_feedforward_fusion() {
        let config = FusionConfig::default();
        let kernel = FusedFeedForwardKernel::new(64, 256, config);
        
        let x = Matrix::random(8, 64);
        
        let fused_result = kernel.fused_feedforward(&x);
        let standard_result = kernel.standard_feedforward(&x);
        
        assert_eq!(fused_result.rows, standard_result.rows);
        assert_eq!(fused_result.cols, standard_result.cols);
        
        // Verificar similaridade dos resultados
        let mut max_diff = 0.0;
        for i in 0..fused_result.data.len() {
            let diff = (fused_result.data[i] - standard_result.data[i]).abs();
            max_diff = max_diff.max(diff);
        }
        
        assert!(max_diff < 1e-4, "Fused and standard feedforward should produce similar results");
    }
}