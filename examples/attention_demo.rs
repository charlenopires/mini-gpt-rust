//! # üß† Demonstra√ß√£o do Mecanismo de Aten√ß√£o
//!
//! Este exemplo demonstra como o mecanismo de Self-Attention funciona na pr√°tica,
//! mostrando desde conceitos b√°sicos at√© implementa√ß√µes avan√ßadas com m√∫ltiplas cabe√ßas.
//!
//! ## üéØ O que voc√™ vai aprender:
//! - Como funciona o Scaled Dot-Product Attention
//! - Visualiza√ß√£o de matrizes Q, K, V
//! - Multi-Head Attention em a√ß√£o
//! - Padr√µes de aten√ß√£o em sequ√™ncias reais
//! - An√°lise de performance e complexidade

use candle_core::{Device, Tensor, DType};
use candle_nn::{VarBuilder, VarMap};
use std::collections::HashMap;
use std::time::Instant;

// Importa os m√≥dulos do projeto principal
// Ajuste os caminhos conforme a estrutura do seu projeto
use std::path::PathBuf;

// Para compilar este exemplo, voc√™ precisa ter acesso aos m√≥dulos
// ou implementar vers√µes simplificadas das estruturas

// Estruturas simplificadas para demonstra√ß√£o
// Em um projeto real, estas viriam de mini_gpt_rust::attention
struct SelfAttention {
    n_embd: usize,
    n_head: usize,
    head_dim: usize,
    dropout: f32,
}

struct MultiHeadAttention {
    attention: SelfAttention,
}

impl SelfAttention {
    fn new(n_embd: usize, n_head: usize, dropout: f32, _vb: VarBuilder) -> candle_core::Result<Self> {
        Ok(Self {
            n_embd,
            n_head,
            head_dim: n_embd / n_head,
            dropout,
        })
    }
    
    fn forward(&self, x: &Tensor, _mask: Option<&Tensor>) -> candle_core::Result<Tensor> {
        // Implementa√ß√£o simplificada para demonstra√ß√£o
        Ok(x.clone())
    }
    
    fn get_qkv_tensors(&self, x: &Tensor) -> candle_core::Result<(Tensor, Tensor, Tensor)> {
        // Simula proje√ß√µes Q, K, V
        let q = x.clone();
        let k = x.clone();
        let v = x.clone();
        Ok((q, k, v))
    }
}

impl MultiHeadAttention {
    fn new(n_embd: usize, n_head: usize, dropout: f32, vb: VarBuilder) -> candle_core::Result<Self> {
        let attention = SelfAttention::new(n_embd, n_head, dropout, vb)?;
        Ok(Self { attention })
    }
    
    fn forward(&self, x: &Tensor, mask: Option<&Tensor>) -> candle_core::Result<Tensor> {
        self.attention.forward(x, mask)
    }
    
    fn get_qkv_tensors(&self, x: &Tensor) -> candle_core::Result<(Tensor, Tensor, Tensor)> {
        self.attention.get_qkv_tensors(x)
    }
}

/// üé≠ Estrutura para demonstrar conceitos de aten√ß√£o
struct AttentionDemo {
    device: Device,
    vocab_size: usize,
    seq_len: usize,
    n_embd: usize,
    n_head: usize,
}

impl AttentionDemo {
    /// üèóÔ∏è Cria uma nova inst√¢ncia de demonstra√ß√£o
    fn new() -> candle_core::Result<Self> {
        let device = Device::Cpu; // Use Device::cuda(0)? para GPU
        
        Ok(Self {
            device,
            vocab_size: 1000,
            seq_len: 8,
            n_embd: 64,
            n_head: 4,
        })
    }
    
    /// üîç Demonstra o conceito b√°sico de aten√ß√£o com exemplo simples
    fn demo_basic_attention_concept(&self) -> candle_core::Result<()> {
        println!("\nüîç === CONCEITO B√ÅSICO DE ATEN√á√ÉO ===");
        println!("Vamos simular como a aten√ß√£o funciona com uma frase simples:");
        println!("Frase: 'O gato subiu no telhado'");
        
        // Simula tokens da frase
        let tokens = vec!["O", "gato", "subiu", "no", "telhado"];
        let seq_len = tokens.len();
        
        println!("\nüìä Tokens indexados:");
        for (i, token) in tokens.iter().enumerate() {
            println!("  {}: {}", i, token);
        }
        
        // Cria embeddings simulados (normalmente seriam aprendidos)
        let embeddings = Tensor::randn(0f32, 1f32, (seq_len, self.n_embd), &self.device)?;
        
        println!("\nüßÆ Dimens√µes dos embeddings: {:?}", embeddings.shape());
        println!("Cada token √© representado por um vetor de {} dimens√µes", self.n_embd);
        
        // Demonstra o c√°lculo de similaridade b√°sico
        self.demo_similarity_calculation(&embeddings, &tokens)?;
        
        Ok(())
    }
    
    /// üìê Demonstra c√°lculo de similaridade entre tokens
    fn demo_similarity_calculation(&self, embeddings: &Tensor, tokens: &[&str]) -> candle_core::Result<()> {
        println!("\nüìê === C√ÅLCULO DE SIMILARIDADE ===");
        
        // Calcula produto escalar entre todos os pares de tokens
        let similarity_matrix = embeddings.matmul(&embeddings.t()?)?;
        let similarity_data = similarity_matrix.to_vec2::<f32>()?;
        
        println!("\nüî¢ Matriz de Similaridade (produto escalar):");
        print!("     ");
        for token in tokens {
            print!("{:>8}", token);
        }
        println!();
        
        for (i, row) in similarity_data.iter().enumerate() {
            print!("{:>4} ", tokens[i]);
            for &value in row {
                print!("{:>8.2}", value);
            }
            println!();
        }
        
        println!("\nüí° Interpreta√ß√£o:");
        println!("  - Valores maiores = tokens mais 'similares' ou relacionados");
        println!("  - Diagonal = auto-similaridade (sempre alta)");
        println!("  - Off-diagonal = rela√ß√µes entre diferentes tokens");
        
        Ok(())
    }
    
    /// üéØ Demonstra Self-Attention completo com Q, K, V
    fn demo_self_attention(&self) -> candle_core::Result<()> {
        println!("\nüéØ === SELF-ATTENTION COMPLETO ===");
        
        // Cria VarBuilder para inicializar pesos
        let varmap = VarMap::new();
        let vb = VarBuilder::from_varmap(&varmap, DType::F32, &self.device);
        
        // Inicializa camada de Self-Attention
        let attention = SelfAttention::new(self.n_embd, self.n_head, 0.1, vb.pp("attention"))?;
        
        // Cria sequ√™ncia de entrada simulada
        let batch_size = 1;
        let input = Tensor::randn(0f32, 1f32, (batch_size, self.seq_len, self.n_embd), &self.device)?;
        
        println!("üìä Entrada: shape = {:?}", input.shape());
        println!("  - Batch size: {}", batch_size);
        println!("  - Sequence length: {}", self.seq_len);
        println!("  - Embedding dimension: {}", self.n_embd);
        
        // Extrai matrizes Q, K, V
        let (q, k, v) = attention.get_qkv_tensors(&input)?;
        
        println!("\nüîç Matrizes Q, K, V:");
        println!("  - Query (Q): {:?} - 'O que eu quero saber?'", q.shape());
        println!("  - Key (K):   {:?} - 'O que eu tenho para oferecer?'", k.shape());
        println!("  - Value (V): {:?} - 'Minha informa√ß√£o real'", v.shape());
        
        // Aplica Self-Attention
        let start_time = Instant::now();
        let output = attention.forward(&input, None)?;
        let duration = start_time.elapsed();
        
        println!("\n‚úÖ Sa√≠da da Self-Attention:");
        println!("  - Shape: {:?}", output.shape());
        println!("  - Tempo de execu√ß√£o: {:?}", duration);
        
        // Analisa mudan√ßas na representa√ß√£o
        self.analyze_attention_output(&input, &output)?;
        
        Ok(())
    }
    
    /// üìä Analisa como a aten√ß√£o modifica as representa√ß√µes
    fn analyze_attention_output(&self, input: &Tensor, output: &Tensor) -> candle_core::Result<()> {
        println!("\nüìä === AN√ÅLISE DA TRANSFORMA√á√ÉO ===");
        
        // Calcula normas dos vetores antes e depois
        let input_norms = input.sqr()?.sum_keepdim(2)?.sqrt()?;
        let output_norms = output.sqr()?.sum_keepdim(2)?.sqrt()?;
        
        let input_norms_data = input_norms.squeeze(0)?.to_vec1::<f32>()?;
        let output_norms_data = output_norms.squeeze(0)?.to_vec1::<f32>()?;
        
        println!("üî¢ Normas dos vetores por posi√ß√£o:");
        println!("Pos  | Antes    | Depois   | Mudan√ßa");
        println!("-----|----------|----------|----------");
        
        for i in 0..self.seq_len {
            let before = input_norms_data[i];
            let after = output_norms_data[i];
            let change = ((after - before) / before * 100.0);
            println!("{:3}  | {:8.3} | {:8.3} | {:+7.2}%", i, before, after, change);
        }
        
        // Calcula similaridade entre entrada e sa√≠da
        let similarity = self.calculate_cosine_similarity(input, output)?;
        println!("\nüéØ Similaridade cosseno m√©dia: {:.4}", similarity);
        println!("  - 1.0 = id√™ntico, 0.0 = ortogonal, -1.0 = oposto");
        
        if similarity > 0.8 {
            println!("  ‚úÖ Alta preserva√ß√£o da informa√ß√£o original");
        } else if similarity > 0.5 {
            println!("  ‚ö†Ô∏è  Transforma√ß√£o moderada da informa√ß√£o");
        } else {
            println!("  üîÑ Transforma√ß√£o significativa da informa√ß√£o");
        }
        
        Ok(())
    }
    
    /// üßÆ Calcula similaridade cosseno entre tensores
    fn calculate_cosine_similarity(&self, a: &Tensor, b: &Tensor) -> candle_core::Result<f32> {
        let a_flat = a.flatten_all()?;
        let b_flat = b.flatten_all()?;
        
        let dot_product = (&a_flat * &b_flat)?.sum_all()?.to_scalar::<f32>()?;
        let norm_a = a_flat.sqr()?.sum_all()?.sqrt()?.to_scalar::<f32>()?;
        let norm_b = b_flat.sqr()?.sum_all()?.sqrt()?.to_scalar::<f32>()?;
        
        Ok(dot_product / (norm_a * norm_b))
    }
    
    /// üëÅÔ∏è Demonstra Multi-Head Attention
    fn demo_multi_head_attention(&self) -> candle_core::Result<()> {
        println!("\nüëÅÔ∏è === MULTI-HEAD ATTENTION ===");
        println!("M√∫ltiplas 'perspectivas' de aten√ß√£o trabalhando em paralelo");
        
        let varmap = VarMap::new();
        let vb = VarBuilder::from_varmap(&varmap, DType::F32, &self.device);
        
        let mha = MultiHeadAttention::new(self.n_embd, self.n_head, 0.1, vb.pp("mha"))?;
        
        let batch_size = 2;
        let input = Tensor::randn(0f32, 1f32, (batch_size, self.seq_len, self.n_embd), &self.device)?;
        
        println!("\nüèóÔ∏è Configura√ß√£o Multi-Head:");
        println!("  - N√∫mero de cabe√ßas: {}", self.n_head);
        println!("  - Dimens√£o por cabe√ßa: {}", self.n_embd / self.n_head);
        println!("  - Dimens√£o total: {}", self.n_embd);
        
        // Mede performance
        let start_time = Instant::now();
        let output = mha.forward(&input, None)?;
        let duration = start_time.elapsed();
        
        println!("\n‚ö° Performance:");
        println!("  - Tempo de execu√ß√£o: {:?}", duration);
        println!("  - Throughput: {:.2} tokens/ms", 
                (batch_size * self.seq_len) as f64 / duration.as_millis() as f64);
        
        // Analisa complexidade
        self.analyze_complexity()?;
        
        Ok(())
    }
    
    /// üìà Analisa complexidade computacional
    fn analyze_complexity(&self) -> candle_core::Result<()> {
        println!("\nüìà === AN√ÅLISE DE COMPLEXIDADE ===");
        
        let seq_len = self.seq_len;
        let d_model = self.n_embd;
        
        // Complexidade do Self-Attention: O(n¬≤d)
        let attention_ops = seq_len * seq_len * d_model;
        
        // Complexidade das proje√ß√µes lineares: O(nd¬≤)
        let linear_ops = 4 * seq_len * d_model * d_model; // Q, K, V, Output
        
        println!("üßÆ Opera√ß√µes computacionais:");
        println!("  - Self-Attention: {} ops (O(n¬≤d))", attention_ops);
        println!("  - Proje√ß√µes lineares: {} ops (O(nd¬≤))", linear_ops);
        println!("  - Total: {} ops", attention_ops + linear_ops);
        
        println!("\nüìä Escalabilidade:");
        for &n in &[16, 32, 64, 128, 256, 512] {
            let attention_cost = n * n * d_model;
            let linear_cost = 4 * n * d_model * d_model;
            let total_cost = attention_cost + linear_cost;
            
            println!("  seq_len={:3}: {:>10} ops ({:.1}x)", 
                    n, total_cost, total_cost as f32 / (attention_ops + linear_ops) as f32);
        }
        
        Ok(())
    }
    
    /// üé® Demonstra padr√µes de aten√ß√£o com m√°scara causal
    fn demo_causal_attention(&self) -> candle_core::Result<()> {
        println!("\nüé® === ATEN√á√ÉO CAUSAL (AUTOREGRESSIVE) ===");
        println!("Impede que tokens 'vejam o futuro' durante o treinamento");
        
        // Cria m√°scara causal (triangular inferior)
        let mask = self.create_causal_mask(self.seq_len)?;
        
        println!("\nüîí M√°scara Causal (1=permitido, 0=bloqueado):");
        let mask_data = mask.to_vec2::<f32>()?;
        
        print!("     ");
        for i in 0..self.seq_len {
            print!("{:3}", i);
        }
        println!();
        
        for (i, row) in mask_data.iter().enumerate() {
            print!("{:3}: ", i);
            for &value in row {
                if value > 0.5 {
                    print!(" ‚úì ");
                } else {
                    print!(" ‚úó ");
                }
            }
            println!();
        }
        
        println!("\nüí° Interpreta√ß√£o:");
        println!("  - Token na posi√ß√£o i s√≥ pode 'ver' tokens nas posi√ß√µes 0..=i");
        println!("  - Isso simula gera√ß√£o sequencial durante infer√™ncia");
        println!("  - Previne vazamento de informa√ß√£o do futuro");
        
        Ok(())
    }
    
    /// üîí Cria m√°scara causal triangular
    fn create_causal_mask(&self, size: usize) -> candle_core::Result<Tensor> {
        let mut mask_data = vec![vec![0.0f32; size]; size];
        
        for i in 0..size {
            for j in 0..=i {
                mask_data[i][j] = 1.0;
            }
        }
        
        let flat_data: Vec<f32> = mask_data.into_iter().flatten().collect();
        Tensor::from_vec(flat_data, (size, size), &self.device)
    }
    
    /// üèÉ‚Äç‚ôÇÔ∏è Executa benchmark de performance
    fn benchmark_attention(&self) -> candle_core::Result<()> {
        println!("\nüèÉ‚Äç‚ôÇÔ∏è === BENCHMARK DE PERFORMANCE ===");
        
        let varmap = VarMap::new();
        let vb = VarBuilder::from_varmap(&varmap, DType::F32, &self.device);
        
        let configs = vec![
            ("Pequeno", 32, 4, 64),
            ("M√©dio", 64, 8, 128),
            ("Grande", 128, 12, 256),
        ];
        
        println!("\n‚è±Ô∏è  Resultados do Benchmark:");
        println!("Config   | Seq Len | Heads | Embd | Tempo (ms) | Throughput");
        println!("---------|---------|-------|------|------------|------------");
        
        for (name, seq_len, n_head, n_embd) in configs {
            let attention = SelfAttention::new(n_embd, n_head, 0.0, vb.pp(&format!("bench_{}", name)))?;
            let input = Tensor::randn(0f32, 1f32, (1, seq_len, n_embd), &self.device)?;
            
            // Aquecimento
            for _ in 0..3 {
                let _ = attention.forward(&input, None)?;
            }
            
            // Benchmark real
            let start = Instant::now();
            let iterations = 10;
            
            for _ in 0..iterations {
                let _ = attention.forward(&input, None)?;
            }
            
            let duration = start.elapsed();
            let avg_time = duration.as_millis() as f32 / iterations as f32;
            let throughput = seq_len as f32 / avg_time * 1000.0;
            
            println!("{:8} | {:7} | {:5} | {:4} | {:10.2} | {:8.0} tok/s", 
                    name, seq_len, n_head, n_embd, avg_time, throughput);
        }
        
        Ok(())
    }
}

/// üéØ Exerc√≠cios pr√°ticos para aprofundar o entendimento
struct AttentionExercises;

impl AttentionExercises {
    /// üìù Exerc√≠cio 1: An√°lise de padr√µes de aten√ß√£o
    fn exercise_attention_patterns() {
        println!("\nüìù === EXERC√çCIO 1: PADR√ïES DE ATEN√á√ÉO ===");
        println!("\nüéØ Objetivo: Entender como diferentes tipos de texto geram padr√µes distintos");
        
        let patterns = vec![
            ("Narrativa", "Era uma vez um rei que vivia em um castelo"),
            ("T√©cnico", "A fun√ß√£o retorna um valor booleano verdadeiro"),
            ("Poesia", "Lua cheia brilha sobre o mar sereno"),
        ];
        
        println!("\nüìä Padr√µes esperados por tipo de texto:");
        for (tipo, exemplo) in patterns {
            println!("\nüî∏ {}: '{}'", tipo, exemplo);
            match tipo {
                "Narrativa" => {
                    println!("  - Aten√ß√£o forte entre sujeito-verbo-objeto");
                    println!("  - Refer√™ncias pronominais conectam a entidades");
                    println!("  - Sequ√™ncia temporal de eventos");
                },
                "T√©cnico" => {
                    println!("  - Aten√ß√£o em termos t√©cnicos espec√≠ficos");
                    println!("  - Rela√ß√µes funcionais entre conceitos");
                    println!("  - Estrutura l√≥gica mais que temporal");
                },
                "Poesia" => {
                    println!("  - Aten√ß√£o em palavras com carga emocional");
                    println!("  - Conex√µes sem√¢nticas e sonoras");
                    println!("  - Padr√µes r√≠tmicos e m√©tricos");
                },
                _ => {}
            }
        }
        
        println!("\nüí° Experimento sugerido:");
        println!("  1. Implemente visualiza√ß√£o de matrizes de aten√ß√£o");
        println!("  2. Compare padr√µes entre diferentes tipos de texto");
        println!("  3. Identifique cabe√ßas especializadas em diferentes aspectos");
    }
    
    /// üî¨ Exerc√≠cio 2: Otimiza√ß√£o de performance
    fn exercise_performance_optimization() {
        println!("\nüî¨ === EXERC√çCIO 2: OTIMIZA√á√ÉO DE PERFORMANCE ===");
        println!("\nüéØ Objetivo: Implementar otimiza√ß√µes para acelerar a aten√ß√£o");
        
        println!("\n‚ö° T√©cnicas de otimiza√ß√£o:");
        println!("\n1. üßÆ Flash Attention:");
        println!("   - Reduz uso de mem√≥ria de O(n¬≤) para O(n)");
        println!("   - Usa tiling e recomputa√ß√£o inteligente");
        println!("   - Speedup de 2-4x em sequ√™ncias longas");
        
        println!("\n2. üéØ Sparse Attention:");
        println!("   - S√≥ computa aten√ß√£o para subconjunto de posi√ß√µes");
        println!("   - Padr√µes: local, strided, random");
        println!("   - Reduz complexidade de O(n¬≤) para O(n‚àön)");
        
        println!("\n3. üì¶ Quantiza√ß√£o:");
        println!("   - FP16 ou INT8 em vez de FP32");
        println!("   - Reduz uso de mem√≥ria e aumenta throughput");
        println!("   - Cuidado com precis√£o num√©rica");
        
        println!("\n4. üîÑ Kernel Fusion:");
        println!("   - Combina opera√ß√µes em um √∫nico kernel GPU");
        println!("   - Reduz transfer√™ncias de mem√≥ria");
        println!("   - Implementa√ß√£o espec√≠fica para hardware");
        
        println!("\nüí° Implementa√ß√£o sugerida:");
        println!("  1. Benchmark baseline com implementa√ß√£o atual");
        println!("  2. Implemente uma das t√©cnicas acima");
        println!("  3. Me√ßa speedup e uso de mem√≥ria");
        println!("  4. Analise trade-offs entre velocidade e precis√£o");
    }
    
    /// üé≠ Exerc√≠cio 3: Visualiza√ß√£o de aten√ß√£o
    fn exercise_attention_visualization() {
        println!("\nüé≠ === EXERC√çCIO 3: VISUALIZA√á√ÉO DE ATEN√á√ÉO ===");
        println!("\nüéØ Objetivo: Criar visualiza√ß√µes para entender padr√µes de aten√ß√£o");
        
        println!("\nüé® Tipos de visualiza√ß√£o:");
        println!("\n1. üî• Heatmap de Aten√ß√£o:");
        println!("   - Matriz colorida mostrando pesos de aten√ß√£o");
        println!("   - Eixo X: tokens de origem, Eixo Y: tokens de destino");
        println!("   - Cores quentes = alta aten√ß√£o, frias = baixa aten√ß√£o");
        
        println!("\n2. üï∏Ô∏è Grafo de Aten√ß√£o:");
        println!("   - N√≥s = tokens, arestas = pesos de aten√ß√£o");
        println!("   - Espessura da aresta proporcional ao peso");
        println!("   - Layout que preserva ordem sequencial");
        
        println!("\n3. üìä An√°lise por Cabe√ßa:");
        println!("   - Visualiza√ß√£o separada para cada cabe√ßa de aten√ß√£o");
        println!("   - Identifica√ß√£o de especializa√ß√µes");
        println!("   - Compara√ß√£o entre diferentes cabe√ßas");
        
        println!("\n4. üé¨ Anima√ß√£o Temporal:");
        println!("   - Mostra evolu√ß√£o da aten√ß√£o durante gera√ß√£o");
        println!("   - √ötil para entender processo autoregressivo");
        println!("   - Revela como contexto influencia pr√≥ximas palavras");
        
        println!("\nüí° Ferramentas sugeridas:");
        println!("  - plotters (Rust) para gr√°ficos est√°ticos");
        println!("  - egui (Rust) para interface interativa");
        println!("  - Export para Python/matplotlib para an√°lise avan√ßada");
    }
}

/// üöÄ Fun√ß√£o principal que executa todas as demonstra√ß√µes
fn main() -> candle_core::Result<()> {
    println!("üß† === DEMONSTRA√á√ÉO DO MECANISMO DE ATEN√á√ÉO ===");
    println!("Explorando o cora√ß√£o dos modelos Transformer");
    
    let demo = AttentionDemo::new()?;
    
    // Executa demonstra√ß√µes b√°sicas
    demo.demo_basic_attention_concept()?;
    demo.demo_self_attention()?;
    demo.demo_multi_head_attention()?;
    demo.demo_causal_attention()?;
    
    // Benchmark de performance
    demo.benchmark_attention()?;
    
    // Exerc√≠cios educacionais
    println!("\n\nüéì === EXERC√çCIOS PR√ÅTICOS ===");
    AttentionExercises::exercise_attention_patterns();
    AttentionExercises::exercise_performance_optimization();
    AttentionExercises::exercise_attention_visualization();
    
    println!("\n\n‚úÖ === DEMONSTRA√á√ÉO CONCLU√çDA ===");
    println!("üéØ Pr√≥ximos passos:");
    println!("  1. Experimente com diferentes configura√ß√µes");
    println!("  2. Implemente os exerc√≠cios sugeridos");
    println!("  3. Teste com sequ√™ncias reais de texto");
    println!("  4. Explore otimiza√ß√µes avan√ßadas");
    
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_attention_demo_creation() {
        let demo = AttentionDemo::new().unwrap();
        assert_eq!(demo.n_embd, 64);
        assert_eq!(demo.n_head, 4);
        assert_eq!(demo.seq_len, 8);
    }
    
    #[test]
    fn test_causal_mask_creation() {
        let demo = AttentionDemo::new().unwrap();
        let mask = demo.create_causal_mask(4).unwrap();
        let mask_data = mask.to_vec2::<f32>().unwrap();
        
        // Verifica estrutura triangular inferior
        assert_eq!(mask_data[0][0], 1.0);
        assert_eq!(mask_data[1][0], 1.0);
        assert_eq!(mask_data[1][1], 1.0);
        assert_eq!(mask_data[0][1], 0.0);
        assert_eq!(mask_data[0][2], 0.0);
    }
    
    #[test]
    fn test_cosine_similarity() {
        let demo = AttentionDemo::new().unwrap();
        let device = &demo.device;
        
        // Testa similaridade de tensor consigo mesmo (deve ser 1.0)
        let tensor = Tensor::randn(0f32, 1f32, (2, 3, 4), device).unwrap();
        let similarity = demo.calculate_cosine_similarity(&tensor, &tensor).unwrap();
        
        assert!((similarity - 1.0).abs() < 1e-5);
    }
}