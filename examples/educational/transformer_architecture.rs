//! # Exemplo Did√°tico: Arquitetura Transformer
//!
//! Este exemplo demonstra os componentes fundamentais da arquitetura Transformer:
//! - Mecanismo de Aten√ß√£o Multi-Head
//! - Feed-Forward Networks
//! - Normaliza√ß√£o de Camadas
//! - Conex√µes Residuais
//!
//! ## Como executar:
//! ```bash
//! cargo run --example transformer_architecture
//! ```

use std::collections::HashMap;

/// Representa um tensor simplificado para fins did√°ticos
#[derive(Debug, Clone)]
struct Tensor {
    data: Vec<f32>,
    shape: Vec<usize>,
}

impl Tensor {
    /// Cria um novo tensor com valores aleat√≥rios
    fn new(shape: Vec<usize>) -> Self {
        let size = shape.iter().product();
        let data = (0..size).map(|i| (i as f32 * 0.1) % 1.0).collect();
        Self { data, shape }
    }

    /// Cria um tensor com valores espec√≠ficos
    fn from_data(data: Vec<f32>, shape: Vec<usize>) -> Self {
        assert_eq!(data.len(), shape.iter().product());
        Self { data, shape }
    }

    /// Multiplica√ß√£o de matrizes simplificada
    fn matmul(&self, other: &Tensor) -> Tensor {
        // Implementa√ß√£o simplificada para fins did√°ticos
        assert_eq!(self.shape.len(), 2);
        assert_eq!(other.shape.len(), 2);
        assert_eq!(self.shape[1], other.shape[0]);
        
        let rows = self.shape[0];
        let cols = other.shape[1];
        let inner = self.shape[1];
        
        let mut result = vec![0.0; rows * cols];
        
        for i in 0..rows {
            for j in 0..cols {
                for k in 0..inner {
                    result[i * cols + j] += self.data[i * inner + k] * other.data[k * cols + j];
                }
            }
        }
        
        Tensor::from_data(result, vec![rows, cols])
    }

    /// Aplicar fun√ß√£o softmax
    fn softmax(&self) -> Tensor {
        let mut result = self.data.clone();
        let seq_len = self.shape[1];
        
        for i in 0..self.shape[0] {
            let start = i * seq_len;
            let end = start + seq_len;
            let slice = &mut result[start..end];
            
            // Encontrar o m√°ximo para estabilidade num√©rica
            let max_val = slice.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b));
            
            // Aplicar exponencial
            for val in slice.iter_mut() {
                *val = (*val - max_val).exp();
            }
            
            // Normalizar
            let sum: f32 = slice.iter().sum();
            for val in slice.iter_mut() {
                *val /= sum;
            }
        }
        
        Tensor::from_data(result, self.shape.clone())
    }

    /// Adicionar dois tensores (conex√£o residual)
    fn add(&self, other: &Tensor) -> Tensor {
        assert_eq!(self.shape, other.shape);
        let result: Vec<f32> = self.data.iter()
            .zip(other.data.iter())
            .map(|(a, b)| a + b)
            .collect();
        Tensor::from_data(result, self.shape.clone())
    }

    /// Normaliza√ß√£o de camada simplificada
    fn layer_norm(&self) -> Tensor {
        let mut result = self.data.clone();
        let feature_size = self.shape[1];
        
        for i in 0..self.shape[0] {
            let start = i * feature_size;
            let end = start + feature_size;
            let slice = &mut result[start..end];
            
            // Calcular m√©dia
            let mean: f32 = slice.iter().sum::<f32>() / feature_size as f32;
            
            // Calcular vari√¢ncia
            let variance: f32 = slice.iter()
                .map(|x| (x - mean).powi(2))
                .sum::<f32>() / feature_size as f32;
            
            // Normalizar
            let std_dev = (variance + 1e-6).sqrt();
            for val in slice.iter_mut() {
                *val = (*val - mean) / std_dev;
            }
        }
        
        Tensor::from_data(result, self.shape.clone())
    }
}

/// ## 1. Mecanismo de Aten√ß√£o Multi-Head
/// 
/// A aten√ß√£o √© o cora√ß√£o do Transformer. Permite que o modelo "preste aten√ß√£o"
/// a diferentes partes da sequ√™ncia de entrada simultaneamente.
#[derive(Debug)]
struct MultiHeadAttention {
    num_heads: usize,
    head_dim: usize,
    model_dim: usize,
    // Matrizes de proje√ß√£o para Query, Key, Value
    w_q: Tensor,
    w_k: Tensor, 
    w_v: Tensor,
    w_o: Tensor, // Proje√ß√£o de sa√≠da
}

impl MultiHeadAttention {
    fn new(model_dim: usize, num_heads: usize) -> Self {
        assert_eq!(model_dim % num_heads, 0, "model_dim deve ser divis√≠vel por num_heads");
        
        let head_dim = model_dim / num_heads;
        
        Self {
            num_heads,
            head_dim,
            model_dim,
            // Inicializar matrizes de peso (simplificado)
            w_q: Tensor::new(vec![model_dim, model_dim]),
            w_k: Tensor::new(vec![model_dim, model_dim]),
            w_v: Tensor::new(vec![model_dim, model_dim]),
            w_o: Tensor::new(vec![model_dim, model_dim]),
        }
    }

    /// Calcula a aten√ß√£o scaled dot-product
    /// 
    /// F√≥rmula: Attention(Q,K,V) = softmax(QK^T / ‚àöd_k)V
    fn scaled_dot_product_attention(&self, q: &Tensor, k: &Tensor, v: &Tensor) -> Tensor {
        println!("üîç Calculando Aten√ß√£o Scaled Dot-Product:");
        println!("   - Query shape: {:?}", q.shape);
        println!("   - Key shape: {:?}", k.shape);
        println!("   - Value shape: {:?}", v.shape);
        
        // 1. Transpor K para calcular QK^T
        let k_transposed = self.transpose(k);
        println!("   - Key transposta shape: {:?}", k_transposed.shape);
        
        // 2. Calcular QK^T
        let scores = q.matmul(&k_transposed);
        println!("   - Scores (QK^T) calculados, shape: {:?}", scores.shape);
        
        // 3. Escalar por ‚àöd_k para estabilidade
        let scale = 1.0 / (self.head_dim as f32).sqrt();
        let scaled_scores = Tensor::from_data(
            scores.data.iter().map(|x| x * scale).collect(),
            scores.shape.clone()
        );
        println!("   - Scores escalados por ‚àöd_k = {:.3}", scale);
        
        // 4. Aplicar softmax para obter pesos de aten√ß√£o
        let attention_weights = scaled_scores.softmax();
        println!("   - Pesos de aten√ß√£o calculados via softmax");
        
        // 5. Aplicar pesos aos valores
        let output = attention_weights.matmul(v);
        println!("   - Sa√≠da final: pesos √ó valores, shape: {:?}", output.shape);
        
        output
    }
    
    /// Transp√µe uma matriz 2D
    fn transpose(&self, tensor: &Tensor) -> Tensor {
        assert_eq!(tensor.shape.len(), 2, "Transpose s√≥ funciona para matrizes 2D");
        
        let rows = tensor.shape[0];
        let cols = tensor.shape[1];
        let mut transposed_data = vec![0.0; rows * cols];
        
        for i in 0..rows {
            for j in 0..cols {
                transposed_data[j * rows + i] = tensor.data[i * cols + j];
            }
        }
        
        Tensor::from_data(transposed_data, vec![cols, rows])
    }

    /// Processa a entrada atrav√©s de m√∫ltiplas cabe√ßas de aten√ß√£o
    fn forward(&self, input: &Tensor) -> Tensor {
        println!("\nüß† === MULTI-HEAD ATTENTION ===");
        println!("Entrada shape: {:?}", input.shape);
        println!("N√∫mero de cabe√ßas: {}", self.num_heads);
        println!("Dimens√£o por cabe√ßa: {}", self.head_dim);
        
        // 1. Projetar entrada para Q, K, V
        let q = input.matmul(&self.w_q);
        let k = input.matmul(&self.w_k);
        let v = input.matmul(&self.w_v);
        
        println!("\nüìä Proje√ß√µes lineares criadas:");
        println!("   - Q (Query): {:?}", q.shape);
        println!("   - K (Key): {:?}", k.shape);
        println!("   - V (Value): {:?}", v.shape);
        
        // 2. Para cada cabe√ßa, calcular aten√ß√£o
        // (Simplificado: usando apenas uma cabe√ßa para demonstra√ß√£o)
        let attention_output = self.scaled_dot_product_attention(&q, &k, &v);
        
        // 3. Proje√ß√£o final
        let output = attention_output.matmul(&self.w_o);
        
        println!("\n‚úÖ Multi-Head Attention conclu√≠da!");
        println!("Sa√≠da shape: {:?}", output.shape);
        
        output
    }
}

/// ## 2. Feed-Forward Network
/// 
/// Rede neural simples que processa cada posi√ß√£o independentemente.
/// Estrutura: Linear -> ReLU -> Linear
#[derive(Debug)]
struct FeedForwardNetwork {
    w1: Tensor, // Primeira camada linear
    w2: Tensor, // Segunda camada linear
    hidden_dim: usize,
}

impl FeedForwardNetwork {
    fn new(model_dim: usize, hidden_dim: usize) -> Self {
        Self {
            w1: Tensor::new(vec![model_dim, hidden_dim]),
            w2: Tensor::new(vec![hidden_dim, model_dim]),
            hidden_dim,
        }
    }

    /// Fun√ß√£o de ativa√ß√£o ReLU
    fn relu(&self, tensor: &Tensor) -> Tensor {
        let data: Vec<f32> = tensor.data.iter()
            .map(|&x| if x > 0.0 { x } else { 0.0 })
            .collect();
        Tensor::from_data(data, tensor.shape.clone())
    }

    fn forward(&self, input: &Tensor) -> Tensor {
        println!("\nüîÑ === FEED-FORWARD NETWORK ===");
        println!("Entrada shape: {:?}", input.shape);
        println!("Dimens√£o oculta: {}", self.hidden_dim);
        
        // 1. Primeira transforma√ß√£o linear
        let hidden = input.matmul(&self.w1);
        println!("Ap√≥s primeira linear: {:?}", hidden.shape);
        
        // 2. Ativa√ß√£o ReLU
        let activated = self.relu(&hidden);
        println!("Ap√≥s ReLU: {:?}", activated.shape);
        
        // 3. Segunda transforma√ß√£o linear
        let output = activated.matmul(&self.w2);
        println!("Sa√≠da final: {:?}", output.shape);
        
        println!("‚úÖ Feed-Forward conclu√≠da!");
        output
    }
}

/// ## 3. Bloco Transformer Completo
/// 
/// Combina aten√ß√£o multi-head, feed-forward e conex√µes residuais
#[derive(Debug)]
struct TransformerBlock {
    attention: MultiHeadAttention,
    feed_forward: FeedForwardNetwork,
}

impl TransformerBlock {
    fn new(model_dim: usize, num_heads: usize, ff_hidden_dim: usize) -> Self {
        Self {
            attention: MultiHeadAttention::new(model_dim, num_heads),
            feed_forward: FeedForwardNetwork::new(model_dim, ff_hidden_dim),
        }
    }

    fn forward(&self, input: &Tensor) -> Tensor {
        println!("\nüèóÔ∏è  === TRANSFORMER BLOCK ===");
        
        // 1. Multi-Head Attention + Conex√£o Residual + Layer Norm
        println!("\nüîó Aplicando aten√ß√£o com conex√£o residual...");
        let attention_output = self.attention.forward(input);
        let residual1 = input.add(&attention_output); // Conex√£o residual
        let norm1 = residual1.layer_norm(); // Normaliza√ß√£o
        
        println!("‚úÖ Primeira sub-camada (Aten√ß√£o + Residual + Norm) conclu√≠da");
        
        // 2. Feed-Forward + Conex√£o Residual + Layer Norm
        println!("\nüîó Aplicando feed-forward com conex√£o residual...");
        let ff_output = self.feed_forward.forward(&norm1);
        let residual2 = norm1.add(&ff_output); // Conex√£o residual
        let norm2 = residual2.layer_norm(); // Normaliza√ß√£o
        
        println!("‚úÖ Segunda sub-camada (FF + Residual + Norm) conclu√≠da");
        println!("\nüéâ TRANSFORMER BLOCK COMPLETO!");
        
        norm2
    }
}

/// Fun√ß√£o principal para demonstrar o funcionamento
fn main() {
    println!("üöÄ === DEMONSTRA√á√ÉO DA ARQUITETURA TRANSFORMER ===");
    println!("\nEste exemplo mostra como funciona um bloco Transformer b√°sico.");
    println!("Vamos processar uma sequ√™ncia de exemplo atrav√©s de todas as camadas.\n");
    
    // Configura√ß√µes do modelo
    let seq_len = 4;      // Comprimento da sequ√™ncia
    let model_dim = 8;    // Dimens√£o do modelo
    let num_heads = 2;    // N√∫mero de cabe√ßas de aten√ß√£o
    let ff_hidden = 16;   // Dimens√£o oculta do feed-forward
    
    println!("üìã Configura√ß√µes:");
    println!("   - Comprimento da sequ√™ncia: {}", seq_len);
    println!("   - Dimens√£o do modelo: {}", model_dim);
    println!("   - N√∫mero de cabe√ßas: {}", num_heads);
    println!("   - Dimens√£o FF oculta: {}", ff_hidden);
    
    // Criar entrada de exemplo (representando embeddings de tokens)
    let input = Tensor::new(vec![seq_len, model_dim]);
    println!("\nüì• Entrada criada: shape {:?}", input.shape);
    println!("   Representa {} tokens, cada um com {} dimens√µes", seq_len, model_dim);
    
    // Criar e executar o bloco Transformer
    let transformer_block = TransformerBlock::new(model_dim, num_heads, ff_hidden);
    let output = transformer_block.forward(&input);
    
    println!("\nüì§ Sa√≠da final: shape {:?}", output.shape);
    println!("   Primeiros valores: {:?}", &output.data[0..4]);
    
    // Explica√ß√£o dos conceitos
    println!("\n\nüìö === CONCEITOS FUNDAMENTAIS ===");
    
    println!("\nüîç ATEN√á√ÉO MULTI-HEAD:");
    println!("   ‚Ä¢ Permite ao modelo focar em diferentes aspectos da entrada");
    println!("   ‚Ä¢ Cada 'cabe√ßa' aprende padr√µes diferentes");
    println!("   ‚Ä¢ Query: 'o que estou procurando?'");
    println!("   ‚Ä¢ Key: 'o que est√° dispon√≠vel?'");
    println!("   ‚Ä¢ Value: 'qual informa√ß√£o carregar?'");
    
    println!("\nüîÑ FEED-FORWARD:");
    println!("   ‚Ä¢ Processa cada posi√ß√£o independentemente");
    println!("   ‚Ä¢ Adiciona capacidade de representa√ß√£o n√£o-linear");
    println!("   ‚Ä¢ Estrutura: Linear ‚Üí ReLU ‚Üí Linear");
    
    println!("\nüîó CONEX√ïES RESIDUAIS:");
    println!("   ‚Ä¢ Facilitam o treinamento de redes profundas");
    println!("   ‚Ä¢ Permitem que gradientes fluam diretamente");
    println!("   ‚Ä¢ F√≥rmula: output = LayerNorm(input + SubLayer(input))");
    
    println!("\nüìè NORMALIZA√á√ÉO DE CAMADAS:");
    println!("   ‚Ä¢ Estabiliza o treinamento");
    println!("   ‚Ä¢ Normaliza ativa√ß√µes para m√©dia 0 e vari√¢ncia 1");
    println!("   ‚Ä¢ Aplicada ap√≥s cada sub-camada");
    
    println!("\n\nüéì === EXERC√çCIOS SUGERIDOS ===");
    println!("1. Modifique o n√∫mero de cabe√ßas e observe o comportamento");
    println!("2. Altere as dimens√µes do modelo e feed-forward");
    println!("3. Adicione mais blocos Transformer em sequ√™ncia");
    println!("4. Implemente diferentes fun√ß√µes de ativa√ß√£o");
    println!("5. Adicione dropout para regulariza√ß√£o");
    
    println!("\n‚ú® Transformer conclu√≠do com sucesso! ‚ú®");
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_tensor_operations() {
        let t1 = Tensor::from_data(vec![1.0, 2.0, 3.0, 4.0], vec![2, 2]);
        let t2 = Tensor::from_data(vec![1.0, 0.0, 0.0, 1.0], vec![2, 2]);
        
        let result = t1.matmul(&t2);
        assert_eq!(result.shape, vec![2, 2]);
    }

    #[test]
    fn test_multi_head_attention() {
        let attention = MultiHeadAttention::new(8, 2);
        let input = Tensor::new(vec![4, 8]);
        
        let output = attention.forward(&input);
        assert_eq!(output.shape, input.shape);
    }

    #[test]
    fn test_transformer_block() {
        let block = TransformerBlock::new(8, 2, 16);
        let input = Tensor::new(vec![4, 8]);
        
        let output = block.forward(&input);
        assert_eq!(output.shape, input.shape);
    }
}