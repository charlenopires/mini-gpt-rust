//! # Exemplo DidÃ¡tico: Arquitetura Transformer
//!
//! Este exemplo demonstra os componentes fundamentais da arquitetura Transformer:
//! - Mecanismo de AtenÃ§Ã£o Multi-Head
//! - Feed-Forward Networks
//! - NormalizaÃ§Ã£o de Camadas
//! - ConexÃµes Residuais
//!
//! ## Como executar:
//! ```bash
//! cargo run --example transformer_architecture
//! ```

use std::collections::HashMap;

/// Representa um tensor simplificado para fins didÃ¡ticos
#[derive(Debug, Clone)]
struct Tensor {
    data: Vec<f32>,
    shape: Vec<usize>,
}

impl Tensor {
    /// Cria um novo tensor com valores aleatÃ³rios
    fn new(shape: Vec<usize>) -> Self {
        let size = shape.iter().product();
        let data = (0..size).map(|i| (i as f32 * 0.1) % 1.0).collect();
        Self { data, shape }
    }

    /// Cria um tensor com valores especÃ­ficos
    fn from_data(data: Vec<f32>, shape: Vec<usize>) -> Self {
        assert_eq!(data.len(), shape.iter().product());
        Self { data, shape }
    }

    /// MultiplicaÃ§Ã£o de matrizes simplificada
    fn matmul(&self, other: &Tensor) -> Tensor {
        // ImplementaÃ§Ã£o simplificada para fins didÃ¡ticos
        assert_eq!(self.shape.len(), 2);
        assert_eq!(other.shape.len(), 2);
        assert_eq!(self.shape[1], other.shape[0]);
        
        let rows = self.shape[0];
        let cols = other.shape[1];
        let inner = self.shape[1];
        
        let mut result = vec![0.0; rows * cols];
        
        for i in 0..rows {
            for j in 0..cols {
                for k in 0..inner {
                    result[i * cols + j] += self.data[i * inner + k] * other.data[k * cols + j];
                }
            }
        }
        
        Tensor::from_data(result, vec![rows, cols])
    }

    /// Aplicar funÃ§Ã£o softmax
    fn softmax(&self) -> Tensor {
        let mut result = self.data.clone();
        let seq_len = self.shape[1];
        
        for i in 0..self.shape[0] {
            let start = i * seq_len;
            let end = start + seq_len;
            let slice = &mut result[start..end];
            
            // Encontrar o mÃ¡ximo para estabilidade numÃ©rica
            let max_val = slice.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b));
            
            // Aplicar exponencial
            for val in slice.iter_mut() {
                *val = (*val - max_val).exp();
            }
            
            // Normalizar
            let sum: f32 = slice.iter().sum();
            for val in slice.iter_mut() {
                *val /= sum;
            }
        }
        
        Tensor::from_data(result, self.shape.clone())
    }

    /// Adicionar dois tensores (conexÃ£o residual)
    fn add(&self, other: &Tensor) -> Tensor {
        assert_eq!(self.shape, other.shape);
        let result: Vec<f32> = self.data.iter()
            .zip(other.data.iter())
            .map(|(a, b)| a + b)
            .collect();
        Tensor::from_data(result, self.shape.clone())
    }

    /// NormalizaÃ§Ã£o de camada simplificada
    fn layer_norm(&self) -> Tensor {
        let mut result = self.data.clone();
        let feature_size = self.shape[1];
        
        for i in 0..self.shape[0] {
            let start = i * feature_size;
            let end = start + feature_size;
            let slice = &mut result[start..end];
            
            // Calcular mÃ©dia
            let mean: f32 = slice.iter().sum::<f32>() / feature_size as f32;
            
            // Calcular variÃ¢ncia
            let variance: f32 = slice.iter()
                .map(|x| (x - mean).powi(2))
                .sum::<f32>() / feature_size as f32;
            
            // Normalizar
            let std_dev = (variance + 1e-6).sqrt();
            for val in slice.iter_mut() {
                *val = (*val - mean) / std_dev;
            }
        }
        
        Tensor::from_data(result, self.shape.clone())
    }
}

/// ## 1. Mecanismo de AtenÃ§Ã£o Multi-Head
/// 
/// A atenÃ§Ã£o Ã© o coraÃ§Ã£o do Transformer. Permite que o modelo "preste atenÃ§Ã£o"
/// a diferentes partes da sequÃªncia de entrada simultaneamente.
#[derive(Debug)]
struct MultiHeadAttention {
    num_heads: usize,
    head_dim: usize,
    model_dim: usize,
    // Matrizes de projeÃ§Ã£o para Query, Key, Value
    w_q: Tensor,
    w_k: Tensor, 
    w_v: Tensor,
    w_o: Tensor, // ProjeÃ§Ã£o de saÃ­da
}

impl MultiHeadAttention {
    fn new(model_dim: usize, num_heads: usize) -> Self {
        assert_eq!(model_dim % num_heads, 0, "model_dim deve ser divisÃ­vel por num_heads");
        
        let head_dim = model_dim / num_heads;
        
        Self {
            num_heads,
            head_dim,
            model_dim,
            // Inicializar matrizes de peso (simplificado)
            w_q: Tensor::new(vec![model_dim, model_dim]),
            w_k: Tensor::new(vec![model_dim, model_dim]),
            w_v: Tensor::new(vec![model_dim, model_dim]),
            w_o: Tensor::new(vec![model_dim, model_dim]),
        }
    }

    /// Calcula a atenÃ§Ã£o scaled dot-product
    /// 
    /// FÃ³rmula: Attention(Q,K,V) = softmax(QK^T / âˆšd_k)V
    fn scaled_dot_product_attention(&self, q: &Tensor, k: &Tensor, v: &Tensor) -> Tensor {
        println!("ğŸ” Calculando AtenÃ§Ã£o Scaled Dot-Product:");
        println!("   - Query shape: {:?}", q.shape);
        println!("   - Key shape: {:?}", k.shape);
        println!("   - Value shape: {:?}", v.shape);
        
        // 1. Transpor K para calcular QK^T
        let k_transposed = self.transpose(k);
        println!("   - Key transposta shape: {:?}", k_transposed.shape);
        
        // 2. Calcular QK^T
        let scores = q.matmul(&k_transposed);
        println!("   - Scores (QK^T) calculados, shape: {:?}", scores.shape);
        
        // 3. Escalar por âˆšd_k para estabilidade
        let scale = 1.0 / (self.head_dim as f32).sqrt();
        let scaled_scores = Tensor::from_data(
            scores.data.iter().map(|x| x * scale).collect(),
            scores.shape.clone()
        );
        println!("   - Scores escalados por âˆšd_k = {:.3}", scale);
        
        // 4. Aplicar softmax para obter pesos de atenÃ§Ã£o
        let attention_weights = scaled_scores.softmax();
        println!("   - Pesos de atenÃ§Ã£o calculados via softmax");
        
        // 5. Aplicar pesos aos valores
        let output = attention_weights.matmul(v);
        println!("   - SaÃ­da final: pesos Ã— valores, shape: {:?}", output.shape);
        
        output
    }
    
    /// TranspÃµe uma matriz 2D
    fn transpose(&self, tensor: &Tensor) -> Tensor {
        assert_eq!(tensor.shape.len(), 2, "Transpose sÃ³ funciona para matrizes 2D");
        
        let rows = tensor.shape[0];
        let cols = tensor.shape[1];
        let mut transposed_data = vec![0.0; rows * cols];
        
        for i in 0..rows {
            for j in 0..cols {
                transposed_data[j * rows + i] = tensor.data[i * cols + j];
            }
        }
        
        Tensor::from_data(transposed_data, vec![cols, rows])
    }

    /// Processa a entrada atravÃ©s de mÃºltiplas cabeÃ§as de atenÃ§Ã£o
    fn forward(&self, input: &Tensor) -> Tensor {
        println!("\nğŸ§  === MULTI-HEAD ATTENTION ===");
        println!("Entrada shape: {:?}", input.shape);
        println!("NÃºmero de cabeÃ§as: {}", self.num_heads);
        println!("DimensÃ£o por cabeÃ§a: {}", self.head_dim);
        
        // 1. Projetar entrada para Q, K, V
        let q = input.matmul(&self.w_q);
        let k = input.matmul(&self.w_k);
        let v = input.matmul(&self.w_v);
        
        println!("\nğŸ“Š ProjeÃ§Ãµes lineares criadas:");
        println!("   - Q (Query): {:?}", q.shape);
        println!("   - K (Key): {:?}", k.shape);
        println!("   - V (Value): {:?}", v.shape);
        
        // 2. Para cada cabeÃ§a, calcular atenÃ§Ã£o
        // (Simplificado: usando apenas uma cabeÃ§a para demonstraÃ§Ã£o)
        let attention_output = self.scaled_dot_product_attention(&q, &k, &v);
        
        // 3. ProjeÃ§Ã£o final
        let output = attention_output.matmul(&self.w_o);
        
        println!("\nâœ… Multi-Head Attention concluÃ­da!");
        println!("SaÃ­da shape: {:?}", output.shape);
        
        output
    }
}

/// ## 2. Feed-Forward Network
/// 
/// Rede neural simples que processa cada posiÃ§Ã£o independentemente.
/// Estrutura: Linear -> ReLU -> Linear
#[derive(Debug)]
struct FeedForwardNetwork {
    w1: Tensor, // Primeira camada linear
    w2: Tensor, // Segunda camada linear
    hidden_dim: usize,
}

impl FeedForwardNetwork {
    fn new(model_dim: usize, hidden_dim: usize) -> Self {
        Self {
            w1: Tensor::new(vec![model_dim, hidden_dim]),
            w2: Tensor::new(vec![hidden_dim, model_dim]),
            hidden_dim,
        }
    }

    /// FunÃ§Ã£o de ativaÃ§Ã£o ReLU
    fn relu(&self, tensor: &Tensor) -> Tensor {
        let data: Vec<f32> = tensor.data.iter()
            .map(|&x| if x > 0.0 { x } else { 0.0 })
            .collect();
        Tensor::from_data(data, tensor.shape.clone())
    }

    fn forward(&self, input: &Tensor) -> Tensor {
        println!("\nğŸ”„ === FEED-FORWARD NETWORK ===");
        println!("Entrada shape: {:?}", input.shape);
        println!("DimensÃ£o oculta: {}", self.hidden_dim);
        
        // 1. Primeira transformaÃ§Ã£o linear
        let hidden = input.matmul(&self.w1);
        println!("ApÃ³s primeira linear: {:?}", hidden.shape);
        
        // 2. AtivaÃ§Ã£o ReLU
        let activated = self.relu(&hidden);
        println!("ApÃ³s ReLU: {:?}", activated.shape);
        
        // 3. Segunda transformaÃ§Ã£o linear
        let output = activated.matmul(&self.w2);
        println!("SaÃ­da final: {:?}", output.shape);
        
        println!("âœ… Feed-Forward concluÃ­da!");
        output
    }
}

/// ## 3. Bloco Transformer Completo
/// 
/// Combina atenÃ§Ã£o multi-head, feed-forward e conexÃµes residuais
#[derive(Debug)]
struct TransformerBlock {
    attention: MultiHeadAttention,
    feed_forward: FeedForwardNetwork,
}

impl TransformerBlock {
    fn new(model_dim: usize, num_heads: usize, ff_hidden_dim: usize) -> Self {
        Self {
            attention: MultiHeadAttention::new(model_dim, num_heads),
            feed_forward: FeedForwardNetwork::new(model_dim, ff_hidden_dim),
        }
    }

    fn forward(&self, input: &Tensor) -> Tensor {
        println!("\nğŸ—ï¸  === TRANSFORMER BLOCK ===");
        
        // 1. Multi-Head Attention + ConexÃ£o Residual + Layer Norm
        println!("\nğŸ”— Aplicando atenÃ§Ã£o com conexÃ£o residual...");
        let attention_output = self.attention.forward(input);
        let residual1 = input.add(&attention_output); // ConexÃ£o residual
        let norm1 = residual1.layer_norm(); // NormalizaÃ§Ã£o
        
        println!("âœ… Primeira sub-camada (AtenÃ§Ã£o + Residual + Norm) concluÃ­da");
        
        // 2. Feed-Forward + ConexÃ£o Residual + Layer Norm
        println!("\nğŸ”— Aplicando feed-forward com conexÃ£o residual...");
        let ff_output = self.feed_forward.forward(&norm1);
        let residual2 = norm1.add(&ff_output); // ConexÃ£o residual
        let norm2 = residual2.layer_norm(); // NormalizaÃ§Ã£o
        
        println!("âœ… Segunda sub-camada (FF + Residual + Norm) concluÃ­da");
        println!("\nğŸ‰ TRANSFORMER BLOCK COMPLETO!");
        
        norm2
    }
}

/// FunÃ§Ã£o principal para demonstrar o funcionamento
fn main() {
    println!("ğŸš€ === DEMONSTRAÃ‡ÃƒO DA ARQUITETURA TRANSFORMER ===");
    println!("\nEste exemplo mostra como funciona um bloco Transformer bÃ¡sico.");
    println!("Vamos processar uma sequÃªncia de exemplo atravÃ©s de todas as camadas.\n");
    
    // ConfiguraÃ§Ãµes do modelo
    let seq_len = 4;      // Comprimento da sequÃªncia
    let model_dim = 8;    // DimensÃ£o do modelo
    let num_heads = 2;    // NÃºmero de cabeÃ§as de atenÃ§Ã£o
    let ff_hidden = 16;   // DimensÃ£o oculta do feed-forward
    
    println!("ğŸ“‹ ConfiguraÃ§Ãµes:");
    println!("   - Comprimento da sequÃªncia: {}", seq_len);
    println!("   - DimensÃ£o do modelo: {}", model_dim);
    println!("   - NÃºmero de cabeÃ§as: {}", num_heads);
    println!("   - DimensÃ£o FF oculta: {}", ff_hidden);
    
    // Criar entrada de exemplo (representando embeddings de tokens)
    let input = Tensor::new(vec![seq_len, model_dim]);
    println!("\nğŸ“¥ Entrada criada: shape {:?}", input.shape);
    println!("   Representa {} tokens, cada um com {} dimensÃµes", seq_len, model_dim);
    
    // Criar e executar o bloco Transformer
    let transformer_block = TransformerBlock::new(model_dim, num_heads, ff_hidden);
    let output = transformer_block.forward(&input);
    
    println!("\nğŸ“¤ SaÃ­da final: shape {:?}", output.shape);
    println!("   Primeiros valores: {:?}", &output.data[0..4]);
    
    // ExplicaÃ§Ã£o dos conceitos
    println!("\n\nğŸ“š === CONCEITOS FUNDAMENTAIS ===");
    
    println!("\nğŸ” ATENÃ‡ÃƒO MULTI-HEAD:");
    println!("   â€¢ Permite ao modelo focar em diferentes aspectos da entrada");
    println!("   â€¢ Cada 'cabeÃ§a' aprende padrÃµes diferentes");
    println!("   â€¢ Query: 'o que estou procurando?'");
    println!("   â€¢ Key: 'o que estÃ¡ disponÃ­vel?'");
    println!("   â€¢ Value: 'qual informaÃ§Ã£o carregar?'");
    
    println!("\nğŸ”„ FEED-FORWARD:");
    println!("   â€¢ Processa cada posiÃ§Ã£o independentemente");
    println!("   â€¢ Adiciona capacidade de representaÃ§Ã£o nÃ£o-linear");
    println!("   â€¢ Estrutura: Linear â†’ ReLU â†’ Linear");
    
    println!("\nğŸ”— CONEXÃ•ES RESIDUAIS:");
    println!("   â€¢ Facilitam o treinamento de redes profundas");
    println!("   â€¢ Permitem que gradientes fluam diretamente");
    println!("   â€¢ FÃ³rmula: output = LayerNorm(input + SubLayer(input))");
    
    println!("\nğŸ“ NORMALIZAÃ‡ÃƒO DE CAMADAS:");
    println!("   â€¢ Estabiliza o treinamento");
    println!("   â€¢ Normaliza ativaÃ§Ãµes para mÃ©dia 0 e variÃ¢ncia 1");
    println!("   â€¢ Aplicada apÃ³s cada sub-camada");
    
    println!("\n\nğŸ“ === EXERCÃCIOS SUGERIDOS ===");
    println!("1. Modifique o nÃºmero de cabeÃ§as e observe o comportamento");
    println!("2. Altere as dimensÃµes do modelo e feed-forward");
    println!("3. Adicione mais blocos Transformer em sequÃªncia");
    println!("4. Implemente diferentes funÃ§Ãµes de ativaÃ§Ã£o");
    println!("5. Adicione dropout para regularizaÃ§Ã£o");
    
    println!("\nâœ¨ Transformer concluÃ­do com sucesso! âœ¨");
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_tensor_operations() {
        let t1 = Tensor::from_data(vec![1.0, 2.0, 3.0, 4.0], vec![2, 2]);
        let t2 = Tensor::from_data(vec![1.0, 0.0, 0.0, 1.0], vec![2, 2]);
        
        let result = t1.matmul(&t2);
        assert_eq!(result.shape, vec![2, 2]);
    }

    #[test]
    fn test_multi_head_attention() {
        let attention = MultiHeadAttention::new(8, 2);
        let input = Tensor::new(vec![4, 8]);
        
        let output = attention.forward(&input);
        assert_eq!(output.shape, input.shape);
    }

    #[test]
    fn test_transformer_block() {
        let block = TransformerBlock::new(8, 2, 16);
        let input = Tensor::new(vec![4, 8]);
        
        let output = block.forward(&input);
        assert_eq!(output.shape, input.shape);
    }
}