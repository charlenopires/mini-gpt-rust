//! # Exemplo Did√°tico: Processo de Tokeniza√ß√£o
//!
//! Este exemplo demonstra como o texto √© processado em modelos de linguagem:
//! - Divis√£o do texto em tokens
//! - Convers√£o de tokens para IDs num√©ricos
//! - Diferentes estrat√©gias de tokeniza√ß√£o
//! - Encoding e decoding de sequ√™ncias
//!
//! ## Como executar:
//! ```bash
//! cargo run --example tokenization_process
//! ```

use std::collections::HashMap;
use std::collections::BTreeMap;

/// Representa um token individual
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
struct Token {
    text: String,
    id: usize,
}

impl Token {
    fn new(text: String, id: usize) -> Self {
        Self { text, id }
    }
}

/// ## 1. Tokenizador Simples por Palavras
/// 
/// A forma mais b√°sica de tokeniza√ß√£o: dividir por espa√ßos e pontua√ß√£o
#[derive(Debug)]
struct WordTokenizer {
    vocab: HashMap<String, usize>,
    reverse_vocab: HashMap<usize, String>,
    next_id: usize,
}

impl WordTokenizer {
    fn new() -> Self {
        let mut tokenizer = Self {
            vocab: HashMap::new(),
            reverse_vocab: HashMap::new(),
            next_id: 0,
        };
        
        // Adicionar tokens especiais
        tokenizer.add_special_tokens();
        tokenizer
    }
    
    fn add_special_tokens(&mut self) {
        let special_tokens = vec![
            "<PAD>",   // Padding
            "<UNK>",   // Unknown
            "<BOS>",   // Beginning of sequence
            "<EOS>",   // End of sequence
        ];
        
        for token in special_tokens {
            self.add_token(token.to_string());
        }
    }
    
    fn add_token(&mut self, token: String) -> usize {
        if let Some(&id) = self.vocab.get(&token) {
            return id;
        }
        
        let id = self.next_id;
        self.vocab.insert(token.clone(), id);
        self.reverse_vocab.insert(id, token);
        self.next_id += 1;
        id
    }
    
    /// Treina o tokenizador com um corpus de texto
    fn train(&mut self, texts: &[&str]) {
        println!("üéì Treinando tokenizador com {} textos...", texts.len());
        
        for text in texts {
            let words = self.simple_tokenize(text);
            for word in words {
                self.add_token(word);
            }
        }
        
        println!("‚úÖ Vocabul√°rio criado com {} tokens", self.vocab.len());
    }
    
    /// Tokeniza√ß√£o simples: divide por espa√ßos e pontua√ß√£o
    fn simple_tokenize(&self, text: &str) -> Vec<String> {
        let mut tokens = Vec::new();
        let mut current_word = String::new();
        
        for ch in text.chars() {
            if ch.is_whitespace() {
                if !current_word.is_empty() {
                    tokens.push(current_word.clone());
                    current_word.clear();
                }
            } else if ch.is_ascii_punctuation() {
                if !current_word.is_empty() {
                    tokens.push(current_word.clone());
                    current_word.clear();
                }
                tokens.push(ch.to_string());
            } else {
                current_word.push(ch.to_lowercase().next().unwrap_or(ch));
            }
        }
        
        if !current_word.is_empty() {
            tokens.push(current_word);
        }
        
        tokens
    }
    
    /// Converte texto em sequ√™ncia de IDs
    fn encode(&self, text: &str) -> Vec<usize> {
        println!("\nüî§ Codificando texto: \"{}\"", text);
        
        let tokens = self.simple_tokenize(text);
        println!("üìù Tokens extra√≠dos: {:?}", tokens);
        
        let mut ids = vec![self.vocab["<BOS>"]];
        
        for token in tokens {
            let id = self.vocab.get(&token)
                .copied()
                .unwrap_or(self.vocab["<UNK>"]);
            ids.push(id);
            
            if self.vocab.contains_key(&token) {
                println!("   '{}' ‚Üí ID {}", token, id);
            } else {
                println!("   '{}' ‚Üí ID {} (<UNK>)", token, id);
            }
        }
        
        ids.push(self.vocab["<EOS>"]);
        println!("üî¢ Sequ√™ncia final: {:?}", ids);
        
        ids
    }
    
    /// Converte sequ√™ncia de IDs de volta para texto
    fn decode(&self, ids: &[usize]) -> String {
        println!("\nüî¢ Decodificando IDs: {:?}", ids);
        
        let tokens: Vec<String> = ids.iter()
            .filter_map(|&id| self.reverse_vocab.get(&id))
            .filter(|token| !matches!(token.as_str(), "<BOS>" | "<EOS>" | "<PAD>"))
            .cloned()
            .collect();
        
        println!("üìù Tokens recuperados: {:?}", tokens);
        
        let text = tokens.join(" ");
        println!("üìÑ Texto final: \"{}\"", text);
        
        text
    }
    
    fn vocab_size(&self) -> usize {
        self.vocab.len()
    }
}

/// ## 2. Tokenizador BPE Simplificado
/// 
/// Byte Pair Encoding: une os pares de caracteres mais frequentes
#[derive(Debug)]
struct SimpleBPETokenizer {
    vocab: HashMap<String, usize>,
    reverse_vocab: HashMap<usize, String>,
    merges: Vec<(String, String)>,
    next_id: usize,
}

impl SimpleBPETokenizer {
    fn new() -> Self {
        let mut tokenizer = Self {
            vocab: HashMap::new(),
            reverse_vocab: HashMap::new(),
            merges: Vec::new(),
            next_id: 0,
        };
        
        tokenizer.add_special_tokens();
        tokenizer
    }
    
    fn add_special_tokens(&mut self) {
        let special_tokens = vec!["<PAD>", "<UNK>", "<BOS>", "<EOS>"];
        for token in special_tokens {
            self.add_token(token.to_string());
        }
    }
    
    fn add_token(&mut self, token: String) -> usize {
        if let Some(&id) = self.vocab.get(&token) {
            return id;
        }
        
        let id = self.next_id;
        self.vocab.insert(token.clone(), id);
        self.reverse_vocab.insert(id, token);
        self.next_id += 1;
        id
    }
    
    /// Treina o BPE com um corpus
    fn train(&mut self, texts: &[&str], num_merges: usize) {
        println!("\nüß† Treinando BPE com {} merges...", num_merges);
        
        // 1. Inicializar vocabul√°rio com caracteres individuais
        let mut word_freqs = HashMap::new();
        
        for text in texts {
            for word in text.split_whitespace() {
                let word = word.to_lowercase();
                *word_freqs.entry(word.clone()).or_insert(0) += 1;
                
                // Adicionar caracteres individuais ao vocabul√°rio
                for ch in word.chars() {
                    self.add_token(ch.to_string());
                }
            }
        }
        
        println!("üìö Vocabul√°rio inicial: {} caracteres", self.vocab.len() - 4);
        
        // 2. Executar merges BPE
        for merge_step in 0..num_merges {
            let best_pair = self.find_best_pair(&word_freqs);
            
            if let Some((left, right)) = best_pair {
                let merged = format!("{}{}", left, right);
                self.add_token(merged.clone());
                self.merges.push((left.clone(), right.clone()));
                
                println!("   Merge {}: '{}' + '{}' ‚Üí '{}'", 
                    merge_step + 1, left, right, merged);
                
                // Atualizar frequ√™ncias das palavras
                self.update_word_freqs(&mut word_freqs, &left, &right, &merged);
            } else {
                break;
            }
        }
        
        println!("‚úÖ BPE treinado: {} tokens no vocabul√°rio", self.vocab.len());
    }
    
    fn find_best_pair(&self, word_freqs: &HashMap<String, usize>) -> Option<(String, String)> {
        let mut pair_freqs = HashMap::new();
        
        for (word, freq) in word_freqs {
            let chars: Vec<char> = word.chars().collect();
            for i in 0..chars.len().saturating_sub(1) {
                let pair = (chars[i].to_string(), chars[i + 1].to_string());
                *pair_freqs.entry(pair).or_insert(0) += freq;
            }
        }
        
        pair_freqs.into_iter()
            .max_by_key(|(_, freq)| *freq)
            .map(|(pair, _)| pair)
    }
    
    fn update_word_freqs(
        &self,
        word_freqs: &mut HashMap<String, usize>,
        left: &str,
        right: &str,
        merged: &str,
    ) {
        let pattern = format!("{}{}", left, right);
        let mut new_word_freqs = HashMap::new();
        
        for (word, freq) in word_freqs.iter() {
            let new_word = word.replace(&pattern, merged);
            new_word_freqs.insert(new_word, *freq);
        }
        
        *word_freqs = new_word_freqs;
    }
    
    /// Aplica BPE para tokenizar uma palavra
    fn apply_bpe(&self, word: &str) -> Vec<String> {
        let mut tokens: Vec<String> = word.chars().map(|c| c.to_string()).collect();
        
        for (left, right) in &self.merges {
            let mut i = 0;
            while i < tokens.len().saturating_sub(1) {
                if tokens[i] == *left && tokens[i + 1] == *right {
                    let merged = format!("{}{}", left, right);
                    tokens[i] = merged;
                    tokens.remove(i + 1);
                } else {
                    i += 1;
                }
            }
        }
        
        tokens
    }
    
    fn encode(&self, text: &str) -> Vec<usize> {
        println!("\nüî§ Codificando com BPE: \"{}\"", text);
        
        let mut ids = vec![self.vocab["<BOS>"]];
        
        for word in text.split_whitespace() {
            let word = word.to_lowercase();
            let tokens = self.apply_bpe(&word);
            
            println!("   '{}' ‚Üí {:?}", word, tokens);
            
            for token in tokens {
                let id = self.vocab.get(&token)
                    .copied()
                    .unwrap_or(self.vocab["<UNK>"]);
                ids.push(id);
            }
        }
        
        ids.push(self.vocab["<EOS>"]);
        println!("üî¢ Sequ√™ncia BPE: {:?}", ids);
        
        ids
    }
}

/// ## 3. Demonstra√ß√£o Comparativa
fn demonstrate_tokenization() {
    println!("üöÄ === DEMONSTRA√á√ÉO DE TOKENIZA√á√ÉO ===");
    
    // Corpus de treinamento
    let training_texts = vec![
        "o gato subiu no telhado",
        "o cachorro desceu da escada",
        "a menina brinca no jardim",
        "o menino estuda na escola",
        "os p√°ssaros voam no c√©u",
        "as flores crescem no campo",
    ];
    
    let test_text = "o menino brinca com o cachorro";
    
    println!("\nüìö Corpus de treinamento:");
    for (i, text) in training_texts.iter().enumerate() {
        println!("   {}: \"{}\"", i + 1, text);
    }
    
    println!("\nüß™ Texto de teste: \"{}\"", test_text);
    
    // === TOKENIZA√á√ÉO POR PALAVRAS ===
    println!("\n\nüî§ === TOKENIZA√á√ÉO POR PALAVRAS ===");
    
    let mut word_tokenizer = WordTokenizer::new();
    word_tokenizer.train(&training_texts);
    
    println!("\nüìä Estat√≠sticas do vocabul√°rio:");
    println!("   - Tamanho: {} tokens", word_tokenizer.vocab_size());
    
    let word_ids = word_tokenizer.encode(test_text);
    let decoded_text = word_tokenizer.decode(&word_ids);
    
    println!("\nüîÑ Verifica√ß√£o de consist√™ncia:");
    println!("   Original: \"{}\"", test_text);
    println!("   Decodificado: \"{}\"", decoded_text);
    println!("   ‚úÖ Consistente: {}", test_text.to_lowercase() == decoded_text);
    
    // === TOKENIZA√á√ÉO BPE ===
    println!("\n\nüß¨ === TOKENIZA√á√ÉO BPE ===");
    
    let mut bpe_tokenizer = SimpleBPETokenizer::new();
    bpe_tokenizer.train(&training_texts, 10);
    
    println!("\nüìä Estat√≠sticas BPE:");
    println!("   - Tamanho do vocabul√°rio: {} tokens", bpe_tokenizer.vocab.len());
    println!("   - N√∫mero de merges: {}", bpe_tokenizer.merges.len());
    
    let bpe_ids = bpe_tokenizer.encode(test_text);
    
    // === COMPARA√á√ÉO ===
    println!("\n\n‚öñÔ∏è  === COMPARA√á√ÉO DE M√âTODOS ===");
    
    println!("\nüìè Efici√™ncia de compress√£o:");
    println!("   - Palavras: {} tokens ‚Üí {} IDs", 
        test_text.split_whitespace().count(), word_ids.len());
    println!("   - BPE: {} tokens ‚Üí {} IDs", 
        test_text.split_whitespace().count(), bpe_ids.len());
    
    println!("\nüíæ Tamanho do vocabul√°rio:");
    println!("   - Palavras: {} tokens", word_tokenizer.vocab_size());
    println!("   - BPE: {} tokens", bpe_tokenizer.vocab.len());
    
    // === AN√ÅLISE DE TOKENS DESCONHECIDOS ===
    println!("\n\n‚ùì === TESTE COM PALAVRAS DESCONHECIDAS ===");
    
    let unknown_text = "o elefante gigante dan√ßa";
    println!("Texto com palavras novas: \"{}\"", unknown_text);
    
    let _unknown_word_ids = word_tokenizer.encode(unknown_text);
    let _unknown_bpe_ids = bpe_tokenizer.encode(unknown_text);
    
    println!("\nüîç Como cada m√©todo lida com palavras desconhecidas:");
    println!("   - Palavras: usa <UNK> para tokens n√£o vistos");
    println!("   - BPE: decomp√µe em sub-palavras conhecidas");
}

/// Fun√ß√£o principal
fn main() {
    println!("üìñ === GUIA COMPLETO DE TOKENIZA√á√ÉO ===");
    println!("\nA tokeniza√ß√£o √© o primeiro passo no processamento de texto em LLMs.");
    println!("Vamos explorar diferentes abordagens e suas caracter√≠sticas.\n");
    
    demonstrate_tokenization();
    
    println!("\n\n=== COMPARA√á√ÉO DE EFICI√äNCIA ===");
    
    let sample_texts = vec![
        "Hello world! How are you today?",
        "The quick brown fox jumps over the lazy dog.",
        "Rust is a systems programming language.",
        "Machine learning models require large datasets.",
        "Natural language processing is fascinating!",
    ];
    
    // Tokenizador por palavras
    let mut word_tokenizer = WordTokenizer::new();
    word_tokenizer.train(&sample_texts);
    
    // Tokenizador BPE
    let mut bpe_tokenizer = SimpleBPETokenizer::new();
    bpe_tokenizer.train(&sample_texts, 50);
    
    println!("\nCompara√ß√£o de vocabul√°rios:");
    println!("Word Tokenizer: {} tokens", word_tokenizer.vocab_size());
    println!("BPE Tokenizer: {} tokens", bpe_tokenizer.vocab.len());
    
    // Testa com texto novo
    let test_text = "Programming languages are amazing tools!";
    
    let word_tokens = word_tokenizer.encode(test_text);
    let bpe_tokens = bpe_tokenizer.encode(test_text);
    
    println!("\nTexto de teste: '{}'", test_text);
    println!("Word tokens: {:?} ({})", word_tokens, word_tokens.len());
    println!("BPE tokens: {:?} ({})", bpe_tokens, bpe_tokens.len());
    
    // Decodifica de volta
    let word_decoded = word_tokenizer.decode(&word_tokens);
    
    println!("\nDecodifica√ß√£o:");
    println!("Word: '{}'", word_decoded);
    
    // EXERC√çCIOS PR√ÅTICOS
    println!("\n=== EXERC√çCIOS PR√ÅTICOS ===");
    exercicio_1_analise_vocabulario(&word_tokenizer, &bpe_tokenizer);
    exercicio_2_eficiencia_compressao(&sample_texts);
    exercicio_3_tratamento_oov();
    exercicio_4_tokenizacao_multilingual();
    exercicio_5_subword_analysis();
    exercicio_6_benchmark_performance();
    
    println!("\n\nüìö === CONCEITOS FUNDAMENTAIS ===");
    
    println!("\nüî§ TOKENIZA√á√ÉO POR PALAVRAS:");
    println!("   ‚úÖ Vantagens:");
    println!("      ‚Ä¢ Simples de implementar e entender");
    println!("      ‚Ä¢ Preserva significado sem√¢ntico das palavras");
    println!("      ‚Ä¢ Boa para idiomas com separa√ß√£o clara de palavras");
    println!("   ‚ùå Desvantagens:");
    println!("      ‚Ä¢ Vocabul√°rio muito grande");
    println!("      ‚Ä¢ Muitos tokens <UNK> para palavras raras");
    println!("      ‚Ä¢ N√£o lida bem com morfologia complexa");
    
    println!("\nüß¨ BYTE PAIR ENCODING (BPE):");
    println!("   ‚úÖ Vantagens:");
    println!("      ‚Ä¢ Vocabul√°rio de tamanho controlado");
    println!("      ‚Ä¢ Lida bem com palavras raras e novas");
    println!("      ‚Ä¢ Captura padr√µes morfol√≥gicos");
    println!("      ‚Ä¢ Usado em modelos modernos (GPT, BERT)");
    println!("   ‚ùå Desvantagens:");
    println!("      ‚Ä¢ Mais complexo de implementar");
    println!("      ‚Ä¢ Pode quebrar palavras de forma n√£o intuitiva");
    
    println!("\nüî¢ TOKENS ESPECIAIS:");
    println!("   ‚Ä¢ <PAD>: Preenchimento para sequ√™ncias de tamanho fixo");
    println!("   ‚Ä¢ <UNK>: Tokens desconhecidos ou raros");
    println!("   ‚Ä¢ <BOS>: In√≠cio de sequ√™ncia");
    println!("   ‚Ä¢ <EOS>: Fim de sequ√™ncia");
    
    println!("\nüéØ ESCOLHA DO M√âTODO:");
    println!("   ‚Ä¢ Palavras: Bom para prototipagem e dom√≠nios espec√≠ficos");
    println!("   ‚Ä¢ BPE: Padr√£o para modelos de produ√ß√£o");
    println!("   ‚Ä¢ SentencePiece: Extens√£o do BPE para mais idiomas");
    println!("   ‚Ä¢ WordPiece: Varia√ß√£o usada no BERT");
    
    println!("\n\nüéì === EXERC√çCIOS SUGERIDOS ===");
    println!("1. Implemente um tokenizador por caracteres");
    println!("2. Adicione suporte a diferentes idiomas");
    println!("3. Experimente com diferentes n√∫meros de merges BPE");
    println!("4. Implemente WordPiece tokenization");
    println!("5. Adicione m√©tricas de qualidade da tokeniza√ß√£o");
    
    println!("\n‚ú® Tokeniza√ß√£o conclu√≠da com sucesso! ‚ú®");
}

/// EXERC√çCIO 1: An√°lise Detalhada de Vocabul√°rio
fn exercicio_1_analise_vocabulario(word_tokenizer: &WordTokenizer, bpe_tokenizer: &SimpleBPETokenizer) {
    println!("\n--- Exerc√≠cio 1: An√°lise Detalhada de Vocabul√°rio ---");
    
    // Analisa distribui√ß√£o de frequ√™ncias
    let textos_analise = vec![
        "The cat sat on the mat. The dog ran in the park.",
        "Programming is fun. Rust programming is very fun.",
        "Machine learning and deep learning are related fields.",
    ];
    
    println!("\nAn√°lise de frequ√™ncia de tokens:");
    
    for (i, texto) in textos_analise.iter().enumerate() {
        let word_tokens = word_tokenizer.encode(texto);
        let bpe_tokens = bpe_tokenizer.encode(texto);
        
        println!("\nTexto {}: '{}'", i + 1, texto);
        println!("  Word tokenizer: {} tokens", word_tokens.len());
        println!("  BPE tokenizer: {} tokens", bpe_tokens.len());
        
        let compressao = (1.0 - bpe_tokens.len() as f32 / word_tokens.len() as f32) * 100.0;
        println!("  Compress√£o BPE: {:.1}%", compressao);
    }
    
    // Analisa tokens mais comuns
    println!("\nüí° Dica: BPE geralmente produz menos tokens ao combinar subpalavras frequentes!");
}

/// EXERC√çCIO 2: Efici√™ncia de Compress√£o
fn exercicio_2_eficiencia_compressao(sample_texts: &[&str]) {
    println!("\n--- Exerc√≠cio 2: Efici√™ncia de Compress√£o ---");
    
    let configuracoes_bpe = vec![10, 25, 50, 100, 200];
    
    println!("\nTestando diferentes n√∫meros de merges BPE:");
    
    for num_merges in configuracoes_bpe {
        let mut bpe = SimpleBPETokenizer::new();
        bpe.train(sample_texts, num_merges);
        
        let mut total_tokens = 0;
        let mut total_chars = 0;
        
        for text in sample_texts {
            let tokens = bpe.encode(text);
            total_tokens += tokens.len();
            total_chars += text.len();
        }
        
        let avg_tokens_per_char = total_tokens as f32 / total_chars as f32;
        let vocab_size = bpe.vocab.len();
        
        println!("  {} merges: vocab={}, tokens/char={:.3}", 
                num_merges, vocab_size, avg_tokens_per_char);
    }
    
    println!("\nüí° Dica: Mais merges = vocabul√°rio maior, mas melhor compress√£o!");
}

/// EXERC√çCIO 3: Tratamento de Palavras Fora do Vocabul√°rio (OOV)
fn exercicio_3_tratamento_oov() {
    println!("\n--- Exerc√≠cio 3: Tratamento de Palavras OOV ---");
    
    // Treina com vocabul√°rio limitado
    let textos_treino = vec![
        "cat dog bird",
        "run jump walk",
        "red blue green",
    ];
    
    let mut tokenizer = WordTokenizer::new();
    tokenizer.train(&textos_treino);
    
    // Testa com palavras novas
    let textos_teste = vec![
        "elephant tiger",  // Palavras completamente novas
        "running jumping", // Varia√ß√µes de palavras conhecidas
        "cat elephant",    // Mistura de conhecidas e novas
    ];
    
    println!("\nVocabul√°rio treinado: {} tokens", tokenizer.vocab_size());
    
    for texto in textos_teste {
        let tokens = tokenizer.encode(texto);
        let decoded = tokenizer.decode(&tokens);
        
        println!("\nOriginal: '{}'", texto);
        println!("Tokens: {:?}", tokens);
        println!("Decodificado: '{}'", decoded);
        
        // Conta tokens UNK
        let unk_id = tokenizer.vocab["<UNK>"];
        let unk_count = tokens.iter().filter(|&&id| id == unk_id).count();
        println!("Tokens UNK: {}/{}", unk_count, tokens.len());
    }
    
    println!("\nüí° Dica: BPE lida melhor com OOV ao quebrar em subpalavras!");
}

/// EXERC√çCIO 4: Tokeniza√ß√£o Multilingual
fn exercicio_4_tokenizacao_multilingual() {
    println!("\n--- Exerc√≠cio 4: Tokeniza√ß√£o Multilingual ---");
    
    let textos_multilingual = vec![
        "Hello world",           // Ingl√™s
        "Bonjour le monde",      // Franc√™s
        "Hola mundo",            // Espanhol
        "Ol√° mundo",             // Portugu√™s
        "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä",            // Russo
    ];
    
    let mut tokenizer_word = WordTokenizer::new();
    let mut tokenizer_bpe = SimpleBPETokenizer::new();
    
    tokenizer_word.train(&textos_multilingual);
    tokenizer_bpe.train(&textos_multilingual, 100);
    
    println!("\nAn√°lise de tokeniza√ß√£o multilingual:");
    
    for texto in &textos_multilingual {
        let word_tokens = tokenizer_word.encode(texto);
        let bpe_tokens = tokenizer_bpe.encode(texto);
        
        println!("\n'{}'", texto);
        println!("  Word: {} tokens", word_tokens.len());
        println!("  BPE: {} tokens", bpe_tokens.len());
        
        // Verifica se consegue decodificar corretamente
        let word_decoded = tokenizer_word.decode(&word_tokens);
        
        let word_match = word_decoded.trim() == texto;
        
        println!("  Decodifica√ß√£o correta: Word={}", word_match);
    }
    
    println!("\nüí° Dica: Caracteres especiais podem ser desafiadores para tokeniza√ß√£o!");
}

/// EXERC√çCIO 5: An√°lise de Subpalavras
fn exercicio_5_subword_analysis() {
    println!("\n--- Exerc√≠cio 5: An√°lise de Subpalavras ---");
    
    let palavras_complexas = vec![
        "unhappiness",
        "preprocessing",
        "internationalization",
        "antidisestablishmentarianism",
        "supercalifragilisticexpialidocious",
    ];
    
    let mut bpe = SimpleBPETokenizer::new();
    
    // Treina com palavras simples para for√ßar decomposi√ß√£o
    let treino_simples = vec![
        "happy sad good bad",
        "process data input output",
        "nation inter national",
        "establish ment arian ism",
        "super special fantastic",
    ];
    
    bpe.train(&treino_simples, 50);
    
    println!("\nDecomposi√ß√£o de palavras complexas:");
    
    for palavra in palavras_complexas {
        let tokens = bpe.encode(palavra);
        let subwords = bpe.apply_bpe(palavra);
        
        println!("\n'{}'", palavra);
        println!("  Subpalavras: {:?}", subwords);
        println!("  Tokens: {:?} ({})", tokens, tokens.len());
        
        let reconstruido = subwords.join("");
         let correto = reconstruido == *palavra;
         println!("  Reconstru√≠do: '{}'", reconstruido);
         println!("  Correto: {}", correto);
    }
    
    println!("\nüí° Dica: BPE quebra palavras desconhecidas em partes menores conhecidas!");
}

/// EXERC√çCIO 6: Benchmark de Performance
fn exercicio_6_benchmark_performance() {
    println!("\n--- Exerc√≠cio 6: Benchmark de Performance ---");
    
    // Gera texto de teste grande
    let texto_base = "The quick brown fox jumps over the lazy dog. ";
    let tamanhos = vec![100, 500, 1000, 5000];
    
    let mut word_tokenizer = WordTokenizer::new();
    let mut bpe_tokenizer = SimpleBPETokenizer::new();
    
    // Treina com texto pequeno
    word_tokenizer.train(&[texto_base]);
    bpe_tokenizer.train(&[texto_base], 50);
    
    println!("\nBenchmark de performance de tokeniza√ß√£o:");
    
    for tamanho in tamanhos {
        let texto_grande = texto_base.repeat(tamanho);
        
        // Benchmark Word Tokenizer
        let start = std::time::Instant::now();
        let word_tokens = word_tokenizer.encode(&texto_grande);
        let word_time = start.elapsed();
        
        // Benchmark BPE Tokenizer
        let start = std::time::Instant::now();
        let bpe_tokens = bpe_tokenizer.encode(&texto_grande);
        let bpe_time = start.elapsed();
        
        let chars = texto_grande.len();
        let word_speed = chars as f64 / word_time.as_secs_f64();
        let bpe_speed = chars as f64 / bpe_time.as_secs_f64();
        
        println!("\n{} repeti√ß√µes (~{} chars):", tamanho, chars);
        println!("  Word: {:?}, {} tokens, {:.0} chars/s", 
                word_time, word_tokens.len(), word_speed);
        println!("  BPE: {:?}, {} tokens, {:.0} chars/s", 
                bpe_time, bpe_tokens.len(), bpe_speed);
        
        let speedup = word_speed / bpe_speed;
        println!("  Word √© {:.1}x mais r√°pido", speedup);
    }
    
    println!("\nüí° Dica: Word tokenization √© mais simples e r√°pida, mas BPE √© mais eficiente!");
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_word_tokenizer() {
        let mut tokenizer = WordTokenizer::new();
        tokenizer.train(&["hello world", "world peace"]);
        
        let ids = tokenizer.encode("hello world");
        let decoded = tokenizer.decode(&ids);
        
        assert_eq!(decoded, "hello world");
    }

    #[test]
    fn test_bpe_tokenizer() {
        let mut tokenizer = SimpleBPETokenizer::new();
        tokenizer.train(&["hello", "world", "help"], 2);
        
        assert!(tokenizer.vocab.len() > 4); // Mais que tokens especiais
    }

    #[test]
    fn test_special_tokens() {
        let tokenizer = WordTokenizer::new();
        
        assert!(tokenizer.vocab.contains_key("<PAD>"));
        assert!(tokenizer.vocab.contains_key("<UNK>"));
        assert!(tokenizer.vocab.contains_key("<BOS>"));
        assert!(tokenizer.vocab.contains_key("<EOS>"));
    }
}