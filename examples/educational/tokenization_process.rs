//! # Exemplo Did√°tico: Processo de Tokeniza√ß√£o
//!
//! Este exemplo demonstra como o texto √© processado em modelos de linguagem:
//! - Divis√£o do texto em tokens
//! - Convers√£o de tokens para IDs num√©ricos
//! - Diferentes estrat√©gias de tokeniza√ß√£o
//! - Encoding e decoding de sequ√™ncias
//!
//! ## Como executar:
//! ```bash
//! cargo run --example tokenization_process
//! ```

use std::collections::HashMap;
use std::collections::BTreeMap;

/// Representa um token individual
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
struct Token {
    text: String,
    id: usize,
}

impl Token {
    fn new(text: String, id: usize) -> Self {
        Self { text, id }
    }
}

/// ## 1. Tokenizador Simples por Palavras
/// 
/// A forma mais b√°sica de tokeniza√ß√£o: dividir por espa√ßos e pontua√ß√£o
#[derive(Debug)]
struct WordTokenizer {
    vocab: HashMap<String, usize>,
    reverse_vocab: HashMap<usize, String>,
    next_id: usize,
}

impl WordTokenizer {
    fn new() -> Self {
        let mut tokenizer = Self {
            vocab: HashMap::new(),
            reverse_vocab: HashMap::new(),
            next_id: 0,
        };
        
        // Adicionar tokens especiais
        tokenizer.add_special_tokens();
        tokenizer
    }
    
    fn add_special_tokens(&mut self) {
        let special_tokens = vec![
            "<PAD>",   // Padding
            "<UNK>",   // Unknown
            "<BOS>",   // Beginning of sequence
            "<EOS>",   // End of sequence
        ];
        
        for token in special_tokens {
            self.add_token(token.to_string());
        }
    }
    
    fn add_token(&mut self, token: String) -> usize {
        if let Some(&id) = self.vocab.get(&token) {
            return id;
        }
        
        let id = self.next_id;
        self.vocab.insert(token.clone(), id);
        self.reverse_vocab.insert(id, token);
        self.next_id += 1;
        id
    }
    
    /// Treina o tokenizador com um corpus de texto
    fn train(&mut self, texts: &[&str]) {
        println!("üéì Treinando tokenizador com {} textos...", texts.len());
        
        for text in texts {
            let words = self.simple_tokenize(text);
            for word in words {
                self.add_token(word);
            }
        }
        
        println!("‚úÖ Vocabul√°rio criado com {} tokens", self.vocab.len());
    }
    
    /// Tokeniza√ß√£o simples: divide por espa√ßos e pontua√ß√£o
    fn simple_tokenize(&self, text: &str) -> Vec<String> {
        let mut tokens = Vec::new();
        let mut current_word = String::new();
        
        for ch in text.chars() {
            if ch.is_whitespace() {
                if !current_word.is_empty() {
                    tokens.push(current_word.clone());
                    current_word.clear();
                }
            } else if ch.is_ascii_punctuation() {
                if !current_word.is_empty() {
                    tokens.push(current_word.clone());
                    current_word.clear();
                }
                tokens.push(ch.to_string());
            } else {
                current_word.push(ch.to_lowercase().next().unwrap_or(ch));
            }
        }
        
        if !current_word.is_empty() {
            tokens.push(current_word);
        }
        
        tokens
    }
    
    /// Converte texto em sequ√™ncia de IDs
    fn encode(&self, text: &str) -> Vec<usize> {
        println!("\nüî§ Codificando texto: \"{}\"", text);
        
        let tokens = self.simple_tokenize(text);
        println!("üìù Tokens extra√≠dos: {:?}", tokens);
        
        let mut ids = vec![self.vocab["<BOS>"]];
        
        for token in tokens {
            let id = self.vocab.get(&token)
                .copied()
                .unwrap_or(self.vocab["<UNK>"]);
            ids.push(id);
            
            if self.vocab.contains_key(&token) {
                println!("   '{}' ‚Üí ID {}", token, id);
            } else {
                println!("   '{}' ‚Üí ID {} (<UNK>)", token, id);
            }
        }
        
        ids.push(self.vocab["<EOS>"]);
        println!("üî¢ Sequ√™ncia final: {:?}", ids);
        
        ids
    }
    
    /// Converte sequ√™ncia de IDs de volta para texto
    fn decode(&self, ids: &[usize]) -> String {
        println!("\nüî¢ Decodificando IDs: {:?}", ids);
        
        let tokens: Vec<String> = ids.iter()
            .filter_map(|&id| self.reverse_vocab.get(&id))
            .filter(|token| !matches!(token.as_str(), "<BOS>" | "<EOS>" | "<PAD>"))
            .cloned()
            .collect();
        
        println!("üìù Tokens recuperados: {:?}", tokens);
        
        let text = tokens.join(" ");
        println!("üìÑ Texto final: \"{}\"", text);
        
        text
    }
    
    fn vocab_size(&self) -> usize {
        self.vocab.len()
    }
}

/// ## 2. Tokenizador BPE Simplificado
/// 
/// Byte Pair Encoding: une os pares de caracteres mais frequentes
#[derive(Debug)]
struct SimpleBPETokenizer {
    vocab: HashMap<String, usize>,
    reverse_vocab: HashMap<usize, String>,
    merges: Vec<(String, String)>,
    next_id: usize,
}

impl SimpleBPETokenizer {
    fn new() -> Self {
        let mut tokenizer = Self {
            vocab: HashMap::new(),
            reverse_vocab: HashMap::new(),
            merges: Vec::new(),
            next_id: 0,
        };
        
        tokenizer.add_special_tokens();
        tokenizer
    }
    
    fn add_special_tokens(&mut self) {
        let special_tokens = vec!["<PAD>", "<UNK>", "<BOS>", "<EOS>"];
        for token in special_tokens {
            self.add_token(token.to_string());
        }
    }
    
    fn add_token(&mut self, token: String) -> usize {
        if let Some(&id) = self.vocab.get(&token) {
            return id;
        }
        
        let id = self.next_id;
        self.vocab.insert(token.clone(), id);
        self.reverse_vocab.insert(id, token);
        self.next_id += 1;
        id
    }
    
    /// Treina o BPE com um corpus
    fn train(&mut self, texts: &[&str], num_merges: usize) {
        println!("\nüß† Treinando BPE com {} merges...", num_merges);
        
        // 1. Inicializar vocabul√°rio com caracteres individuais
        let mut word_freqs = HashMap::new();
        
        for text in texts {
            for word in text.split_whitespace() {
                let word = word.to_lowercase();
                *word_freqs.entry(word.clone()).or_insert(0) += 1;
                
                // Adicionar caracteres individuais ao vocabul√°rio
                for ch in word.chars() {
                    self.add_token(ch.to_string());
                }
            }
        }
        
        println!("üìö Vocabul√°rio inicial: {} caracteres", self.vocab.len() - 4);
        
        // 2. Executar merges BPE
        for merge_step in 0..num_merges {
            let best_pair = self.find_best_pair(&word_freqs);
            
            if let Some((left, right)) = best_pair {
                let merged = format!("{}{}", left, right);
                self.add_token(merged.clone());
                self.merges.push((left.clone(), right.clone()));
                
                println!("   Merge {}: '{}' + '{}' ‚Üí '{}'", 
                    merge_step + 1, left, right, merged);
                
                // Atualizar frequ√™ncias das palavras
                self.update_word_freqs(&mut word_freqs, &left, &right, &merged);
            } else {
                break;
            }
        }
        
        println!("‚úÖ BPE treinado: {} tokens no vocabul√°rio", self.vocab.len());
    }
    
    fn find_best_pair(&self, word_freqs: &HashMap<String, usize>) -> Option<(String, String)> {
        let mut pair_freqs = HashMap::new();
        
        for (word, freq) in word_freqs {
            let chars: Vec<char> = word.chars().collect();
            for i in 0..chars.len().saturating_sub(1) {
                let pair = (chars[i].to_string(), chars[i + 1].to_string());
                *pair_freqs.entry(pair).or_insert(0) += freq;
            }
        }
        
        pair_freqs.into_iter()
            .max_by_key(|(_, freq)| *freq)
            .map(|(pair, _)| pair)
    }
    
    fn update_word_freqs(
        &self,
        word_freqs: &mut HashMap<String, usize>,
        left: &str,
        right: &str,
        merged: &str,
    ) {
        let pattern = format!("{}{}", left, right);
        let mut new_word_freqs = HashMap::new();
        
        for (word, freq) in word_freqs.iter() {
            let new_word = word.replace(&pattern, merged);
            new_word_freqs.insert(new_word, *freq);
        }
        
        *word_freqs = new_word_freqs;
    }
    
    /// Aplica BPE para tokenizar uma palavra
    fn apply_bpe(&self, word: &str) -> Vec<String> {
        let mut tokens: Vec<String> = word.chars().map(|c| c.to_string()).collect();
        
        for (left, right) in &self.merges {
            let mut i = 0;
            while i < tokens.len().saturating_sub(1) {
                if tokens[i] == *left && tokens[i + 1] == *right {
                    let merged = format!("{}{}", left, right);
                    tokens[i] = merged;
                    tokens.remove(i + 1);
                } else {
                    i += 1;
                }
            }
        }
        
        tokens
    }
    
    fn encode(&self, text: &str) -> Vec<usize> {
        println!("\nüî§ Codificando com BPE: \"{}\"", text);
        
        let mut ids = vec![self.vocab["<BOS>"]];
        
        for word in text.split_whitespace() {
            let word = word.to_lowercase();
            let tokens = self.apply_bpe(&word);
            
            println!("   '{}' ‚Üí {:?}", word, tokens);
            
            for token in tokens {
                let id = self.vocab.get(&token)
                    .copied()
                    .unwrap_or(self.vocab["<UNK>"]);
                ids.push(id);
            }
        }
        
        ids.push(self.vocab["<EOS>"]);
        println!("üî¢ Sequ√™ncia BPE: {:?}", ids);
        
        ids
    }
}

/// ## 3. Demonstra√ß√£o Comparativa
fn demonstrate_tokenization() {
    println!("üöÄ === DEMONSTRA√á√ÉO DE TOKENIZA√á√ÉO ===");
    
    // Corpus de treinamento
    let training_texts = vec![
        "o gato subiu no telhado",
        "o cachorro desceu da escada",
        "a menina brinca no jardim",
        "o menino estuda na escola",
        "os p√°ssaros voam no c√©u",
        "as flores crescem no campo",
    ];
    
    let test_text = "o menino brinca com o cachorro";
    
    println!("\nüìö Corpus de treinamento:");
    for (i, text) in training_texts.iter().enumerate() {
        println!("   {}: \"{}\"", i + 1, text);
    }
    
    println!("\nüß™ Texto de teste: \"{}\"", test_text);
    
    // === TOKENIZA√á√ÉO POR PALAVRAS ===
    println!("\n\nüî§ === TOKENIZA√á√ÉO POR PALAVRAS ===");
    
    let mut word_tokenizer = WordTokenizer::new();
    word_tokenizer.train(&training_texts);
    
    println!("\nüìä Estat√≠sticas do vocabul√°rio:");
    println!("   - Tamanho: {} tokens", word_tokenizer.vocab_size());
    
    let word_ids = word_tokenizer.encode(test_text);
    let decoded_text = word_tokenizer.decode(&word_ids);
    
    println!("\nüîÑ Verifica√ß√£o de consist√™ncia:");
    println!("   Original: \"{}\"", test_text);
    println!("   Decodificado: \"{}\"", decoded_text);
    println!("   ‚úÖ Consistente: {}", test_text.to_lowercase() == decoded_text);
    
    // === TOKENIZA√á√ÉO BPE ===
    println!("\n\nüß¨ === TOKENIZA√á√ÉO BPE ===");
    
    let mut bpe_tokenizer = SimpleBPETokenizer::new();
    bpe_tokenizer.train(&training_texts, 10);
    
    println!("\nüìä Estat√≠sticas BPE:");
    println!("   - Tamanho do vocabul√°rio: {} tokens", bpe_tokenizer.vocab.len());
    println!("   - N√∫mero de merges: {}", bpe_tokenizer.merges.len());
    
    let bpe_ids = bpe_tokenizer.encode(test_text);
    
    // === COMPARA√á√ÉO ===
    println!("\n\n‚öñÔ∏è  === COMPARA√á√ÉO DE M√âTODOS ===");
    
    println!("\nüìè Efici√™ncia de compress√£o:");
    println!("   - Palavras: {} tokens ‚Üí {} IDs", 
        test_text.split_whitespace().count(), word_ids.len());
    println!("   - BPE: {} tokens ‚Üí {} IDs", 
        test_text.split_whitespace().count(), bpe_ids.len());
    
    println!("\nüíæ Tamanho do vocabul√°rio:");
    println!("   - Palavras: {} tokens", word_tokenizer.vocab_size());
    println!("   - BPE: {} tokens", bpe_tokenizer.vocab.len());
    
    // === AN√ÅLISE DE TOKENS DESCONHECIDOS ===
    println!("\n\n‚ùì === TESTE COM PALAVRAS DESCONHECIDAS ===");
    
    let unknown_text = "o elefante gigante dan√ßa";
    println!("Texto com palavras novas: \"{}\"", unknown_text);
    
    let _unknown_word_ids = word_tokenizer.encode(unknown_text);
    let _unknown_bpe_ids = bpe_tokenizer.encode(unknown_text);
    
    println!("\nüîç Como cada m√©todo lida com palavras desconhecidas:");
    println!("   - Palavras: usa <UNK> para tokens n√£o vistos");
    println!("   - BPE: decomp√µe em sub-palavras conhecidas");
}

/// Fun√ß√£o principal
fn main() {
    println!("üìñ === GUIA COMPLETO DE TOKENIZA√á√ÉO ===");
    println!("\nA tokeniza√ß√£o √© o primeiro passo no processamento de texto em LLMs.");
    println!("Vamos explorar diferentes abordagens e suas caracter√≠sticas.\n");
    
    demonstrate_tokenization();
    
    println!("\n\nüìö === CONCEITOS FUNDAMENTAIS ===");
    
    println!("\nüî§ TOKENIZA√á√ÉO POR PALAVRAS:");
    println!("   ‚úÖ Vantagens:");
    println!("      ‚Ä¢ Simples de implementar e entender");
    println!("      ‚Ä¢ Preserva significado sem√¢ntico das palavras");
    println!("      ‚Ä¢ Boa para idiomas com separa√ß√£o clara de palavras");
    println!("   ‚ùå Desvantagens:");
    println!("      ‚Ä¢ Vocabul√°rio muito grande");
    println!("      ‚Ä¢ Muitos tokens <UNK> para palavras raras");
    println!("      ‚Ä¢ N√£o lida bem com morfologia complexa");
    
    println!("\nüß¨ BYTE PAIR ENCODING (BPE):");
    println!("   ‚úÖ Vantagens:");
    println!("      ‚Ä¢ Vocabul√°rio de tamanho controlado");
    println!("      ‚Ä¢ Lida bem com palavras raras e novas");
    println!("      ‚Ä¢ Captura padr√µes morfol√≥gicos");
    println!("      ‚Ä¢ Usado em modelos modernos (GPT, BERT)");
    println!("   ‚ùå Desvantagens:");
    println!("      ‚Ä¢ Mais complexo de implementar");
    println!("      ‚Ä¢ Pode quebrar palavras de forma n√£o intuitiva");
    
    println!("\nüî¢ TOKENS ESPECIAIS:");
    println!("   ‚Ä¢ <PAD>: Preenchimento para sequ√™ncias de tamanho fixo");
    println!("   ‚Ä¢ <UNK>: Tokens desconhecidos ou raros");
    println!("   ‚Ä¢ <BOS>: In√≠cio de sequ√™ncia");
    println!("   ‚Ä¢ <EOS>: Fim de sequ√™ncia");
    
    println!("\nüéØ ESCOLHA DO M√âTODO:");
    println!("   ‚Ä¢ Palavras: Bom para prototipagem e dom√≠nios espec√≠ficos");
    println!("   ‚Ä¢ BPE: Padr√£o para modelos de produ√ß√£o");
    println!("   ‚Ä¢ SentencePiece: Extens√£o do BPE para mais idiomas");
    println!("   ‚Ä¢ WordPiece: Varia√ß√£o usada no BERT");
    
    println!("\n\nüéì === EXERC√çCIOS SUGERIDOS ===");
    println!("1. Implemente um tokenizador por caracteres");
    println!("2. Adicione suporte a diferentes idiomas");
    println!("3. Experimente com diferentes n√∫meros de merges BPE");
    println!("4. Implemente WordPiece tokenization");
    println!("5. Adicione m√©tricas de qualidade da tokeniza√ß√£o");
    
    println!("\n‚ú® Tokeniza√ß√£o conclu√≠da com sucesso! ‚ú®");
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_word_tokenizer() {
        let mut tokenizer = WordTokenizer::new();
        tokenizer.train(&["hello world", "world peace"]);
        
        let ids = tokenizer.encode("hello world");
        let decoded = tokenizer.decode(&ids);
        
        assert_eq!(decoded, "hello world");
    }

    #[test]
    fn test_bpe_tokenizer() {
        let mut tokenizer = SimpleBPETokenizer::new();
        tokenizer.train(&["hello", "world", "help"], 2);
        
        assert!(tokenizer.vocab.len() > 4); // Mais que tokens especiais
    }

    #[test]
    fn test_special_tokens() {
        let tokenizer = WordTokenizer::new();
        
        assert!(tokenizer.vocab.contains_key("<PAD>"));
        assert!(tokenizer.vocab.contains_key("<UNK>"));
        assert!(tokenizer.vocab.contains_key("<BOS>"));
        assert!(tokenizer.vocab.contains_key("<EOS>"));
    }
}