//! # Exemplo Did√°tico: Embeddings e Representa√ß√µes Vetoriais
//!
//! Este exemplo demonstra como funcionam os embeddings em modelos de linguagem:
//! - Cria√ß√£o de embeddings de tokens
//! - Embeddings posicionais
//! - Opera√ß√µes com vetores de embedding
//! - Similaridade sem√¢ntica
//! - Visualiza√ß√£o de rela√ß√µes
//!
//! ## Como executar:
//! ```bash
//! cargo run --example embeddings_explained
//! ```

use std::collections::HashMap;
use std::f32::consts::PI;

/// Representa um vetor de embedding
#[derive(Debug, Clone)]
struct Embedding {
    vector: Vec<f32>,
    dimension: usize,
}

impl Embedding {
    /// Cria um novo embedding com valores aleat√≥rios
    fn new(dimension: usize) -> Self {
        let vector: Vec<f32> = (0..dimension)
            .map(|i| ((i as f32 * 0.1).sin() * 0.5))
            .collect();
        
        Self { vector, dimension }
    }
    
    /// Cria um embedding a partir de valores espec√≠ficos
    fn from_values(values: Vec<f32>) -> Self {
        let dimension = values.len();
        Self {
            vector: values,
            dimension,
        }
    }
    
    /// Calcula a similaridade cosseno entre dois embeddings
    fn cosine_similarity(&self, other: &Embedding) -> f32 {
        assert_eq!(self.dimension, other.dimension);
        
        let dot_product: f32 = self.vector.iter()
            .zip(other.vector.iter())
            .map(|(a, b)| a * b)
            .sum();
        
        let norm_a: f32 = self.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
        let norm_b: f32 = other.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
        
        if norm_a == 0.0 || norm_b == 0.0 {
            0.0
        } else {
            dot_product / (norm_a * norm_b)
        }
    }
    
    /// Adiciona dois embeddings (usado em conex√µes residuais)
    fn add(&self, other: &Embedding) -> Embedding {
        assert_eq!(self.dimension, other.dimension);
        
        let vector: Vec<f32> = self.vector.iter()
            .zip(other.vector.iter())
            .map(|(a, b)| a + b)
            .collect();
        
        Embedding::from_values(vector)
    }
    
    /// Multiplica o embedding por um escalar
    fn scale(&self, factor: f32) -> Embedding {
        let vector: Vec<f32> = self.vector.iter()
            .map(|x| x * factor)
            .collect();
        
        Embedding::from_values(vector)
    }
    
    /// Normaliza o embedding para ter norma unit√°ria
    fn normalize(&self) -> Embedding {
        let norm: f32 = self.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
        
        if norm == 0.0 {
            return self.clone();
        }
        
        let vector: Vec<f32> = self.vector.iter()
            .map(|x| x / norm)
            .collect();
        
        Embedding::from_values(vector)
    }
    
    /// Calcula a dist√¢ncia euclidiana
    fn euclidean_distance(&self, other: &Embedding) -> f32 {
        assert_eq!(self.dimension, other.dimension);
        
        self.vector.iter()
            .zip(other.vector.iter())
            .map(|(a, b)| (a - b).powi(2))
            .sum::<f32>()
            .sqrt()
    }
}

/// ## 1. Camada de Embedding de Tokens
/// 
/// Converte IDs de tokens em vetores densos de dimens√£o fixa
#[derive(Debug)]
struct TokenEmbedding {
    embeddings: HashMap<usize, Embedding>,
    vocab_size: usize,
    embedding_dim: usize,
}

impl TokenEmbedding {
    fn new(vocab_size: usize, embedding_dim: usize) -> Self {
        let mut embeddings = HashMap::new();
        
        // Inicializar embeddings para cada token no vocabul√°rio
        for token_id in 0..vocab_size {
            // Usar uma semente baseada no ID para embeddings consistentes
            let embedding = Self::create_deterministic_embedding(token_id, embedding_dim);
            embeddings.insert(token_id, embedding);
        }
        
        Self {
            embeddings,
            vocab_size,
            embedding_dim,
        }
    }
    
    /// Cria um embedding determin√≠stico baseado no ID do token
    fn create_deterministic_embedding(token_id: usize, dim: usize) -> Embedding {
        let mut vector = Vec::with_capacity(dim);
        
        for i in 0..dim {
            // Usar fun√ß√µes trigonom√©tricas para criar padr√µes √∫nicos
            let value = ((token_id as f32 * 0.1 + i as f32 * 0.05).sin() * 0.5) +
                       ((token_id as f32 * 0.07 + i as f32 * 0.03).cos() * 0.3);
            vector.push(value);
        }
        
        Embedding::from_values(vector)
    }
    
    /// Obt√©m o embedding para um token espec√≠fico
    fn get_embedding(&self, token_id: usize) -> Option<&Embedding> {
        self.embeddings.get(&token_id)
    }
    
    /// Processa uma sequ√™ncia de tokens
    fn forward(&self, token_ids: &[usize]) -> Vec<Embedding> {
        println!("\nüî§ Convertendo tokens em embeddings...");
        println!("Sequ√™ncia de entrada: {:?}", token_ids);
        
        let mut embeddings = Vec::new();
        
        for &token_id in token_ids {
            if let Some(embedding) = self.get_embedding(token_id) {
                embeddings.push(embedding.clone());
                println!("   Token {} ‚Üí Embedding[{}] (primeiros 4 valores: {:?})", 
                    token_id, 
                    self.embedding_dim,
                    &embedding.vector[0..4.min(embedding.vector.len())]
                );
            } else {
                println!("   ‚ö†Ô∏è  Token {} n√£o encontrado no vocabul√°rio!", token_id);
            }
        }
        
        println!("‚úÖ {} embeddings gerados", embeddings.len());
        embeddings
    }
}

/// ## 2. Embeddings Posicionais
/// 
/// Adicionam informa√ß√£o sobre a posi√ß√£o dos tokens na sequ√™ncia
#[derive(Debug)]
struct PositionalEmbedding {
    max_length: usize,
    embedding_dim: usize,
    embeddings: Vec<Embedding>,
}

impl PositionalEmbedding {
    fn new(max_length: usize, embedding_dim: usize) -> Self {
        let mut embeddings = Vec::with_capacity(max_length);
        
        // Gerar embeddings posicionais usando encoding sinusoidal
        for pos in 0..max_length {
            embeddings.push(Self::create_sinusoidal_embedding(pos, embedding_dim));
        }
        
        Self {
            max_length,
            embedding_dim,
            embeddings,
        }
    }
    
    /// Cria embedding posicional usando fun√ß√µes sinusoidais
    /// 
    /// F√≥rmula do paper "Attention Is All You Need":
    /// PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
    /// PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    fn create_sinusoidal_embedding(position: usize, dim: usize) -> Embedding {
        let mut vector = Vec::with_capacity(dim);
        
        for i in 0..dim {
            let angle = position as f32 / 10000_f32.powf(2.0 * (i / 2) as f32 / dim as f32);
            
            let value = if i % 2 == 0 {
                angle.sin()  // Posi√ß√µes pares: seno
            } else {
                angle.cos()  // Posi√ß√µes √≠mpares: cosseno
            };
            
            vector.push(value);
        }
        
        Embedding::from_values(vector)
    }
    
    /// Obt√©m embeddings posicionais para uma sequ√™ncia
    fn forward(&self, sequence_length: usize) -> Vec<Embedding> {
        println!("\nüìç Gerando embeddings posicionais...");
        println!("Comprimento da sequ√™ncia: {}", sequence_length);
        
        let mut pos_embeddings = Vec::new();
        
        for pos in 0..sequence_length {
            if pos < self.max_length {
                pos_embeddings.push(self.embeddings[pos].clone());
                println!("   Posi√ß√£o {} ‚Üí Embedding posicional (primeiros 4: {:?})", 
                    pos, 
                    &self.embeddings[pos].vector[0..4.min(self.embedding_dim)]
                );
            } else {
                println!("   ‚ö†Ô∏è  Posi√ß√£o {} excede comprimento m√°ximo {}", pos, self.max_length);
                break;
            }
        }
        
        println!("‚úÖ {} embeddings posicionais gerados", pos_embeddings.len());
        pos_embeddings
    }
}

/// ## 3. Combina√ß√£o de Embeddings
/// 
/// Combina embeddings de tokens e posicionais
#[derive(Debug)]
struct EmbeddingLayer {
    token_embedding: TokenEmbedding,
    positional_embedding: PositionalEmbedding,
}

impl EmbeddingLayer {
    fn new(vocab_size: usize, embedding_dim: usize, max_length: usize) -> Self {
        Self {
            token_embedding: TokenEmbedding::new(vocab_size, embedding_dim),
            positional_embedding: PositionalEmbedding::new(max_length, embedding_dim),
        }
    }
    
    /// Processa uma sequ√™ncia completa
    fn forward(&self, token_ids: &[usize]) -> Vec<Embedding> {
        println!("\nüèóÔ∏è  === CAMADA DE EMBEDDING COMPLETA ===");
        
        // 1. Obter embeddings de tokens
        let token_embeddings = self.token_embedding.forward(token_ids);
        
        // 2. Obter embeddings posicionais
        let pos_embeddings = self.positional_embedding.forward(token_ids.len());
        
        // 3. Combinar embeddings (soma elemento a elemento)
        println!("\n‚ûï Combinando embeddings de tokens e posicionais...");
        let mut combined_embeddings = Vec::new();
        
        for (i, (token_emb, pos_emb)) in token_embeddings.iter().zip(pos_embeddings.iter()).enumerate() {
            let combined = token_emb.add(pos_emb);
            combined_embeddings.push(combined);
            
            println!("   Posi√ß√£o {}: Token + Posicional ‚Üí Embedding final", i);
        }
        
        println!("‚úÖ Embeddings finais prontos para o Transformer!");
        combined_embeddings
    }
}

/// ## 4. An√°lise de Similaridade Sem√¢ntica
struct SemanticAnalyzer {
    embedding_layer: EmbeddingLayer,
    vocabulary: HashMap<String, usize>,
}

impl SemanticAnalyzer {
    fn new() -> Self {
        // Criar vocabul√°rio de exemplo
        let mut vocabulary = HashMap::new();
        let words = vec![
            "gato", "cachorro", "animal", "pet",
            "carro", "bicicleta", "transporte", "ve√≠culo",
            "casa", "apartamento", "moradia", "lar",
            "livro", "revista", "jornal", "leitura",
            "feliz", "alegre", "contente", "satisfeito",
        ];
        
        for (i, word) in words.iter().enumerate() {
            vocabulary.insert(word.to_string(), i);
        }
        
        let embedding_layer = EmbeddingLayer::new(words.len(), 64, 100);
        
        Self {
            embedding_layer,
            vocabulary,
        }
    }
    
    fn get_word_embedding(&self, word: &str) -> Option<Embedding> {
        if let Some(&token_id) = self.vocabulary.get(word) {
            self.embedding_layer.token_embedding.get_embedding(token_id).cloned()
        } else {
            None
        }
    }
    
    fn find_similar_words(&self, target_word: &str, top_k: usize) -> Vec<(String, f32)> {
        if let Some(target_embedding) = self.get_word_embedding(target_word) {
            let mut similarities = Vec::new();
            
            for (word, &token_id) in &self.vocabulary {
                if word != target_word {
                    if let Some(word_embedding) = self.embedding_layer.token_embedding.get_embedding(token_id) {
                        let similarity = target_embedding.cosine_similarity(word_embedding);
                        similarities.push((word.clone(), similarity));
                    }
                }
            }
            
            // Ordenar por similaridade (maior primeiro)
            similarities.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
            similarities.into_iter().take(top_k).collect()
        } else {
            Vec::new()
        }
    }
    
    fn demonstrate_semantic_relationships(&self) {
        println!("\n\nüîç === AN√ÅLISE DE SIMILARIDADE SEM√ÇNTICA ===");
        
        let test_words = vec!["gato", "carro", "casa", "feliz"];
        
        for word in test_words {
            println!("\nüéØ Palavras similares a '{}':", word);
            let similar = self.find_similar_words(word, 3);
            
            for (i, (similar_word, similarity)) in similar.iter().enumerate() {
                println!("   {}. {} (similaridade: {:.3})", 
                    i + 1, similar_word, similarity);
            }
        }
    }
}

/// ## 5. Demonstra√ß√£o de Opera√ß√µes Vetoriais
fn demonstrate_vector_operations() {
    println!("\n\nüßÆ === OPERA√á√ïES COM EMBEDDINGS ===");
    
    // Criar embeddings de exemplo
    let emb1 = Embedding::from_values(vec![1.0, 2.0, 3.0, 4.0]);
    let emb2 = Embedding::from_values(vec![2.0, 1.0, 4.0, 3.0]);
    let emb3 = Embedding::from_values(vec![1.0, 2.0, 3.0, 4.0]); // Id√™ntico ao emb1
    
    println!("\nüìä Embeddings de teste:");
    println!("   Embedding 1: {:?}", emb1.vector);
    println!("   Embedding 2: {:?}", emb2.vector);
    println!("   Embedding 3: {:?}", emb3.vector);
    
    // Similaridade cosseno
    println!("\nüìê Similaridades cosseno:");
    println!("   emb1 ‚Üî emb2: {:.3}", emb1.cosine_similarity(&emb2));
    println!("   emb1 ‚Üî emb3: {:.3}", emb1.cosine_similarity(&emb3));
    println!("   emb2 ‚Üî emb3: {:.3}", emb2.cosine_similarity(&emb3));
    
    // Dist√¢ncia euclidiana
    println!("\nüìè Dist√¢ncias euclidianas:");
    println!("   emb1 ‚Üî emb2: {:.3}", emb1.euclidean_distance(&emb2));
    println!("   emb1 ‚Üî emb3: {:.3}", emb1.euclidean_distance(&emb3));
    println!("   emb2 ‚Üî emb3: {:.3}", emb2.euclidean_distance(&emb3));
    
    // Opera√ß√µes aritm√©ticas
    println!("\n‚ûï Opera√ß√µes aritm√©ticas:");
    let sum = emb1.add(&emb2);
    println!("   emb1 + emb2: {:?}", sum.vector);
    
    let scaled = emb1.scale(2.0);
    println!("   emb1 √ó 2: {:?}", scaled.vector);
    
    let normalized = emb1.normalize();
    println!("   emb1 normalizado: {:?}", 
        normalized.vector.iter().map(|x| format!("{:.3}", x)).collect::<Vec<_>>());
}

/// Fun√ß√£o principal
fn main() {
    println!("üöÄ === GUIA COMPLETO DE EMBEDDINGS ===");
    println!("\nEmbeddings s√£o representa√ß√µes vetoriais densas que capturam");
    println!("significado sem√¢ntico e rela√ß√µes entre tokens.\n");
    
    // === DEMONSTRA√á√ÉO B√ÅSICA ===
    println!("\nüìö === DEMONSTRA√á√ÉO B√ÅSICA ===");
    
    let vocab_size = 1000;
    let embedding_dim = 128;
    let max_length = 50;
    
    println!("\n‚öôÔ∏è  Configura√ß√µes:");
    println!("   - Tamanho do vocabul√°rio: {}", vocab_size);
    println!("   - Dimens√£o dos embeddings: {}", embedding_dim);
    println!("   - Comprimento m√°ximo: {}", max_length);
    
    // Criar camada de embedding
    let embedding_layer = EmbeddingLayer::new(vocab_size, embedding_dim, max_length);
    
    // Sequ√™ncia de exemplo
    let token_sequence = vec![10, 25, 7, 42, 3];
    println!("\nüî¢ Sequ√™ncia de tokens: {:?}", token_sequence);
    
    // Processar atrav√©s da camada de embedding
    let embeddings = embedding_layer.forward(&token_sequence);
    
    println!("\nüìä Resultado:");
    println!("   - {} embeddings gerados", embeddings.len());
    println!("   - Cada embedding tem {} dimens√µes", embedding_dim);
    
    // === OPERA√á√ïES VETORIAIS ===
    demonstrate_vector_operations();
    
    // === AN√ÅLISE SEM√ÇNTICA ===
    let analyzer = SemanticAnalyzer::new();
    analyzer.demonstrate_semantic_relationships();
    
    // === CONCEITOS FUNDAMENTAIS ===
    println!("\n\nüìö === CONCEITOS FUNDAMENTAIS ===");
    
    println!("\nüéØ O QUE S√ÉO EMBEDDINGS?");
    println!("   ‚Ä¢ Representa√ß√µes vetoriais densas de tokens");
    println!("   ‚Ä¢ Capturam significado sem√¢ntico e sint√°tico");
    println!("   ‚Ä¢ Permitem opera√ß√µes matem√°ticas com palavras");
    println!("   ‚Ä¢ Dimens√£o t√≠pica: 128, 256, 512, 768, 1024");
    
    println!("\nüî§ EMBEDDINGS DE TOKENS:");
    println!("   ‚Ä¢ Cada token do vocabul√°rio ‚Üí vetor √∫nico");
    println!("   ‚Ä¢ Aprendidos durante o treinamento");
    println!("   ‚Ä¢ Palavras similares ‚Üí vetores similares");
    println!("   ‚Ä¢ Matriz de embedding: [vocab_size √ó embedding_dim]");
    
    println!("\nüìç EMBEDDINGS POSICIONAIS:");
    println!("   ‚Ä¢ Adicionam informa√ß√£o sobre posi√ß√£o na sequ√™ncia");
    println!("   ‚Ä¢ Essenciais para modelos sem recorr√™ncia");
    println!("   ‚Ä¢ Encoding sinusoidal: padr√£o no Transformer original");
    println!("   ‚Ä¢ Embeddings aprendidos: alternativa moderna");
    
    println!("\nüßÆ OPERA√á√ïES IMPORTANTES:");
    println!("   ‚Ä¢ Similaridade cosseno: mede dire√ß√£o dos vetores");
    println!("   ‚Ä¢ Dist√¢ncia euclidiana: mede magnitude da diferen√ßa");
    println!("   ‚Ä¢ Soma: combina informa√ß√µes (token + posi√ß√£o)");
    println!("   ‚Ä¢ Normaliza√ß√£o: estabiliza magnitudes");
    
    println!("\nüé® PROPRIEDADES INTERESSANTES:");
    println!("   ‚Ä¢ Analogias: rei - homem + mulher ‚âà rainha");
    println!("   ‚Ä¢ Clustering: palavras similares se agrupam");
    println!("   ‚Ä¢ Interpola√ß√£o: pontos intermedi√°rios t√™m significado");
    println!("   ‚Ä¢ Transfer√™ncia: embeddings pr√©-treinados s√£o √∫teis");
    
    println!("\n‚ö° OTIMIZA√á√ïES:");
    println!("   ‚Ä¢ Embedding sharing: compartilhar entre entrada/sa√≠da");
    println!("   ‚Ä¢ Quantiza√ß√£o: reduzir precis√£o para economizar mem√≥ria");
    println!("   ‚Ä¢ Pruning: remover dimens√µes menos importantes");
    println!("   ‚Ä¢ Factorization: decompor matriz grande em menores");
    
    println!("\n\nüéì === EXERC√çCIOS SUGERIDOS ===");
    println!("1. Implemente embeddings aprendidos vs. sinusoidais");
    println!("2. Experimente com diferentes dimens√µes");
    println!("3. Visualize embeddings em 2D usando t-SNE ou PCA");
    println!("4. Implemente analogias vetoriais");
    println!("5. Compare embeddings de diferentes modelos");
    println!("6. Implemente embedding de subpalavras");
    
    println!("\n‚ú® Embeddings explicados com sucesso! ‚ú®");
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_embedding_operations() {
        let emb1 = Embedding::from_values(vec![1.0, 0.0, 0.0]);
        let emb2 = Embedding::from_values(vec![0.0, 1.0, 0.0]);
        
        // Vetores ortogonais devem ter similaridade cosseno 0
        assert!((emb1.cosine_similarity(&emb2) - 0.0).abs() < 1e-6);
        
        // Dist√¢ncia euclidiana entre vetores unit√°rios ortogonais
        assert!((emb1.euclidean_distance(&emb2) - 2.0_f32.sqrt()).abs() < 1e-6);
    }

    #[test]
    fn test_token_embedding() {
        let token_emb = TokenEmbedding::new(100, 64);
        
        assert_eq!(token_emb.vocab_size, 100);
        assert_eq!(token_emb.embedding_dim, 64);
        
        let embedding = token_emb.get_embedding(0).unwrap();
        assert_eq!(embedding.dimension, 64);
    }

    #[test]
    fn test_positional_embedding() {
        let pos_emb = PositionalEmbedding::new(50, 64);
        
        let embeddings = pos_emb.forward(5);
        assert_eq!(embeddings.len(), 5);
        
        for emb in embeddings {
            assert_eq!(emb.dimension, 64);
        }
    }

    #[test]
    fn test_embedding_layer() {
        let layer = EmbeddingLayer::new(100, 64, 50);
        let token_ids = vec![1, 5, 10, 15];
        
        let embeddings = layer.forward(&token_ids);
        assert_eq!(embeddings.len(), 4);
        
        for emb in embeddings {
            assert_eq!(emb.dimension, 64);
        }
    }
}