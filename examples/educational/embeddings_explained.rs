//! # Exemplo Did√°tico: Embeddings e Representa√ß√µes Vetoriais
//!
//! Este exemplo demonstra como funcionam os embeddings em modelos de linguagem:
//! - Cria√ß√£o de embeddings de tokens
//! - Embeddings posicionais
//! - Opera√ß√µes com vetores de embedding
//! - Similaridade sem√¢ntica
//! - Visualiza√ß√£o de rela√ß√µes
//!
//! ## Como executar:
//! ```bash
//! cargo run --example embeddings_explained
//! ```

use std::collections::HashMap;
use std::f32::consts::PI;

/// Representa um vetor de embedding
#[derive(Debug, Clone)]
struct Embedding {
    vector: Vec<f32>,
    dimension: usize,
}

impl Embedding {
    /// Cria um novo embedding com valores aleat√≥rios
    fn new(dimension: usize) -> Self {
        let vector: Vec<f32> = (0..dimension)
            .map(|i| ((i as f32 * 0.1).sin() * 0.5))
            .collect();
        
        Self { vector, dimension }
    }
    
    /// Cria um embedding a partir de valores espec√≠ficos
    fn from_values(values: Vec<f32>) -> Self {
        let dimension = values.len();
        Self {
            vector: values,
            dimension,
        }
    }
    
    /// Calcula a similaridade cosseno entre dois embeddings
    fn cosine_similarity(&self, other: &Embedding) -> f32 {
        assert_eq!(self.dimension, other.dimension);
        
        let dot_product: f32 = self.vector.iter()
            .zip(other.vector.iter())
            .map(|(a, b)| a * b)
            .sum();
        
        let norm_a: f32 = self.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
        let norm_b: f32 = other.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
        
        if norm_a == 0.0 || norm_b == 0.0 {
            0.0
        } else {
            dot_product / (norm_a * norm_b)
        }
    }
    
    /// Adiciona dois embeddings (usado em conex√µes residuais)
    fn add(&self, other: &Embedding) -> Embedding {
        assert_eq!(self.dimension, other.dimension);
        
        let vector: Vec<f32> = self.vector.iter()
            .zip(other.vector.iter())
            .map(|(a, b)| a + b)
            .collect();
        
        Embedding::from_values(vector)
    }
    
    /// Multiplica o embedding por um escalar
    fn scale(&self, factor: f32) -> Embedding {
        let vector: Vec<f32> = self.vector.iter()
            .map(|x| x * factor)
            .collect();
        
        Embedding::from_values(vector)
    }
    
    /// Normaliza o embedding para ter norma unit√°ria
    fn normalize(&self) -> Embedding {
        let norm: f32 = self.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
        
        if norm == 0.0 {
            return self.clone();
        }
        
        let vector: Vec<f32> = self.vector.iter()
            .map(|x| x / norm)
            .collect();
        
        Embedding::from_values(vector)
    }
    
    /// Calcula a dist√¢ncia euclidiana
    fn euclidean_distance(&self, other: &Embedding) -> f32 {
        assert_eq!(self.dimension, other.dimension);
        
        self.vector.iter()
            .zip(other.vector.iter())
            .map(|(a, b)| (a - b).powi(2))
            .sum::<f32>()
            .sqrt()
    }
}

/// ## 1. Camada de Embedding de Tokens
/// 
/// Converte IDs de tokens em vetores densos de dimens√£o fixa
#[derive(Debug)]
struct TokenEmbedding {
    embeddings: HashMap<usize, Embedding>,
    vocab_size: usize,
    embedding_dim: usize,
}

impl TokenEmbedding {
    fn new(vocab_size: usize, embedding_dim: usize) -> Self {
        let mut embeddings = HashMap::new();
        
        // Inicializar embeddings para cada token no vocabul√°rio
        for token_id in 0..vocab_size {
            // Usar uma semente baseada no ID para embeddings consistentes
            let embedding = Self::create_deterministic_embedding(token_id, embedding_dim);
            embeddings.insert(token_id, embedding);
        }
        
        Self {
            embeddings,
            vocab_size,
            embedding_dim,
        }
    }
    
    /// Cria um embedding determin√≠stico baseado no ID do token
    fn create_deterministic_embedding(token_id: usize, dim: usize) -> Embedding {
        let mut vector = Vec::with_capacity(dim);
        
        for i in 0..dim {
            // Usar fun√ß√µes trigonom√©tricas para criar padr√µes √∫nicos
            let value = ((token_id as f32 * 0.1 + i as f32 * 0.05).sin() * 0.5) +
                       ((token_id as f32 * 0.07 + i as f32 * 0.03).cos() * 0.3);
            vector.push(value);
        }
        
        Embedding::from_values(vector)
    }
    
    /// Obt√©m o embedding para um token espec√≠fico
    fn get_embedding(&self, token_id: usize) -> Option<&Embedding> {
        self.embeddings.get(&token_id)
    }
    
    /// Processa uma sequ√™ncia de tokens
    fn forward(&self, token_ids: &[usize]) -> Vec<Embedding> {
        println!("\nüî§ Convertendo tokens em embeddings...");
        println!("Sequ√™ncia de entrada: {:?}", token_ids);
        
        let mut embeddings = Vec::new();
        
        for &token_id in token_ids {
            if let Some(embedding) = self.get_embedding(token_id) {
                embeddings.push(embedding.clone());
                println!("   Token {} ‚Üí Embedding[{}] (primeiros 4 valores: {:?})", 
                    token_id, 
                    self.embedding_dim,
                    &embedding.vector[0..4.min(embedding.vector.len())]
                );
            } else {
                println!("   ‚ö†Ô∏è  Token {} n√£o encontrado no vocabul√°rio!", token_id);
            }
        }
        
        println!("‚úÖ {} embeddings gerados", embeddings.len());
        embeddings
    }
}

/// ## 2. Embeddings Posicionais
/// 
/// Adicionam informa√ß√£o sobre a posi√ß√£o dos tokens na sequ√™ncia
#[derive(Debug)]
struct PositionalEmbedding {
    max_length: usize,
    embedding_dim: usize,
    embeddings: Vec<Embedding>,
}

impl PositionalEmbedding {
    fn new(max_length: usize, embedding_dim: usize) -> Self {
        let mut embeddings = Vec::with_capacity(max_length);
        
        // Gerar embeddings posicionais usando encoding sinusoidal
        for pos in 0..max_length {
            embeddings.push(Self::create_sinusoidal_embedding(pos, embedding_dim));
        }
        
        Self {
            max_length,
            embedding_dim,
            embeddings,
        }
    }
    
    /// Cria embedding posicional usando fun√ß√µes sinusoidais
    /// 
    /// F√≥rmula do paper "Attention Is All You Need":
    /// PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
    /// PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    fn create_sinusoidal_embedding(position: usize, dim: usize) -> Embedding {
        let mut vector = Vec::with_capacity(dim);
        
        for i in 0..dim {
            let angle = position as f32 / 10000_f32.powf(2.0 * (i / 2) as f32 / dim as f32);
            
            let value = if i % 2 == 0 {
                angle.sin()  // Posi√ß√µes pares: seno
            } else {
                angle.cos()  // Posi√ß√µes √≠mpares: cosseno
            };
            
            vector.push(value);
        }
        
        Embedding::from_values(vector)
    }
    
    /// Obt√©m embeddings posicionais para uma sequ√™ncia
    fn forward(&self, sequence_length: usize) -> Vec<Embedding> {
        println!("\nüìç Gerando embeddings posicionais...");
        println!("Comprimento da sequ√™ncia: {}", sequence_length);
        
        let mut pos_embeddings = Vec::new();
        
        for pos in 0..sequence_length {
            if pos < self.max_length {
                pos_embeddings.push(self.embeddings[pos].clone());
                println!("   Posi√ß√£o {} ‚Üí Embedding posicional (primeiros 4: {:?})", 
                    pos, 
                    &self.embeddings[pos].vector[0..4.min(self.embedding_dim)]
                );
            } else {
                println!("   ‚ö†Ô∏è  Posi√ß√£o {} excede comprimento m√°ximo {}", pos, self.max_length);
                break;
            }
        }
        
        println!("‚úÖ {} embeddings posicionais gerados", pos_embeddings.len());
        pos_embeddings
    }
}

/// ## 3. Combina√ß√£o de Embeddings
/// 
/// Combina embeddings de tokens e posicionais
#[derive(Debug)]
struct EmbeddingLayer {
    token_embedding: TokenEmbedding,
    positional_embedding: PositionalEmbedding,
}

impl EmbeddingLayer {
    fn new(vocab_size: usize, embedding_dim: usize, max_length: usize) -> Self {
        Self {
            token_embedding: TokenEmbedding::new(vocab_size, embedding_dim),
            positional_embedding: PositionalEmbedding::new(max_length, embedding_dim),
        }
    }
    
    /// Processa uma sequ√™ncia completa
    fn forward(&self, token_ids: &[usize]) -> Vec<Embedding> {
        println!("\nüèóÔ∏è  === CAMADA DE EMBEDDING COMPLETA ===");
        
        // 1. Obter embeddings de tokens
        let token_embeddings = self.token_embedding.forward(token_ids);
        
        // 2. Obter embeddings posicionais
        let pos_embeddings = self.positional_embedding.forward(token_ids.len());
        
        // 3. Combinar embeddings (soma elemento a elemento)
        println!("\n‚ûï Combinando embeddings de tokens e posicionais...");
        let mut combined_embeddings = Vec::new();
        
        for (i, (token_emb, pos_emb)) in token_embeddings.iter().zip(pos_embeddings.iter()).enumerate() {
            let combined = token_emb.add(pos_emb);
            combined_embeddings.push(combined);
            
            println!("   Posi√ß√£o {}: Token + Posicional ‚Üí Embedding final", i);
        }
        
        println!("‚úÖ Embeddings finais prontos para o Transformer!");
        combined_embeddings
    }
}

/// ## 4. An√°lise de Similaridade Sem√¢ntica
struct SemanticAnalyzer {
    embedding_layer: EmbeddingLayer,
    vocabulary: HashMap<String, usize>,
}

impl SemanticAnalyzer {
    fn new() -> Self {
        // Criar vocabul√°rio de exemplo
        let mut vocabulary = HashMap::new();
        let words = vec![
            "gato", "cachorro", "animal", "pet",
            "carro", "bicicleta", "transporte", "ve√≠culo",
            "casa", "apartamento", "moradia", "lar",
            "livro", "revista", "jornal", "leitura",
            "feliz", "alegre", "contente", "satisfeito",
        ];
        
        for (i, word) in words.iter().enumerate() {
            vocabulary.insert(word.to_string(), i);
        }
        
        let embedding_layer = EmbeddingLayer::new(words.len(), 64, 100);
        
        Self {
            embedding_layer,
            vocabulary,
        }
    }
    
    fn get_word_embedding(&self, word: &str) -> Option<Embedding> {
        if let Some(&token_id) = self.vocabulary.get(word) {
            self.embedding_layer.token_embedding.get_embedding(token_id).cloned()
        } else {
            None
        }
    }
    
    fn find_similar_words(&self, target_word: &str, top_k: usize) -> Vec<(String, f32)> {
        if let Some(target_embedding) = self.get_word_embedding(target_word) {
            let mut similarities = Vec::new();
            
            for (word, &token_id) in &self.vocabulary {
                if word != target_word {
                    if let Some(word_embedding) = self.embedding_layer.token_embedding.get_embedding(token_id) {
                        let similarity = target_embedding.cosine_similarity(word_embedding);
                        similarities.push((word.clone(), similarity));
                    }
                }
            }
            
            // Ordenar por similaridade (maior primeiro)
            similarities.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
            similarities.into_iter().take(top_k).collect()
        } else {
            Vec::new()
        }
    }
    
    fn demonstrate_semantic_relationships(&self) {
        println!("\n\nüîç === AN√ÅLISE DE SIMILARIDADE SEM√ÇNTICA ===");
        
        let test_words = vec!["gato", "carro", "casa", "feliz"];
        
        for word in test_words {
            println!("\nüéØ Palavras similares a '{}':", word);
            let similar = self.find_similar_words(word, 3);
            
            for (i, (similar_word, similarity)) in similar.iter().enumerate() {
                println!("   {}. {} (similaridade: {:.3})", 
                    i + 1, similar_word, similarity);
            }
        }
    }
}

/// ## 5. Demonstra√ß√£o de Opera√ß√µes Vetoriais
fn demonstrate_vector_operations() {
    println!("\n\nüßÆ === OPERA√á√ïES COM EMBEDDINGS ===");
    
    // Criar embeddings de exemplo
    let emb1 = Embedding::from_values(vec![1.0, 2.0, 3.0, 4.0]);
    let emb2 = Embedding::from_values(vec![2.0, 1.0, 4.0, 3.0]);
    let emb3 = Embedding::from_values(vec![1.0, 2.0, 3.0, 4.0]); // Id√™ntico ao emb1
    
    println!("\nüìä Embeddings de teste:");
    println!("   Embedding 1: {:?}", emb1.vector);
    println!("   Embedding 2: {:?}", emb2.vector);
    println!("   Embedding 3: {:?}", emb3.vector);
    
    // Similaridade cosseno
    println!("\nüìê Similaridades cosseno:");
    println!("   emb1 ‚Üî emb2: {:.3}", emb1.cosine_similarity(&emb2));
    println!("   emb1 ‚Üî emb3: {:.3}", emb1.cosine_similarity(&emb3));
    println!("   emb2 ‚Üî emb3: {:.3}", emb2.cosine_similarity(&emb3));
    
    // Dist√¢ncia euclidiana
    println!("\nüìè Dist√¢ncias euclidianas:");
    println!("   emb1 ‚Üî emb2: {:.3}", emb1.euclidean_distance(&emb2));
    println!("   emb1 ‚Üî emb3: {:.3}", emb1.euclidean_distance(&emb3));
    println!("   emb2 ‚Üî emb3: {:.3}", emb2.euclidean_distance(&emb3));
    
    // Opera√ß√µes aritm√©ticas
    println!("\n‚ûï Opera√ß√µes aritm√©ticas:");
    let sum = emb1.add(&emb2);
    println!("   emb1 + emb2: {:?}", sum.vector);
    
    let scaled = emb1.scale(2.0);
    println!("   emb1 √ó 2: {:?}", scaled.vector);
    
    let normalized = emb1.normalize();
    println!("   emb1 normalizado: {:?}", 
        normalized.vector.iter().map(|x| format!("{:.3}", x)).collect::<Vec<_>>());
}

/// Fun√ß√£o principal
fn main() {
    println!("üöÄ === GUIA COMPLETO DE EMBEDDINGS ===");
    println!("\nEmbeddings s√£o representa√ß√µes vetoriais densas que capturam");
    println!("significado sem√¢ntico e rela√ß√µes entre tokens.\n");
    
    // === DEMONSTRA√á√ÉO B√ÅSICA ===
    println!("\nüìö === DEMONSTRA√á√ÉO B√ÅSICA ===");
    
    let vocab_size = 1000;
    let embedding_dim = 128;
    let max_length = 50;
    
    println!("\n‚öôÔ∏è  Configura√ß√µes:");
    println!("   - Tamanho do vocabul√°rio: {}", vocab_size);
    println!("   - Dimens√£o dos embeddings: {}", embedding_dim);
    println!("   - Comprimento m√°ximo: {}", max_length);
    
    // Criar camada de embedding
    let embedding_layer = EmbeddingLayer::new(vocab_size, embedding_dim, max_length);
    
    // Sequ√™ncia de exemplo
    let token_sequence = vec![10, 25, 7, 42, 3];
    println!("\nüî¢ Sequ√™ncia de tokens: {:?}", token_sequence);
    
    // Processar atrav√©s da camada de embedding
    let embeddings = embedding_layer.forward(&token_sequence);
    
    println!("\nüìä Resultado:");
    println!("   - {} embeddings gerados", embeddings.len());
    println!("   - Cada embedding tem {} dimens√µes", embedding_dim);
    
    // === OPERA√á√ïES VETORIAIS ===
    demonstrate_vector_operations();
    
    // === AN√ÅLISE SEM√ÇNTICA ===
    let analyzer = SemanticAnalyzer::new();
    analyzer.demonstrate_semantic_relationships();
    
    // === EXERC√çCIOS PR√ÅTICOS ===
    println!("\n\nüéØ === EXERC√çCIOS PR√ÅTICOS ===");
    exercicio_1_analise_dimensionalidade();
    exercicio_2_clustering_semantico();
    exercicio_3_analogias_vetoriais();
    exercicio_4_visualizacao_embeddings();
    exercicio_5_reducao_dimensionalidade();
    exercicio_6_benchmark_operacoes();
    
    // === CONCEITOS FUNDAMENTAIS ===
    println!("\n\nüìö === CONCEITOS FUNDAMENTAIS ===");
    
    println!("\nüéØ O QUE S√ÉO EMBEDDINGS?");
    println!("   ‚Ä¢ Representa√ß√µes vetoriais densas de tokens");
    println!("   ‚Ä¢ Capturam significado sem√¢ntico e sint√°tico");
    println!("   ‚Ä¢ Permitem opera√ß√µes matem√°ticas com palavras");
    println!("   ‚Ä¢ Dimens√£o t√≠pica: 128, 256, 512, 768, 1024");
    
    println!("\nüî§ EMBEDDINGS DE TOKENS:");
    println!("   ‚Ä¢ Cada token do vocabul√°rio ‚Üí vetor √∫nico");
    println!("   ‚Ä¢ Aprendidos durante o treinamento");
    println!("   ‚Ä¢ Palavras similares ‚Üí vetores similares");
    println!("   ‚Ä¢ Matriz de embedding: [vocab_size √ó embedding_dim]");
    
    println!("\nüìç EMBEDDINGS POSICIONAIS:");
    println!("   ‚Ä¢ Adicionam informa√ß√£o sobre posi√ß√£o na sequ√™ncia");
    println!("   ‚Ä¢ Essenciais para modelos sem recorr√™ncia");
    println!("   ‚Ä¢ Encoding sinusoidal: padr√£o no Transformer original");
    println!("   ‚Ä¢ Embeddings aprendidos: alternativa moderna");
    
    println!("\nüßÆ OPERA√á√ïES IMPORTANTES:");
    println!("   ‚Ä¢ Similaridade cosseno: mede dire√ß√£o dos vetores");
    println!("   ‚Ä¢ Dist√¢ncia euclidiana: mede magnitude da diferen√ßa");
    println!("   ‚Ä¢ Soma: combina informa√ß√µes (token + posi√ß√£o)");
    println!("   ‚Ä¢ Normaliza√ß√£o: estabiliza magnitudes");
    
    println!("\nüé® PROPRIEDADES INTERESSANTES:");
    println!("   ‚Ä¢ Analogias: rei - homem + mulher ‚âà rainha");
    println!("   ‚Ä¢ Clustering: palavras similares se agrupam");
    println!("   ‚Ä¢ Interpola√ß√£o: pontos intermedi√°rios t√™m significado");
    println!("   ‚Ä¢ Transfer√™ncia: embeddings pr√©-treinados s√£o √∫teis");
    
    println!("\n‚ö° OTIMIZA√á√ïES:");
    println!("   ‚Ä¢ Embedding sharing: compartilhar entre entrada/sa√≠da");
    println!("   ‚Ä¢ Quantiza√ß√£o: reduzir precis√£o para economizar mem√≥ria");
    println!("   ‚Ä¢ Pruning: remover dimens√µes menos importantes");
    println!("   ‚Ä¢ Factorization: decompor matriz grande em menores");
    
    println!("\n\nüéì === EXERC√çCIOS SUGERIDOS ===");
    println!("1. Implemente embeddings aprendidos vs. sinusoidais");
    println!("2. Experimente com diferentes dimens√µes");
    println!("3. Visualize embeddings em 2D usando t-SNE ou PCA");
    println!("4. Implemente analogias vetoriais");
    println!("5. Compare embeddings de diferentes modelos");
    println!("6. Implemente embedding de subpalavras");
    
    println!("\n‚ú® Embeddings explicados com sucesso! ‚ú®");
}

/// EXERC√çCIO 1: An√°lise de Dimensionalidade
fn exercicio_1_analise_dimensionalidade() {
    println!("\n--- Exerc√≠cio 1: An√°lise de Dimensionalidade ---");
    
    let dimensoes = vec![32, 64, 128, 256, 512];
    let vocab_size = 1000;
    
    println!("\nImpacto da dimensionalidade nos embeddings:");
    
    for dim in dimensoes {
        let token_emb = TokenEmbedding::new(vocab_size, dim);
        
        // Calcula estat√≠sticas dos embeddings
        let test_tokens = vec![1, 2, 3, 4, 5];
        let embeddings = token_emb.forward(&test_tokens);
        
        // Calcula similaridade m√©dia entre tokens aleat√≥rios
        let mut total_similarity = 0.0;
        let mut count = 0;
        
        for i in 0..embeddings.len() {
            for j in i+1..embeddings.len() {
                total_similarity += embeddings[i].cosine_similarity(&embeddings[j]);
                count += 1;
            }
        }
        
        let avg_similarity = total_similarity / count as f32;
        let memory_mb = (vocab_size * dim * 4) as f32 / (1024.0 * 1024.0);
        
        println!("  Dim {}: similaridade m√©dia={:.3}, mem√≥ria={:.1}MB", 
                dim, avg_similarity, memory_mb);
    }
    
    println!("\nüí° Dica: Dimens√µes maiores capturam mais nuances, mas custam mais mem√≥ria!");
}

/// EXERC√çCIO 2: Clustering Sem√¢ntico
fn exercicio_2_clustering_semantico() {
    println!("\n--- Exerc√≠cio 2: Clustering Sem√¢ntico ---");
    
    let analyzer = SemanticAnalyzer::new();
    
    // Grupos sem√¢nticos para testar
    let grupos = vec![
        ("Animais", vec!["gato", "cachorro"]),
        ("Transporte", vec!["carro", "bicicleta"]),
        ("Moradia", vec!["casa", "apartamento"]),
    ];
    
    println!("\nAn√°lise de coes√£o intra-grupo:");
    
    for (nome_grupo, palavras) in &grupos {
        let mut similaridades = Vec::new();
        
        // Calcula similaridades dentro do grupo
        for i in 0..palavras.len() {
            for j in i+1..palavras.len() {
                if let (Some(emb1), Some(emb2)) = (
                    analyzer.get_word_embedding(palavras[i]),
                    analyzer.get_word_embedding(palavras[j])
                ) {
                    let sim = emb1.cosine_similarity(&emb2);
                    similaridades.push(sim);
                }
            }
        }
        
        if !similaridades.is_empty() {
            let media = similaridades.iter().sum::<f32>() / similaridades.len() as f32;
            let min = similaridades.iter().fold(f32::INFINITY, |a, &b| a.min(b));
            let max = similaridades.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b));
            
            println!("  {}: m√©dia={:.3}, min={:.3}, max={:.3}", 
                    nome_grupo, media, min, max);
        }
    }
    
    println!("\nüí° Dica: Grupos sem√¢nticos devem ter alta similaridade interna!");
}

/// EXERC√çCIO 3: Analogias Vetoriais
fn exercicio_3_analogias_vetoriais() {
    println!("\n--- Exerc√≠cio 3: Analogias Vetoriais ---");
    
    let analyzer = SemanticAnalyzer::new();
    
    // Testa analogias simples com palavras dispon√≠veis
    let analogias = vec![
        ("gato", "animal", "carro", "ve√≠culo"),
        ("casa", "moradia", "livro", "leitura"),
        ("feliz", "alegre", "contente", "satisfeito"),
    ];
    
    println!("\nTestando analogias vetoriais (A - B + C ‚âà D):");
    
    for (a, b, c, d_esperado) in analogias {
        if let (Some(emb_a), Some(emb_b), Some(emb_c), Some(emb_d)) = (
            analyzer.get_word_embedding(a),
            analyzer.get_word_embedding(b),
            analyzer.get_word_embedding(c),
            analyzer.get_word_embedding(d_esperado)
        ) {
            // Calcula: A - B + C
            let resultado = emb_a.add(&emb_c).add(&emb_b.scale(-1.0));
            let similaridade = resultado.cosine_similarity(&emb_d);
            
            println!("  {} - {} + {} ‚âà {} (similaridade: {:.3})", 
                    a, b, c, d_esperado, similaridade);
        }
    }
    
    println!("\nüí° Dica: Analogias funcionam porque embeddings capturam rela√ß√µes sem√¢nticas!");
}

/// EXERC√çCIO 4: Visualiza√ß√£o de Embeddings
fn exercicio_4_visualizacao_embeddings() {
    println!("\n--- Exerc√≠cio 4: Visualiza√ß√£o de Embeddings ---");
    
    let analyzer = SemanticAnalyzer::new();
    let palavras = vec!["gato", "cachorro", "carro", "casa", "feliz", "livro"];
    
    println!("\nProje√ß√£o 2D simplificada (primeiras 2 dimens√µes):");
    
    for palavra in &palavras {
        if let Some(embedding) = analyzer.get_word_embedding(palavra) {
            let x = embedding.vector[0];
            let y = embedding.vector[1];
            
            // Normaliza para visualiza√ß√£o
            let x_norm = (x * 10.0) as i32;
            let y_norm = (y * 10.0) as i32;
            
            println!("  {}: ({:.3}, {:.3}) -> ({}, {})", 
                    palavra, x, y, x_norm, y_norm);
        }
    }
    
    // Simula mapa de calor de dist√¢ncias
    println!("\nMapa de dist√¢ncias euclidianas:");
    print!("      ");
    for palavra in &palavras {
        print!("{:>6}", &palavra[0..3]);
    }
    println!();
    
    for palavra1 in &palavras {
        print!("{:>6}", &palavra1[0..3]);
        for palavra2 in &palavras {
            if let (Some(emb1), Some(emb2)) = (
                analyzer.get_word_embedding(palavra1),
                analyzer.get_word_embedding(palavra2)
            ) {
                let dist = emb1.euclidean_distance(&emb2);
                print!("{:6.2}", dist);
            } else {
                print!("  N/A ");
            }
        }
        println!();
    }
    
    println!("\nüí° Dica: Visualiza√ß√£o ajuda a entender rela√ß√µes espaciais entre conceitos!");
}

/// EXERC√çCIO 5: Redu√ß√£o de Dimensionalidade
fn exercicio_5_reducao_dimensionalidade() {
    println!("\n--- Exerc√≠cio 5: Redu√ß√£o de Dimensionalidade ---");
    
    let dim_original = 128;
    let dims_reduzidas = vec![64, 32, 16, 8];
    
    println!("\nImpacto da redu√ß√£o de dimensionalidade:");
    
    let token_emb = TokenEmbedding::new(100, dim_original);
    let test_tokens = vec![1, 2, 3, 4, 5];
    let embeddings_originais = token_emb.forward(&test_tokens);
    
    for dim_reduzida in dims_reduzidas {
        // Simula redu√ß√£o truncando dimens√µes
        let embeddings_reduzidos: Vec<Embedding> = embeddings_originais.iter()
            .map(|emb| {
                let mut vetor_reduzido = emb.vector[0..dim_reduzida].to_vec();
                // Renormaliza ap√≥s truncamento
                let norma = vetor_reduzido.iter().map(|x| x * x).sum::<f32>().sqrt();
                if norma > 0.0 {
                    vetor_reduzido.iter_mut().for_each(|x| *x /= norma);
                }
                Embedding::from_values(vetor_reduzido)
            })
            .collect();
        
        // Calcula preserva√ß√£o de similaridades
        let mut preservacao_total = 0.0;
        let mut count = 0;
        
        for i in 0..embeddings_originais.len() {
            for j in i+1..embeddings_originais.len() {
                let sim_original = embeddings_originais[i].cosine_similarity(&embeddings_originais[j]);
                let sim_reduzida = embeddings_reduzidos[i].cosine_similarity(&embeddings_reduzidos[j]);
                
                preservacao_total += (sim_original - sim_reduzida).abs();
                count += 1;
            }
        }
        
        let erro_medio = preservacao_total / count as f32;
        let reducao_memoria = (1.0 - dim_reduzida as f32 / dim_original as f32) * 100.0;
        
        println!("  {}D: erro m√©dio={:.4}, redu√ß√£o mem√≥ria={:.1}%", 
                dim_reduzida, erro_medio, reducao_memoria);
    }
    
    println!("\nüí° Dica: Redu√ß√£o de dimensionalidade economiza mem√≥ria mas pode perder informa√ß√£o!");
}

/// EXERC√çCIO 6: Benchmark de Opera√ß√µes
fn exercicio_6_benchmark_operacoes() {
    println!("\n--- Exerc√≠cio 6: Benchmark de Opera√ß√µes ---");
    
    let dimensoes = vec![64, 128, 256, 512];
    let num_operacoes = 10000;
    
    println!("\nPerformance de opera√ß√µes com embeddings:");
    
    for dim in dimensoes {
        let emb1 = Embedding::new(dim);
        let emb2 = Embedding::new(dim);
        
        // Benchmark similaridade cosseno
        let start = std::time::Instant::now();
        for _ in 0..num_operacoes {
            let _ = emb1.cosine_similarity(&emb2);
        }
        let tempo_cosine = start.elapsed();
        
        // Benchmark dist√¢ncia euclidiana
        let start = std::time::Instant::now();
        for _ in 0..num_operacoes {
            let _ = emb1.euclidean_distance(&emb2);
        }
        let tempo_euclidean = start.elapsed();
        
        // Benchmark adi√ß√£o
        let start = std::time::Instant::now();
        for _ in 0..num_operacoes {
            let _ = emb1.add(&emb2);
        }
        let tempo_add = start.elapsed();
        
        let ops_cosine = num_operacoes as f64 / tempo_cosine.as_secs_f64();
        let ops_euclidean = num_operacoes as f64 / tempo_euclidean.as_secs_f64();
        let ops_add = num_operacoes as f64 / tempo_add.as_secs_f64();
        
        println!("\n  Dimens√£o {}:", dim);
        println!("    Cosseno: {:.0} ops/s", ops_cosine);
        println!("    Euclidiana: {:.0} ops/s", ops_euclidean);
        println!("    Adi√ß√£o: {:.0} ops/s", ops_add);
    }
    
    println!("\nüí° Dica: Performance decresce com dimensionalidade, mas opera√ß√µes vetoriais s√£o eficientes!");
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_embedding_operations() {
        let emb1 = Embedding::from_values(vec![1.0, 0.0, 0.0]);
        let emb2 = Embedding::from_values(vec![0.0, 1.0, 0.0]);
        
        // Vetores ortogonais devem ter similaridade cosseno 0
        assert!((emb1.cosine_similarity(&emb2) - 0.0).abs() < 1e-6);
        
        // Dist√¢ncia euclidiana entre vetores unit√°rios ortogonais
        assert!((emb1.euclidean_distance(&emb2) - 2.0_f32.sqrt()).abs() < 1e-6);
    }

    #[test]
    fn test_token_embedding() {
        let token_emb = TokenEmbedding::new(100, 64);
        
        assert_eq!(token_emb.vocab_size, 100);
        assert_eq!(token_emb.embedding_dim, 64);
        
        let embedding = token_emb.get_embedding(0).unwrap();
        assert_eq!(embedding.dimension, 64);
    }

    #[test]
    fn test_positional_embedding() {
        let pos_emb = PositionalEmbedding::new(50, 64);
        
        let embeddings = pos_emb.forward(5);
        assert_eq!(embeddings.len(), 5);
        
        for emb in embeddings {
            assert_eq!(emb.dimension, 64);
        }
    }

    #[test]
    fn test_embedding_layer() {
        let layer = EmbeddingLayer::new(100, 64, 50);
        let token_ids = vec![1, 5, 10, 15];
        
        let embeddings = layer.forward(&token_ids);
        assert_eq!(embeddings.len(), 4);
        
        for emb in embeddings {
            assert_eq!(emb.dimension, 64);
        }
    }
}