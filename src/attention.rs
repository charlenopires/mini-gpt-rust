//! # üß† SELF-ATTENTION: O CORA√á√ÉO PULSANTE DO TRANSFORMER
//! 
//! Este m√≥dulo implementa o mecanismo de aten√ß√£o que revolucionou o NLP.
//! √â aqui que a "m√°gica" dos Transformers realmente acontece!
//! 
//! ## üéØ ANALOGIA INTUITIVA: A SALA DE AULA INTELIGENTE
//! 
//! Imagine uma sala de aula onde cada aluno (token) pode "prestar aten√ß√£o"
//! em todos os outros alunos simultaneamente para entender melhor o contexto:
//! 
//! ```text
//! Frase: "O gato subiu no telhado"
//! 
//! Token "gato":
//!   - Presta muita aten√ß√£o em "subiu" (verbo relacionado)
//!   - Presta pouca aten√ß√£o em "no" (preposi√ß√£o menos relevante)
//!   - Presta aten√ß√£o m√©dia em "telhado" (objeto da a√ß√£o)
//! 
//! Token "subiu":
//!   - Presta muita aten√ß√£o em "gato" (sujeito da a√ß√£o)
//!   - Presta muita aten√ß√£o em "telhado" (destino da a√ß√£o)
//!   - Presta pouca aten√ß√£o em "O" (artigo menos relevante)
//! ```
//! 
//! ## üî¨ FUNDAMENTOS MATEM√ÅTICOS
//! 
//! O mecanismo de aten√ß√£o implementa a f√≥rmula:
//! 
//! ```text
//! Attention(Q,K,V) = softmax(QK^T / ‚àöd_k)V
//! 
//! Onde:
//! - Q (Query): "O que eu quero saber?"
//! - K (Key): "O que eu tenho para oferecer?"
//! - V (Value): "Qual √© minha informa√ß√£o real?"
//! - d_k: Dimens√£o das chaves (para normaliza√ß√£o)
//! ```
//! 
//! ## üé≠ MULTI-HEAD ATTENTION: M√öLTIPLAS PERSPECTIVAS
//! 
//! Como ter v√°rios "tipos de aten√ß√£o" simultaneamente:
//! - **Cabe√ßa 1**: Foca em rela√ß√µes sint√°ticas (sujeito-verbo)
//! - **Cabe√ßa 2**: Foca em rela√ß√µes sem√¢nticas (palavras relacionadas)
//! - **Cabe√ßa 3**: Foca em depend√™ncias de longo alcance
//! - **Cabe√ßa N**: Cada uma aprende padr√µes diferentes!

use candle_core::{Result, Tensor};
use candle_nn::{linear, Linear, Module, VarBuilder};

/// üéØ **SELF-ATTENTION: IMPLEMENTA√á√ÉO DO MECANISMO REVOLUCION√ÅRIO**
/// 
/// Esta estrutura implementa o Scaled Dot-Product Attention, o cora√ß√£o
/// dos modelos Transformer que permitiu avan√ßos como GPT, BERT, e ChatGPT.
/// 
/// ## üßÆ COMPONENTES PRINCIPAIS:
/// 
/// ### üîç **Query (Q)**: "O que eu quero saber?"
/// - Representa a "pergunta" que cada token faz
/// - Determina que tipo de informa√ß√£o o token est√° buscando
/// - Exemplo: token "ele" busca informa√ß√£o sobre a quem se refere
/// 
/// ### üóùÔ∏è **Key (K)**: "O que eu tenho para oferecer?"
/// - Representa o "√≠ndice" ou "etiqueta" de cada token
/// - Determina que tipo de informa√ß√£o cada token pode fornecer
/// - Exemplo: token "Jo√£o" oferece informa√ß√£o sobre uma pessoa
/// 
/// ### üíé **Value (V)**: "Qual √© minha informa√ß√£o real?"
/// - Cont√©m a informa√ß√£o sem√¢ntica real do token
/// - √â o que ser√° "misturado" baseado nos scores de aten√ß√£o
/// - Exemplo: representa√ß√£o rica do significado de "Jo√£o"
/// 
/// ## ‚ö° PROCESSO DE ATEN√á√ÉO:
/// 
/// 1. **Proje√ß√£o**: X ‚Üí Q, K, V (atrav√©s de matrizes aprendidas)
/// 2. **Scores**: Q √ó K^T (mede compatibilidade entre tokens)
/// 3. **Escala**: Divide por ‚àöd_k (evita gradientes muito pequenos)
/// 4. **M√°scara**: Aplica m√°scara causal (impede vis√£o do futuro)
/// 5. **Softmax**: Converte scores em probabilidades
/// 6. **Agrega√ß√£o**: Combina Values baseado nas probabilidades
/// 7. **Proje√ß√£o final**: Transforma resultado para dimens√£o original
pub struct SelfAttention {
    // üéØ **MATRIZES DE PROJE√á√ÉO APRENDIDAS**
    // Estas s√£o as "lentes" que transformam embeddings em Q, K, V
    w_query: Linear,    // üîç Projeta X ‚Üí Query ("o que eu quero saber?")
    w_key: Linear,      // üóùÔ∏è Projeta X ‚Üí Key ("o que eu ofere√ßo?")
    w_value: Linear,    // üíé Projeta X ‚Üí Value ("minha informa√ß√£o real")
    w_out: Linear,      // üé™ Projeta resultado final de volta ao espa√ßo original
    
    // üìê **HIPERPAR√ÇMETROS DE ARQUITETURA**
    n_embd: usize,      // üßÆ Dimens√£o dos embeddings (largura do modelo)
    n_head: usize,      // üëÅÔ∏è N√∫mero de cabe√ßas de aten√ß√£o paralelas
    head_dim: usize,    // üìè Dimens√£o de cada cabe√ßa (n_embd / n_head)
    dropout: f32,       // üé≤ Taxa de dropout para regulariza√ß√£o
}

impl SelfAttention {
    /// üèóÔ∏è **CONSTRUTOR: INICIALIZANDO O MECANISMO DE ATEN√á√ÉO**
    /// 
    /// Este m√©todo cria uma nova inst√¢ncia de Self-Attention, configurando
    /// todas as matrizes de proje√ß√£o e hiperpar√¢metros necess√°rios.
    /// 
    /// ## üéØ **Par√¢metros de Entrada:**
    /// 
    /// ### üìê **n_embd**: Dimens√£o dos Embeddings
    /// - Largura do modelo (ex: 768 para GPT-2 small, 1024 para medium)
    /// - Determina a "capacidade" de representa√ß√£o do modelo
    /// - Deve ser divis√≠vel por n_head para distribui√ß√£o uniforme
    /// 
    /// ### üëÅÔ∏è **n_head**: N√∫mero de Cabe√ßas de Aten√ß√£o
    /// - Permite m√∫ltiplas "perspectivas" de aten√ß√£o simult√¢neas
    /// - Cada cabe√ßa aprende padr√µes diferentes (sint√°tico, sem√¢ntico, etc.)
    /// - Valores t√≠picos: 8, 12, 16 (pot√™ncias de 2 para efici√™ncia)
    /// 
    /// ### üé≤ **dropout**: Taxa de Regulariza√ß√£o
    /// - Previne overfitting zerando aleatoriamente algumas conex√µes
    /// - Valores t√≠picos: 0.1 (10%) para modelos pequenos, 0.0 para infer√™ncia
    /// 
    /// ### üèóÔ∏è **vb**: VarBuilder para Inicializa√ß√£o de Pesos
    /// - Gerencia a cria√ß√£o e inicializa√ß√£o das matrizes de peso
    /// - Garante inicializa√ß√£o adequada (Xavier/Kaiming) para converg√™ncia
    /// 
    /// ## üßÆ **C√°lculos de Dimens√£o:**
    /// ```text
    /// head_dim = n_embd / n_head
    /// 
    /// Exemplo com n_embd=768, n_head=12:
    /// head_dim = 768 / 12 = 64
    /// 
    /// Cada cabe√ßa processa vetores de 64 dimens√µes
    /// 12 cabe√ßas √ó 64 dim = 768 dim total (preserva dimensionalidade)
    /// ```
    pub fn new(n_embd: usize, n_head: usize, dropout: f32, vb: VarBuilder) -> Result<Self> {
        // üìè **C√ÅLCULO DA DIMENS√ÉO POR CABE√áA**
        // Divide o espa√ßo de embeddings igualmente entre as cabe√ßas
        // Garante que n_embd seja divis√≠vel por n_head
        assert_eq!(n_embd % n_head, 0, 
                   "n_embd ({}) deve ser divis√≠vel por n_head ({})", n_embd, n_head);
        let head_dim = n_embd / n_head;
        
        // üéØ **CRIA√á√ÉO DAS MATRIZES DE PROJE√á√ÉO**
        // Cada matriz √© uma transforma√ß√£o linear aprendida:
        // 
        // üîç Query: "Que tipo de informa√ß√£o eu estou procurando?"
        // Transforma embedding em vetor de "busca"
        let w_query = linear(n_embd, n_embd, vb.pp("w_query"))?;
        
        // üóùÔ∏è Key: "Que tipo de informa√ß√£o eu posso oferecer?"
        // Transforma embedding em vetor de "√≠ndice/etiqueta"
        let w_key = linear(n_embd, n_embd, vb.pp("w_key"))?;
        
        // üíé Value: "Qual √© minha informa√ß√£o sem√¢ntica real?"
        // Transforma embedding no conte√∫do que ser√° misturado
        let w_value = linear(n_embd, n_embd, vb.pp("w_value"))?;
        
        // üé™ Output: "Como combino informa√ß√µes de todas as cabe√ßas?"
        // Projeta resultado concatenado de volta ao espa√ßo original
        let w_out = linear(n_embd, n_embd, vb.pp("w_out"))?;
        
        // üèóÔ∏è **CONSTRU√á√ÉO DA ESTRUTURA FINAL**
        Ok(Self {
            w_query,
            w_key,
            w_value,
            w_out,
            n_embd,
            n_head,
            head_dim,
            dropout,
        })
    }
    
    /// üöÄ **FORWARD PASS: O CORA√á√ÉO DO MECANISMO DE ATEN√á√ÉO**
    /// 
    /// Este m√©todo implementa o algoritmo completo de Self-Attention,
    /// permitindo que cada token "converse" com todos os outros tokens
    /// na sequ√™ncia para entender o contexto.
    /// 
    /// ## üé≠ Analogia da Sala de Aula:
    /// Imagine uma sala de aula onde cada aluno (token) pode fazer
    /// perguntas para todos os outros alunos simultaneamente:
    /// - **Query**: "Que informa√ß√£o eu preciso?"
    /// - **Key**: "Que informa√ß√£o eu tenho?"
    /// - **Value**: "Aqui est√° minha informa√ß√£o!"
    /// 
    /// ## üìä Dimens√µes dos Tensores:
    /// ```text
    /// Input:  [B, T, C] = [batch, sequence_length, embedding_dim]
    /// Q,K,V:  [B, H, T, D] = [batch, heads, seq_len, head_dim]
    /// Scores: [B, H, T, T] = [batch, heads, seq_len, seq_len]
    /// Output: [B, T, C] = [batch, sequence_length, embedding_dim]
    /// ```
    /// 
    /// ## ‚ö° Complexidade Computacional:
    /// - **Tempo**: O(T¬≤ √ó C) - quadr√°tica no comprimento da sequ√™ncia
    /// - **Mem√≥ria**: O(T¬≤) - para armazenar matriz de aten√ß√£o
    /// 
    /// ## üéØ Par√¢metros:
    /// - `x`: Tensor de entrada [batch_size, seq_len, n_embd]
    /// - `mask`: M√°scara causal opcional para bloquear tokens futuros
    pub fn forward(&self, x: &Tensor, mask: Option<&Tensor>) -> Result<Tensor> {
        // üìè **EXTRA√á√ÉO DAS DIMENS√ïES**
        // Obt√©m as dimens√µes do tensor de entrada para reshaping posterior
        let (batch_size, seq_len, _) = x.dims3()?;
        
        // üéØ **ETAPA 1: PROJE√á√ïES LINEARES Q, K, V**
        // Transforma cada embedding em tr√™s "vis√µes" diferentes:
        // 
        // üîç Query: "Que tipo de informa√ß√£o eu estou procurando?"
        // Exemplo: Para "gato", query pode focar em "animal", "dom√©stico"
        let q = self.w_query.forward(x)?;  // [B, T, C]
        
        // üóùÔ∏è Key: "Que informa√ß√£o eu posso oferecer?"
        // Exemplo: Para "ronrona", key oferece "som", "comportamento"
        let k = self.w_key.forward(x)?;    // [B, T, C]
        
        // üíé Value: "Qual √© minha informa√ß√£o sem√¢ntica real?"
        // Exemplo: Para "felino", value cont√©m caracter√≠sticas espec√≠ficas
        let v = self.w_value.forward(x)?;  // [B, T, C]
        
        // üé≠ **ETAPA 2: DIVIS√ÉO EM M√öLTIPLAS CABE√áAS**
        // Analogia: Dividir a turma em grupos especializados
        // Cada cabe√ßa foca em um aspecto diferente da linguagem:
        // - Cabe√ßa 1: Rela√ß√µes sint√°ticas (sujeito-verbo)
        // - Cabe√ßa 2: Sem√¢ntica (significado)
        // - Cabe√ßa 3: Depend√™ncias de longo alcance
        let q = self.split_heads(&q, batch_size, seq_len)?; // [B, H, T, D]
        let k = self.split_heads(&k, batch_size, seq_len)?; // [B, H, T, D]
        let v = self.split_heads(&v, batch_size, seq_len)?; // [B, H, T, D]
        
        // üßÆ **ETAPA 3: C√ÅLCULO DOS SCORES DE ATEN√á√ÉO**
        let scores = self.attention_scores(&q, &k)?; // [B, H, T, T]
        
        // üîç **DEBUG: Verificar scores de aten√ß√£o**
        let scores_vec = scores.flatten_all()?.to_vec1::<f32>()?;
        if scores_vec.iter().any(|&x| x.is_nan()) {
            eprintln!("‚ö†Ô∏è DEBUG: Attention scores cont√©m NaN!");
            return Err(candle_core::Error::Msg("Attention scores cont√©m NaN".to_string()));
        }
        if scores_vec.iter().any(|&x| x.is_infinite()) {
            eprintln!("‚ö†Ô∏è DEBUG: Attention scores cont√©m infinito!");
            return Err(candle_core::Error::Msg("Attention scores cont√©m infinito".to_string()));
        }
        
        // üîç **DEBUG: Verificar scores antes da m√°scara**
        let scores_before_mask = scores.flatten_all()?.to_vec1::<f32>()?;
        if scores_before_mask.iter().any(|&x| x.is_nan() || x.is_infinite()) {
            eprintln!("‚ö†Ô∏è DEBUG: Scores cont√©m valores inv√°lidos ANTES da m√°scara!");
            return Err(candle_core::Error::Msg("Scores inv√°lidos antes da m√°scara".to_string()));
        }
        
        // üö´ **ETAPA 4: APLICA√á√ÉO DA M√ÅSCARA CAUSAL**
        let scores = if let Some(mask) = mask {
            // Expande m√°scara de [T, T] para [B, H, T, T]
            let expanded_mask = mask.unsqueeze(0)?.unsqueeze(0)?
                .expand(&[batch_size, self.n_head, seq_len, seq_len])?;
            
            // üîç **DEBUG: Verificar m√°scara**
            let mask_vec = expanded_mask.flatten_all()?.to_vec1::<f32>()?;
            if mask_vec.iter().any(|&x| x.is_nan() || x.is_infinite()) {
                eprintln!("‚ö†Ô∏è DEBUG: M√°scara cont√©m valores inv√°lidos!");
                return Err(candle_core::Error::Msg("M√°scara inv√°lida".to_string()));
            }
            
            // Usar valor menor para evitar overflow
            let mask_value = -1e4; // Menor que -1e9 para evitar overflow
            let masked_scores = (scores + expanded_mask * mask_value)?;
            
            // üîç **DEBUG: Verificar scores ap√≥s m√°scara**
            let masked_vec = masked_scores.flatten_all()?.to_vec1::<f32>()?;
            if masked_vec.iter().any(|&x| x.is_nan() || x.is_infinite()) {
                eprintln!("‚ö†Ô∏è DEBUG: Scores cont√©m valores inv√°lidos AP√ìS a m√°scara!");
                let max_before = scores_before_mask.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b.abs()));
                let max_mask = mask_vec.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b.abs()));
                eprintln!("‚ö†Ô∏è DEBUG: Max scores antes: {}, Max mask: {}", max_before, max_mask);
                return Err(candle_core::Error::Msg("Scores inv√°lidos ap√≥s m√°scara".to_string()));
            }
            
            masked_scores
        } else {
            scores
        };
        
        // üîç **DEBUG: Verificar scores antes do softmax**
        let scores_vec = scores.flatten_all()?.to_vec1::<f32>()?;
        if scores_vec.iter().any(|&x| x.is_nan()) {
            eprintln!("‚ö†Ô∏è DEBUG: Scores cont√©m NaN antes do softmax!");
            return Err(candle_core::Error::Msg("Scores cont√©m NaN antes do softmax".to_string()));
        }
        if scores_vec.iter().any(|&x| x.is_infinite()) {
            eprintln!("‚ö†Ô∏è DEBUG: Scores cont√©m infinito antes do softmax!");
            return Err(candle_core::Error::Msg("Scores cont√©m infinito antes do softmax".to_string()));
        }
        
        // üé≤ **ETAPA 5: SOFTMAX - CONVERS√ÉO EM PROBABILIDADES**
        // Transforma scores em probabilidades que somam 1
        // 
        // üìä Interpreta√ß√£o: "Quanto de aten√ß√£o dar para cada token?"
        // Exemplo: [0.1, 0.7, 0.2] = 10% token1, 70% token2, 20% token3
        let weights = candle_nn::ops::softmax(&scores, 3)?; // [B, H, T, T]
        
        // üéØ **ETAPA 6: DROPOUT PARA REGULARIZA√á√ÉO**
        // Durante o treinamento, "desliga" aleatoriamente algumas conex√µes
        // Previne overfitting e melhora generaliza√ß√£o
        let weights = if self.dropout > 0.0 {
            candle_nn::ops::dropout(&weights, self.dropout)?
        } else {
            weights
        };
        
        // üí´ **ETAPA 7: AGREGA√á√ÉO PONDERADA DOS VALUES**
        // Combina informa√ß√µes de todos os tokens baseado nos pesos
        // 
        // üé≠ Analogia: Cada aluno contribui com sua informa√ß√£o
        // proporcionalmente ao quanto √© "relevante" para a pergunta
        let attention = weights.matmul(&v)?; // [B, H, T, D]
        
        // üîó **ETAPA 8: CONCATENA√á√ÉO DAS CABE√áAS**
        // Junta as informa√ß√µes de todas as cabe√ßas especializadas
        // De [B, H, T, D] volta para [B, T, C] onde C = H √ó D
        let attention = self.merge_heads(&attention, batch_size, seq_len)?; // [B, T, C]
        
        // üé™ **ETAPA 9: PROJE√á√ÉO FINAL DE SA√çDA**
        // √öltima transforma√ß√£o linear para refinar o resultado
        // Permite que o modelo "misture" informa√ß√µes das diferentes cabe√ßas
        self.w_out.forward(&attention) // [B, T, C]
    }
    
    /// üîß **EXTRA√á√ÉO DE TENSORES Q, K, V PARA KERNELS FUSIONADOS**
    /// 
    /// Extrai e processa os tensores Query, Key e Value sem aplicar
    /// o mecanismo de aten√ß√£o completo. Usado por kernels fusionados
    /// para otimiza√ß√µes de performance.
    /// 
    /// ## üìä **Pipeline de Processamento:**
    /// 1. **Proje√ß√µes Lineares**: X ‚Üí Q, K, V
    /// 2. **Divis√£o em Cabe√ßas**: Reshape para multi-head
    /// 3. **Retorno**: Tensores prontos para aten√ß√£o fusionada
    /// 
    /// ## üéØ **Formato de Sa√≠da:**
    /// - Q: [batch_size, n_head, seq_len, head_dim]
    /// - K: [batch_size, n_head, seq_len, head_dim]
    /// - V: [batch_size, n_head, seq_len, head_dim]
    pub fn get_qkv_tensors(&self, x: &Tensor) -> Result<(Tensor, Tensor, Tensor)> {
        // üìè **EXTRA√á√ÉO DAS DIMENS√ïES**
        let (batch_size, seq_len, _) = x.dims3()?;
        
        // üéØ **PROJE√á√ïES LINEARES Q, K, V**
        let q = self.w_query.forward(x)?;  // [B, T, C]
        let k = self.w_key.forward(x)?;    // [B, T, C]
        let v = self.w_value.forward(x)?;  // [B, T, C]
        
        // üé≠ **DIVIS√ÉO EM M√öLTIPLAS CABE√áAS**
        let q = self.split_heads(&q, batch_size, seq_len)?; // [B, H, T, D]
        let k = self.split_heads(&k, batch_size, seq_len)?; // [B, H, T, D]
        let v = self.split_heads(&v, batch_size, seq_len)?; // [B, H, T, D]
        
        Ok((q, k, v))
    }
    
    /// üîÄ **SPLIT HEADS: DIVIDINDO EM M√öLTIPLAS PERSPECTIVAS**
    /// 
    /// Transforma um tensor "monol√≠tico" em m√∫ltiplas cabe√ßas de aten√ß√£o,
    /// permitindo que o modelo processe diferentes aspectos da informa√ß√£o
    /// simultaneamente.
    /// 
    /// ## üé≠ Analogia da Orquestra:
    /// √â como dividir uma orquestra em se√ß√µes (cordas, sopros, percuss√£o).
    /// Cada se√ß√£o toca sua parte, mas todas contribuem para a harmonia final.
    /// 
    /// ## üìê Transforma√ß√£o de Dimens√µes:
    /// ```text
    /// Input:  [B, T, C] = [batch, seq_len, n_embd]
    ///         ‚Üì reshape
    /// Step 1: [B, T, H, D] = [batch, seq_len, n_head, head_dim]
    ///         ‚Üì transpose(1,2)
    /// Output: [B, H, T, D] = [batch, n_head, seq_len, head_dim]
    /// 
    /// Onde: C = H √ó D (n_embd = n_head √ó head_dim)
    /// ```
    /// 
    /// ## üßÆ Exemplo Num√©rico:
    /// ```text
    /// n_embd=768, n_head=12 ‚Üí head_dim=64
    /// [2, 10, 768] ‚Üí [2, 10, 12, 64] ‚Üí [2, 12, 10, 64]
    /// ```
    fn split_heads(&self, x: &Tensor, batch_size: usize, seq_len: usize) -> Result<Tensor> {
        // üìè **PASSO 1: RESHAPE PARA EXPOR AS CABE√áAS**
        // Divide a dimens√£o de embedding em (n_head, head_dim)
        // [B, T, C] ‚Üí [B, T, H, D]
        let reshaped = x.reshape(&[batch_size, seq_len, self.n_head, self.head_dim])?;
        
        // üîÑ **PASSO 2: TRANSPOSE PARA AGRUPAR POR CABE√áA**
        // Move a dimens√£o das cabe√ßas para a segunda posi√ß√£o
        // [B, T, H, D] ‚Üí [B, H, T, D]
        // Isso permite processamento paralelo de cada cabe√ßa
        // Agora cada cabe√ßa pode processar independentemente!
        reshaped.transpose(1, 2)?.contiguous()
    }
    
    /// üîó **MERGE HEADS: REUNINDO AS PERSPECTIVAS**
    /// 
    /// Reconstr√≥i o tensor original concatenando todas as cabe√ßas,
    /// combinando as diferentes perspectivas em uma representa√ß√£o unificada.
    /// 
    /// ## üé≠ Analogia da Orquestra:
    /// √â como mixar todas as se√ß√µes da orquestra em uma grava√ß√£o final.
    /// Cada instrumento contribuiu sua parte, agora temos a m√∫sica completa.
    /// 
    /// ## üìê Transforma√ß√£o de Dimens√µes:
    /// ```text
    /// Input:  [B, H, T, D] = [batch, n_head, seq_len, head_dim]
    ///         ‚Üì transpose(1,2)
    /// Step 1: [B, T, H, D] = [batch, seq_len, n_head, head_dim]
    ///         ‚Üì reshape
    /// Output: [B, T, C] = [batch, seq_len, n_embd]
    /// 
    /// Onde: C = H √ó D (concatena√ß√£o das cabe√ßas)
    /// ```
    fn merge_heads(&self, x: &Tensor, batch_size: usize, seq_len: usize) -> Result<Tensor> {
        // üîÑ **PASSO 1: TRANSPOSE DE VOLTA**
        // [B, H, T, D] ‚Üí [B, T, H, D]
        // Reorganiza para que as cabe√ßas fiquem na √∫ltima dimens√£o
        let transposed = x.transpose(1, 2)?.contiguous();
        
        // üîó **PASSO 2: CONCATENA√á√ÉO DAS CABE√áAS**
        // [B, T, H, D] ‚Üí [B, T, C] onde C = H √ó D
        // Achata as dimens√µes H e D em uma √∫nica dimens√£o
        transposed?.reshape(&[batch_size, seq_len, self.n_embd])
    }
    
    /// üßÆ **ATTENTION SCORES: O CORA√á√ÉO DO MECANISMO**
    /// 
    /// Calcula a "compatibilidade" entre queries e keys usando o produto
    /// escalar, implementando a f√≥rmula fundamental da aten√ß√£o.
    /// 
    /// ## üìä F√≥rmula Matem√°tica:
    /// ```text
    /// Attention(Q,K,V) = softmax(QK^T / ‚àöd_k)V
    ///                              ‚Üë
    ///                    Esta fun√ß√£o calcula esta parte
    /// ```
    /// 
    /// ## üéØ Por que dividir por ‚àöd_k?
    /// 
    /// ### üìà **Problema dos Gradientes Explosivos:**
    /// - Sem escalonamento: valores muito grandes ‚Üí softmax saturado
    /// - Com ‚àöd_k: valores controlados ‚Üí gradientes est√°veis
    /// 
    /// ### üßÆ **Intui√ß√£o Matem√°tica:**
    /// ```text
    /// Se Q e K t√™m vari√¢ncia 1, ent√£o QK^T tem vari√¢ncia d_k
    /// Dividindo por ‚àöd_k, restauramos vari√¢ncia ‚âà 1
    /// ```
    /// 
    /// ### üìä **Exemplo Pr√°tico:**
    /// ```text
    /// d_k = 64 ‚Üí ‚àöd_k = 8
    /// Scores antes: [-50, 30, 80] (muito extremos)
    /// Scores depois: [-6.25, 3.75, 10] (mais balanceados)
    /// ```
    /// 
    /// ## üé≠ Analogia:
    /// √â como ajustar o volume de um amplificador - muito alto
    /// e voc√™ n√£o consegue distinguir os detalhes, muito baixo
    /// e voc√™ n√£o ouve nada!
    fn attention_scores(&self, q: &Tensor, k: &Tensor) -> Result<Tensor> {
        // üîç **DEBUG: Verificar Q e K**
        let q_vec = q.flatten_all()?.to_vec1::<f32>()?;
        let k_vec = k.flatten_all()?.to_vec1::<f32>()?;
        
        if q_vec.iter().any(|&x| x.is_nan() || x.is_infinite()) {
            eprintln!("‚ö†Ô∏è DEBUG: Q cont√©m NaN ou infinito!");
            return Err(candle_core::Error::Msg("Q cont√©m valores inv√°lidos".to_string()));
        }
        
        if k_vec.iter().any(|&x| x.is_nan() || x.is_infinite()) {
            eprintln!("‚ö†Ô∏è DEBUG: K cont√©m NaN ou infinito!");
            return Err(candle_core::Error::Msg("K cont√©m valores inv√°lidos".to_string()));
        }
        
        let q_max = q_vec.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b.abs()));
        let k_max = k_vec.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b.abs()));
        
        if q_max > 100.0 || k_max > 100.0 {
            eprintln!("‚ö†Ô∏è DEBUG: Valores muito grandes - Q_max: {}, K_max: {}", q_max, k_max);
        }
        
        // üìê **C√ÅLCULO DO FATOR DE ESCALA**
        let scale = 1.0 / (self.head_dim as f64).sqrt();
        
        // üîÑ **PRODUTO MATRICIAL Q @ K^T**
        let scores = q.matmul(&k.transpose(2, 3)?)?;
        
        // üîç **DEBUG: Verificar scores ap√≥s matmul**
        let scores_vec = scores.flatten_all()?.to_vec1::<f32>()?;
        if scores_vec.iter().any(|&x| x.is_nan() || x.is_infinite()) {
            eprintln!("‚ö†Ô∏è DEBUG: Scores cont√©m valores inv√°lidos ap√≥s matmul!");
            let max_score = scores_vec.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b.abs()));
            eprintln!("‚ö†Ô∏è DEBUG: Max score absoluto: {}", max_score);
            return Err(candle_core::Error::Msg("Scores inv√°lidos ap√≥s matmul".to_string()));
        }
        
        // ‚öñÔ∏è **APLICA√á√ÉO DO ESCALONAMENTO**
        let scaled_scores = (scores * scale)?;
        
        // üîç **DEBUG: Verificar scores ap√≥s escalonamento**
        let scaled_vec = scaled_scores.flatten_all()?.to_vec1::<f32>()?;
        if scaled_vec.iter().any(|&x| x.is_nan() || x.is_infinite()) {
            eprintln!("‚ö†Ô∏è DEBUG: Scores cont√©m valores inv√°lidos ap√≥s escalonamento!");
            eprintln!("‚ö†Ô∏è DEBUG: Scale factor: {}", scale);
            return Err(candle_core::Error::Msg("Scores inv√°lidos ap√≥s escalonamento".to_string()));
        }
        
        Ok(scaled_scores)
    }
}

/// üé≠ **MULTI-HEAD ATTENTION: M√öLTIPLAS PERSPECTIVAS SIMULT√ÇNEAS**
/// 
/// Esta estrutura √© um wrapper elegante em torno do SelfAttention,
/// fornecendo uma interface limpa para o mecanismo de aten√ß√£o multi-cabe√ßa.
/// 
/// ## üé¨ **Analogia do Cinema:**
/// Imagine assistir a um filme com m√∫ltiplas c√¢meras simultaneamente:
/// - **C√¢mera 1**: Foco nos rostos (express√µes emocionais)
/// - **C√¢mera 2**: Foco nas a√ß√µes (movimentos corporais)
/// - **C√¢mera 3**: Foco no cen√°rio (contexto ambiental)
/// - **C√¢mera N**: Cada uma captura aspectos √∫nicos!
/// 
/// ## üß† **Por que Multi-Head?**
/// 
/// ### üéØ **Especializa√ß√£o de Cabe√ßas:**
/// Cada cabe√ßa pode aprender a focar em diferentes aspectos:
/// - **Sint√°tico**: Rela√ß√µes gramaticais (sujeito-verbo-objeto)
/// - **Sem√¢ntico**: Significados e conceitos relacionados
/// - **Posicional**: Depend√™ncias de curta e longa dist√¢ncia
/// - **Contextual**: Nuances e ambiguidades
/// 
/// ### üìä **Vantagens Computacionais:**
/// - **Paraleliza√ß√£o**: Todas as cabe√ßas processam simultaneamente
/// - **Diversidade**: M√∫ltiplas representa√ß√µes do mesmo input
/// - **Robustez**: Se uma cabe√ßa falha, outras compensam
/// 
/// ## üîÑ **Fluxo de Processamento:**
/// ```text
/// Input: [B, T, C]
///   ‚Üì
/// SelfAttention (com N cabe√ßas internas)
///   ‚Üì
/// Output: [B, T, C]
/// ```
pub struct MultiHeadAttention {
    /// üß† Inst√¢ncia do mecanismo de self-attention
    /// Cont√©m todas as N cabe√ßas e l√≥gica de processamento
    attention: SelfAttention,
}

impl MultiHeadAttention {
    /// üèóÔ∏è **CONSTRUTOR: CRIANDO O MECANISMO MULTI-CABE√áA**
    /// 
    /// Inicializa uma nova inst√¢ncia de Multi-Head Attention,
    /// delegando toda a complexidade para a estrutura SelfAttention.
    /// 
    /// ## üéØ **Par√¢metros:**
    /// - `n_embd`: Dimens√£o dos embeddings (largura do modelo)
    /// - `n_head`: N√∫mero de cabe√ßas de aten√ß√£o paralelas
    /// - `dropout`: Taxa de regulariza√ß√£o durante treinamento
    /// - `vb`: VarBuilder para inicializa√ß√£o de pesos
    /// 
    /// ## üìê **Valida√ß√£o Autom√°tica:**
    /// - Verifica se n_embd √© divis√≠vel por n_head
    /// - Garante dimens√µes consistentes para todas as cabe√ßas
    pub fn new(n_embd: usize, n_head: usize, dropout: f32, vb: VarBuilder) -> Result<Self> {
        Ok(Self {
            attention: SelfAttention::new(n_embd, n_head, dropout, vb)?,
        })
    }
    
    /// üöÄ **FORWARD PASS: PROCESSAMENTO MULTI-CABE√áA**
    /// 
    /// Executa o mecanismo completo de aten√ß√£o multi-cabe√ßa,
    /// permitindo que o modelo "olhe" para a sequ√™ncia atrav√©s
    /// de m√∫ltiplas perspectivas especializadas simultaneamente.
    /// 
    /// ## üîÑ **Processo Interno:**
    /// 1. **Proje√ß√£o**: X ‚Üí Q, K, V (para todas as cabe√ßas)
    /// 2. **Divis√£o**: Separa em N cabe√ßas independentes
    /// 3. **Aten√ß√£o**: Cada cabe√ßa calcula sua pr√≥pria aten√ß√£o
    /// 4. **Concatena√ß√£o**: Junta resultados de todas as cabe√ßas
    /// 5. **Proje√ß√£o Final**: Transforma resultado concatenado
    /// 
    /// ## üéØ **Par√¢metros:**
    /// - `x`: Tensor de entrada [batch_size, seq_len, n_embd]
    /// - `mask`: M√°scara causal opcional (para modelos autoregressivos)
    /// 
    /// ## üì§ **Retorno:**
    /// - Tensor processado [batch_size, seq_len, n_embd]
    ///   com informa√ß√µes contextuais enriquecidas
    pub fn forward(&self, x: &Tensor, mask: Option<&Tensor>) -> Result<Tensor> {
        self.attention.forward(x, mask)
    }
    
    /// üîß **EXTRA√á√ÉO DE TENSORES Q, K, V PARA KERNELS FUSIONADOS**
    /// 
    /// Extrai os tensores Query, Key e Value processados para uso
    /// em kernels fusionados de aten√ß√£o otimizados.
    /// 
    /// ## üìä **Formato de Sa√≠da:**
    /// - Q: [batch_size, n_head, seq_len, head_dim]
    /// - K: [batch_size, n_head, seq_len, head_dim] 
    /// - V: [batch_size, n_head, seq_len, head_dim]
    /// 
    /// ## ‚ö° **Uso em Kernels Fusionados:**
    /// ```rust
    /// let (q, k, v) = attention.get_qkv_tensors(&input)?;
    /// let output = fused_attention_kernel.forward(&q, &k, &v, mask, dropout)?;
    /// ```
    pub fn get_qkv_tensors(&self, x: &Tensor) -> Result<(Tensor, Tensor, Tensor)> {
        self.attention.get_qkv_tensors(x)
    }
}