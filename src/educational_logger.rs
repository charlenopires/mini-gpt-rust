//! # ğŸ“š Sistema de Logging Educacional AvanÃ§ado para Mini-GPT
//!
//! Este mÃ³dulo implementa um sistema completo de logging educacional que acompanha
//! todo o ciclo de vida de um Large Language Model (LLM), desde a inicializaÃ§Ã£o
//! atÃ© o treinamento e inferÃªncia.
//!
//! ## ğŸ§  Por que um Sistema de Log Educacional?
//!
//! Um LLM Ã© uma "caixa preta" complexa. Este sistema torna visÃ­vel:
//! - **Como os dados fluem** atravÃ©s das camadas neurais
//! - **O que acontece matematicamente** em cada operaÃ§Ã£o
//! - **Como o modelo aprende** durante o treinamento
//! - **Por que certas decisÃµes** sÃ£o tomadas durante a inferÃªncia
//!
//! ## ğŸ¯ Fases do Ciclo de Vida de um LLM
//!
//! ### 1. ğŸ—ï¸ **INICIALIZAÃ‡ÃƒO**
//! - CriaÃ§Ã£o da arquitetura Transformer
//! - InicializaÃ§Ã£o de pesos (Xavier, He, etc.)
//! - ConfiguraÃ§Ã£o de hiperparÃ¢metros
//!
//! ### 2. ğŸ“Š **PRÃ‰-PROCESSAMENTO**
//! - TokenizaÃ§Ã£o do texto de entrada
//! - CriaÃ§Ã£o de embeddings de posiÃ§Ã£o
//! - PreparaÃ§Ã£o de mÃ¡scaras de atenÃ§Ã£o
//!
//! ### 3. ğŸ“ **TREINAMENTO**
//! - Forward pass: dados â†’ prediÃ§Ãµes
//! - CÃ¡lculo da loss function
//! - Backward pass: gradientes
//! - AtualizaÃ§Ã£o de pesos (otimizador)
//!
//! ### 4. ğŸ”® **INFERÃŠNCIA**
//! - GeraÃ§Ã£o autoregressiva de tokens
//! - AplicaÃ§Ã£o de estratÃ©gias de sampling
//! - DecodificaÃ§Ã£o para texto final
//!
//! ## ğŸ“ˆ Tipos de Logs Implementados
//!
//! - **ğŸ—ï¸ STRUCTURAL**: Arquitetura e dimensÃµes
//! - **ğŸ§® MATHEMATICAL**: OperaÃ§Ãµes e cÃ¡lculos
//! - **âš¡ PERFORMANCE**: Tempos e memÃ³ria
//! - **ğŸ“ EDUCATIONAL**: ExplicaÃ§Ãµes conceituais
//! - **ğŸ” DEBUGGING**: DetecÃ§Ã£o de problemas
//! - **ğŸ“Š METRICS**: MÃ©tricas de treinamento

use std::collections::HashMap;
use candle_core::Tensor;
use crate::tokenizer::BPETokenizer;
use anyhow::Result;
// use std::fmt::Write; // Removido - nÃ£o utilizado
use std::time::Instant;

/// ğŸ“ **SISTEMA DE LOGGING EDUCACIONAL AVANÃ‡ADO**
///
/// Este sistema monitora e explica todo o ciclo de vida de um LLM:
/// desde a inicializaÃ§Ã£o atÃ© a geraÃ§Ã£o de texto, tornando visÃ­vel
/// cada processo interno que normalmente seria "invisÃ­vel".
///
/// ## ğŸ” O que Este Sistema Revela:
///
/// ### Durante a InicializaÃ§Ã£o:
/// - Como os pesos sÃ£o inicializados (distribuiÃ§Ãµes, escalas)
/// - Quantos parÃ¢metros o modelo possui
/// - Como a memÃ³ria Ã© alocada
///
/// ### Durante o Treinamento:
/// - Como a loss diminui a cada Ã©poca
/// - Como os gradientes fluem (ou nÃ£o) pelas camadas
/// - Quando ocorre overfitting ou underfitting
/// - Performance de cada componente (atenÃ§Ã£o, FFN, etc.)
///
/// ### Durante a InferÃªncia:
/// - Como cada token influencia a prediÃ§Ã£o do prÃ³ximo
/// - Quais tokens recebem mais "atenÃ§Ã£o"
/// - Como as probabilidades sÃ£o calculadas
/// - Por que certas palavras sÃ£o escolhidas
#[derive(Debug, Clone)]
pub struct EducationalLogger {
    /// ğŸ“š HistÃ³rico completo de todas as operaÃ§Ãµes
    pub operations: Vec<LogEntry>,
    /// â±ï¸ MediÃ§Ã£o de tempos para anÃ¡lise de performance
    pub timestamps: HashMap<String, Instant>,
    /// ğŸ“Š EstatÃ­sticas acumuladas durante execuÃ§Ã£o
    pub stats: LogStats,
    /// ğŸ¯ NÃ­vel de detalhamento (0=mÃ­nimo, 3=mÃ¡ximo)
    pub verbosity: u8,
    /// ğŸ—ï¸ Fase atual do modelo (Init, Training, Inference)
    pub current_phase: ModelPhase,
    /// ğŸ“ˆ MÃ©tricas de treinamento por Ã©poca
    pub training_metrics: Vec<TrainingEpochMetrics>,
    /// ğŸ§® Contador de operaÃ§Ãµes matemÃ¡ticas
    pub operation_counts: HashMap<String, u64>,
    /// ğŸ”§ ConfiguraÃ§Ãµes de visualizaÃ§Ã£o
     pub show_tensors: bool,
     pub show_attention: bool,
}

/// ğŸ—ï¸ **FASES DO CICLO DE VIDA DO MODELO**
///
/// Cada fase tem caracterÃ­sticas e mÃ©tricas especÃ­ficas que devem
/// ser monitoradas de forma diferente.
#[derive(Debug, Clone, PartialEq)]
pub enum ModelPhase {
    /// ğŸ—ï¸ Inicializando arquitetura e pesos
    Initialization,
    /// ğŸ“Š Preparando dados de entrada
    Preprocessing,
    /// ğŸ“ Treinando o modelo (aprendendo padrÃµes)
    Training { epoch: u32, batch: u32 },
    /// ğŸ”® Gerando texto (inferÃªncia)
    Inference { step: u32 },
    /// âœ… Avaliando performance
    Evaluation,
}

/// ğŸ“Š **MÃ‰TRICAS DE UMA Ã‰POCA DE TREINAMENTO**
///
/// Captura todas as informaÃ§Ãµes importantes de uma Ã©poca,
/// permitindo anÃ¡lise de convergÃªncia e debugging.
#[derive(Debug, Clone)]
pub struct TrainingEpochMetrics {
    /// ğŸ“ˆ NÃºmero da Ã©poca
    pub epoch: u32,
    /// ğŸ’” Loss mÃ©dia da Ã©poca
    pub avg_loss: f32,
    /// ğŸ“‰ Loss mÃ­nima observada
    pub min_loss: f32,
    /// ğŸ“ˆ Loss mÃ¡xima observada
    pub max_loss: f32,
    /// â±ï¸ Tempo total da Ã©poca
    pub duration_ms: u64,
    /// ğŸ¯ AcurÃ¡cia (se disponÃ­vel)
    pub accuracy: Option<f32>,
    /// ğŸ“Š Norma dos gradientes
    pub gradient_norm: f32,
    /// ğŸ§  Taxa de aprendizado usada
    pub learning_rate: f32,
    /// ğŸ’¾ Uso de memÃ³ria (MB)
    pub memory_usage_mb: f32,
}

/// ğŸ“ **ENTRADA DE LOG DETALHADA**
///
/// Cada operaÃ§Ã£o importante gera uma entrada de log com
/// contexto completo para anÃ¡lise posterior.
#[derive(Debug, Clone)]
pub struct LogEntry {
    /// â° Timestamp da operaÃ§Ã£o
    pub timestamp: Instant,
    /// ğŸ·ï¸ Tipo da operaÃ§Ã£o
    pub operation_type: OperationType,
    /// ğŸ“ DescriÃ§Ã£o educacional
    pub description: String,
    /// ğŸ“Š Dados da operaÃ§Ã£o
    pub data: LogData,
    /// ğŸ¯ NÃ­vel de importÃ¢ncia (0-3)
    pub importance: u8,
}

/// ğŸ”§ **TIPOS DE OPERAÃ‡Ã•ES MONITORADAS**
#[derive(Debug, Clone)]
pub enum OperationType {
    /// ğŸ—ï¸ InicializaÃ§Ã£o de componentes
    Initialization(String),
    /// ğŸ”„ Forward pass
    Forward(String),
    /// â¬…ï¸ Backward pass
    Backward(String),
    /// ğŸ¯ CÃ¡lculo de atenÃ§Ã£o
    Attention(String),
    /// ğŸ§® OperaÃ§Ã£o matemÃ¡tica
    Mathematical(String),
    /// ğŸ“Š MÃ©trica calculada
    Metric(String),
    /// âš ï¸ Aviso ou problema detectado
    Warning(String),
}

/// ğŸ“Š **DADOS ASSOCIADOS A UMA OPERAÃ‡ÃƒO**
#[derive(Debug, Clone)]
pub enum LogData {
    /// ğŸ“ InformaÃ§Ãµes sobre tensores
    TensorInfo {
        shape: Vec<usize>,
        dtype: String,
        device: String,
        memory_mb: f32,
    },
    /// ğŸ§® Resultado de operaÃ§Ã£o matemÃ¡tica
    MathResult {
        input_shapes: Vec<Vec<usize>>,
        output_shape: Vec<usize>,
        operation: String,
        flops: u64,
    },
    /// ğŸ“ˆ MÃ©trica numÃ©rica
    Metric {
        name: String,
        value: f32,
        unit: String,
    },
    /// â±ï¸ InformaÃ§Ã£o de performance
    Performance {
        duration_ms: f64,
        memory_delta_mb: f32,
        cpu_usage: f32,
    },
    /// ğŸ“ Texto explicativo
    Text(String),
}

/// ğŸ“Š **ESTATÃSTICAS ACUMULADAS**
#[derive(Debug, Clone, Default)]
pub struct LogStats {
    /// ğŸ”¢ Total de operaÃ§Ãµes realizadas
    pub total_operations: u64,
    /// â±ï¸ Tempo total de execuÃ§Ã£o (ms)
    pub total_time_ms: f64,
    /// ğŸ’¾ Pico de uso de memÃ³ria (MB)
    pub peak_memory_mb: f32,
    /// ğŸ§® Total de FLOPs executados
    pub total_flops: u64,
    /// ğŸ“Š DistribuiÃ§Ã£o de tipos de operaÃ§Ã£o
     pub operation_distribution: HashMap<String, u64>,
 }

impl EducationalLogger {
    /// ğŸ—ï¸ **CRIAR NOVO LOGGER EDUCACIONAL**
    ///
    /// Inicializa o sistema de logging com configuraÃ§Ãµes padrÃ£o
    /// otimizadas para aprendizado e debugging.
    pub fn new(verbosity: u8) -> Self {
        println!("\nğŸ“ ===== SISTEMA DE LOGGING EDUCACIONAL INICIADO =====");
        println!("ğŸ“š Este sistema vai mostrar como um LLM funciona internamente!");
        println!("ğŸ” NÃ­vel de detalhamento: {} (0=bÃ¡sico, 3=expert)", verbosity);
        
        Self {
            operations: Vec::new(),
            timestamps: HashMap::new(),
            stats: LogStats::default(),
            verbosity,
            current_phase: ModelPhase::Initialization,
            training_metrics: Vec::new(),
            operation_counts: HashMap::new(),
            show_tensors: verbosity >= 2,
            show_attention: verbosity >= 1,
        }
    }

    /// ğŸ—ï¸ **INICIAR FASE DO MODELO**
    ///
    /// Marca o inÃ­cio de uma nova fase (inicializaÃ§Ã£o, treinamento, inferÃªncia)
    /// e configura o logging apropriado para essa fase.
    pub fn start_phase(&mut self, phase: ModelPhase) {
        self.current_phase = phase.clone();
        
        match &phase {
            ModelPhase::Initialization => {
                println!("\nğŸ—ï¸ ===== FASE: INICIALIZAÃ‡ÃƒO DO MODELO =====");
                println!("ğŸ“‹ Nesta fase, vamos:");
                println!("   â€¢ Criar a arquitetura Transformer");
                println!("   â€¢ Inicializar todos os pesos neurais");
                println!("   â€¢ Configurar hiperparÃ¢metros");
                println!("   â€¢ Alocar memÃ³ria necessÃ¡ria");
            },
            ModelPhase::Preprocessing => {
                println!("\nğŸ“Š ===== FASE: PRÃ‰-PROCESSAMENTO =====");
                println!("ğŸ“‹ Nesta fase, vamos:");
                println!("   â€¢ Tokenizar o texto de entrada");
                println!("   â€¢ Criar embeddings de palavras");
                println!("   â€¢ Adicionar informaÃ§Ãµes de posiÃ§Ã£o");
                println!("   â€¢ Preparar mÃ¡scaras de atenÃ§Ã£o");
            },
            ModelPhase::Training { epoch, batch } => {
                println!("\nğŸ“ ===== FASE: TREINAMENTO (Ã‰poca {}, Batch {}) =====", epoch, batch);
                println!("ğŸ“‹ Nesta fase, vamos:");
                println!("   â€¢ Fazer forward pass (dados â†’ prediÃ§Ãµes)");
                println!("   â€¢ Calcular a loss (erro do modelo)");
                println!("   â€¢ Fazer backward pass (calcular gradientes)");
                println!("   â€¢ Atualizar pesos (aprender!)");
            },
            ModelPhase::Inference { step } => {
                println!("\nğŸ”® ===== FASE: INFERÃŠNCIA (Passo {}) =====", step);
                println!("ğŸ“‹ Nesta fase, vamos:");
                println!("   â€¢ Processar tokens de entrada");
                println!("   â€¢ Calcular probabilidades de prÃ³ximos tokens");
                println!("   â€¢ Escolher prÃ³ximo token (sampling)");
                println!("   â€¢ Gerar texto de forma autoregressiva");
            },
            ModelPhase::Evaluation => {
                println!("\nâœ… ===== FASE: AVALIAÃ‡ÃƒO =====");
                println!("ğŸ“‹ Nesta fase, vamos:");
                println!("   â€¢ Testar o modelo em dados nÃ£o vistos");
                println!("   â€¢ Calcular mÃ©tricas de performance");
                println!("   â€¢ Analisar qualidade das prediÃ§Ãµes");
            },
        }
        
        self.start_timer(&format!("phase_{:?}", phase));
    }

    /// â±ï¸ **INICIAR CRONÃ”METRO**
    ///
    /// Marca o inÃ­cio de uma operaÃ§Ã£o para mediÃ§Ã£o de tempo.
    pub fn start_timer(&mut self, operation: &str) {
        self.timestamps.insert(operation.to_string(), Instant::now());
        
        if self.verbosity >= 2 {
            println!("â±ï¸ Iniciando cronÃ´metro para: {}", operation);
        }
    }

    /// â¹ï¸ **PARAR CRONÃ”METRO E REGISTRAR**
    ///
    /// Para o cronÃ´metro e registra o tempo decorrido.
    pub fn end_timer(&mut self, operation: &str) -> f64 {
        if let Some(start_time) = self.timestamps.remove(operation) {
            let duration = start_time.elapsed().as_secs_f64() * 1000.0; // em ms
            
            if self.verbosity >= 2 {
                println!("â¹ï¸ {} concluÃ­do em {:.2}ms", operation, duration);
            }
            
            self.stats.total_time_ms += duration;
            duration
        } else {
             0.0
         }
     }

    /// ğŸ“Š **REGISTRAR OPERAÃ‡ÃƒO MATEMÃTICA**
    ///
    /// Documenta uma operaÃ§Ã£o matemÃ¡tica com contexto educacional completo.
    pub fn log_math_operation(
        &mut self,
        operation_name: &str,
        input_shapes: Vec<Vec<usize>>,
        output_shape: Vec<usize>,
        explanation: &str,
    ) {
        // Calcular FLOPs aproximados
        let flops = self.estimate_flops(&operation_name, &input_shapes, &output_shape);
        
        if self.verbosity >= 1 {
            println!("\nğŸ§® === OPERAÃ‡ÃƒO MATEMÃTICA: {} ===", operation_name);
            println!("ğŸ“ ExplicaÃ§Ã£o: {}", explanation);
            
            if self.show_tensors {
                println!("ğŸ“ Formas dos tensores:");
                for (i, shape) in input_shapes.iter().enumerate() {
                    println!("   ğŸ“¥ Entrada {}: {:?}", i + 1, shape);
                }
                println!("   ğŸ“¤ SaÃ­da: {:?}", output_shape);
                println!("âš¡ FLOPs estimados: {}", self.format_flops(flops));
            }
        }
        
        // Registrar entrada de log
        let entry = LogEntry {
            timestamp: Instant::now(),
            operation_type: OperationType::Mathematical(operation_name.to_string()),
            description: explanation.to_string(),
            data: LogData::MathResult {
                input_shapes,
                output_shape,
                operation: operation_name.to_string(),
                flops,
            },
            importance: 2,
        };
        
        self.operations.push(entry);
        self.stats.total_operations += 1;
        self.stats.total_flops += flops;
        
        // Atualizar contador de operaÃ§Ãµes
        *self.operation_counts.entry(operation_name.to_string()).or_insert(0) += 1;
    }

    /// ğŸ¯ **REGISTRAR OPERAÃ‡ÃƒO DE ATENÃ‡ÃƒO**
    ///
    /// Documenta o mecanismo de atenÃ§Ã£o com explicaÃ§Ãµes detalhadas.
    pub fn log_attention_operation(
        &mut self,
        layer: usize,
        head: usize,
        seq_len: usize,
        attention_scores: Option<&Tensor>,
    ) -> Result<()> {
        if self.verbosity >= 1 {
            println!("\nğŸ¯ === MECANISMO DE ATENÃ‡ÃƒO ====");
            println!("ğŸ·ï¸ Camada: {}, CabeÃ§a: {}", layer, head);
            println!("ğŸ“ SequÃªncia: {} tokens", seq_len);
            
            println!("\nğŸ§  Como a AtenÃ§Ã£o Funciona:");
            println!("   1ï¸âƒ£ Cada token 'olha' para todos os outros tokens");
            println!("   2ï¸âƒ£ Calcula o quÃ£o 'importante' cada token Ã©");
            println!("   3ï¸âƒ£ Cria uma representaÃ§Ã£o ponderada baseada na importÃ¢ncia");
            println!("   4ï¸âƒ£ Isso permite que o modelo entenda relaÃ§Ãµes entre palavras");
            
            if self.show_attention && attention_scores.is_some() {
                self.visualize_attention_pattern(attention_scores.unwrap(), seq_len)?;
            }
        }
        
        let entry = LogEntry {
            timestamp: Instant::now(),
            operation_type: OperationType::Attention(format!("layer_{}_head_{}", layer, head)),
            description: format!(
                "AtenÃ§Ã£o multi-cabeÃ§a: camada {} cabeÃ§a {} processando {} tokens. \
                 Cada token calcula sua relevÃ¢ncia com todos os outros tokens.",
                layer, head, seq_len
            ),
            data: LogData::TensorInfo {
                shape: vec![seq_len, seq_len],
                dtype: "f32".to_string(),
                device: "cpu".to_string(),
                memory_mb: (seq_len * seq_len * 4) as f32 / 1024.0 / 1024.0,
            },
            importance: 3,
        };
        
        self.operations.push(entry);
        Ok(())
    }

    /// ğŸ“ˆ **REGISTRAR MÃ‰TRICAS DE TREINAMENTO**
    ///
    /// Documenta mÃ©tricas de uma Ã©poca de treinamento com anÃ¡lise educacional.
    pub fn log_training_epoch(
        &mut self,
        epoch: u32,
        avg_loss: f32,
        min_loss: f32,
        max_loss: f32,
        gradient_norm: f32,
        learning_rate: f32,
        duration_ms: u64,
    ) {
        let metrics = TrainingEpochMetrics {
            epoch,
            avg_loss,
            min_loss,
            max_loss,
            duration_ms,
            accuracy: None,
            gradient_norm,
            learning_rate,
            memory_usage_mb: 0.0, // TODO: implementar mediÃ§Ã£o real
        };
        
        if self.verbosity >= 1 {
            println!("\nğŸ“ˆ === MÃ‰TRICAS DA Ã‰POCA {} ===", epoch);
            println!("ğŸ’” Loss mÃ©dia: {:.6}", avg_loss);
            println!("ğŸ“‰ Loss mÃ­nima: {:.6}", min_loss);
            println!("ğŸ“ˆ Loss mÃ¡xima: {:.6}", max_loss);
            println!("ğŸ“Š Norma dos gradientes: {:.6}", gradient_norm);
            println!("ğŸ§  Taxa de aprendizado: {:.6}", learning_rate);
            println!("â±ï¸ DuraÃ§Ã£o: {}ms", duration_ms);
            
            // AnÃ¡lise educacional
            self.analyze_training_progress(&metrics);
        }
        
        self.training_metrics.push(metrics);
    }

    /// ğŸ” **ANALISAR PROGRESSO DO TREINAMENTO**
    ///
    /// Fornece insights educacionais sobre o progresso do treinamento.
    fn analyze_training_progress(&self, current: &TrainingEpochMetrics) {
        println!("\nğŸ” === ANÃLISE EDUCACIONAL ===");
        
        if let Some(previous) = self.training_metrics.last() {
            let loss_change = current.avg_loss - previous.avg_loss;
            let loss_change_percent = (loss_change / previous.avg_loss) * 100.0;
            
            if loss_change < 0.0 {
                println!("âœ… Ã“timo! A loss diminuiu {:.2}% ({:.6})", 
                        loss_change_percent.abs(), loss_change.abs());
                println!("   ğŸ“š Isso significa que o modelo estÃ¡ aprendendo!");
            } else {
                println!("âš ï¸ A loss aumentou {:.2}% (+{:.6})", 
                        loss_change_percent, loss_change);
                println!("   ğŸ“š PossÃ­veis causas: taxa de aprendizado alta, overfitting, ou dados ruins");
            }
        }
        
        // AnÃ¡lise da norma dos gradientes
        if current.gradient_norm > 10.0 {
            println!("âš ï¸ Norma dos gradientes alta ({:.2})", current.gradient_norm);
            println!("   ğŸ“š Pode indicar exploding gradients - considere gradient clipping");
        } else if current.gradient_norm < 0.001 {
            println!("âš ï¸ Norma dos gradientes muito baixa ({:.6})", current.gradient_norm);
            println!("   ğŸ“š Pode indicar vanishing gradients - verifique inicializaÃ§Ã£o dos pesos");
        } else {
            println!("âœ… Norma dos gradientes saudÃ¡vel ({:.4})", current.gradient_norm);
        }
    }

    /// âš¡ **ESTIMAR FLOPs DE UMA OPERAÃ‡ÃƒO**
    ///
    /// Calcula aproximadamente quantas operaÃ§Ãµes de ponto flutuante sÃ£o necessÃ¡rias.
    fn estimate_flops(&self, operation: &str, inputs: &[Vec<usize>], output: &[usize]) -> u64 {
        match operation {
            "matmul" | "linear" => {
                if inputs.len() >= 2 {
                    let a_shape = &inputs[0];
                    let b_shape = &inputs[1];
                    if a_shape.len() >= 2 && b_shape.len() >= 2 {
                        let m = a_shape[a_shape.len() - 2];
                        let k = a_shape[a_shape.len() - 1];
                        let n = b_shape[b_shape.len() - 1];
                        return (2 * m * k * n) as u64;
                    }
                }
            },
            "softmax" => {
                let total_elements: usize = output.iter().product();
                return (3 * total_elements) as u64; // exp + sum + div
            },
            "gelu" | "relu" => {
                let total_elements: usize = output.iter().product();
                return total_elements as u64;
            },
            _ => {},
        }
        0
    }

    /// ğŸ“Š **FORMATAR FLOPs PARA EXIBIÃ‡ÃƒO**
    fn format_flops(&self, flops: u64) -> String {
        if flops >= 1_000_000_000 {
            format!("{:.2}G FLOPs", flops as f64 / 1_000_000_000.0)
        } else if flops >= 1_000_000 {
            format!("{:.2}M FLOPs", flops as f64 / 1_000_000.0)
        } else if flops >= 1_000 {
            format!("{:.2}K FLOPs", flops as f64 / 1_000.0)
        } else {
            format!("{} FLOPs", flops)
        }
    }

    /// ğŸ¯ **VISUALIZAR PADRÃƒO DE ATENÃ‡ÃƒO**
    ///
    /// Mostra uma representaÃ§Ã£o visual simplificada dos pesos de atenÃ§Ã£o.
    fn visualize_attention_pattern(&self, _attention_scores: &Tensor, seq_len: usize) -> Result<()> {
        println!("\nğŸ¯ PadrÃ£o de AtenÃ§Ã£o (simplificado):");
        println!("   ğŸ“Š Cada linha mostra para onde um token 'presta atenÃ§Ã£o'");
        println!("   ğŸ”¥ = alta atenÃ§Ã£o, ğŸ”¸ = mÃ©dia atenÃ§Ã£o, âšª = baixa atenÃ§Ã£o\n");
        
        // SimplificaÃ§Ã£o: mostrar apenas uma representaÃ§Ã£o conceitual
        // Em uma implementaÃ§Ã£o real, vocÃª extrairia os valores do tensor
        for i in 0..std::cmp::min(seq_len, 8) {
            print!("   Token {}: ", i);
            for j in 0..std::cmp::min(seq_len, 8) {
                // SimulaÃ§Ã£o de padrÃ£o de atenÃ§Ã£o
                let attention_strength = if i == j { 0.8 } else { 0.1 + (i as f32 - j as f32).abs() * 0.1 };
                
                if attention_strength > 0.6 {
                    print!("ğŸ”¥");
                } else if attention_strength > 0.3 {
                    print!("ğŸ”¸");
                } else {
                    print!("âšª");
                }
            }
            println!();
        }
        
        if seq_len > 8 {
            println!("   ... (mostrando apenas primeiros 8 tokens)");
        }
        
        Ok(())
    }

    /// ğŸ“Š **GERAR RELATÃ“RIO FINAL**
    ///
    /// Cria um resumo educacional completo de toda a execuÃ§Ã£o.
    pub fn generate_final_report(&self) {
        println!("\nğŸ“Š ===== RELATÃ“RIO FINAL DO SISTEMA EDUCACIONAL =====");
        println!("ğŸ”¢ Total de operaÃ§Ãµes: {}", self.stats.total_operations);
        println!("â±ï¸ Tempo total: {:.2}ms", self.stats.total_time_ms);
        println!("âš¡ Total de FLOPs: {}", self.format_flops(self.stats.total_flops));
        
        if !self.training_metrics.is_empty() {
            println!("\nğŸ“ˆ === RESUMO DO TREINAMENTO ===");
            let first_loss = self.training_metrics.first().unwrap().avg_loss;
            let last_loss = self.training_metrics.last().unwrap().avg_loss;
            let improvement = ((first_loss - last_loss) / first_loss) * 100.0;
            
            println!("ğŸ¯ Ã‰pocas treinadas: {}", self.training_metrics.len());
            println!("ğŸ“‰ Loss inicial: {:.6}", first_loss);
            println!("ğŸ“ˆ Loss final: {:.6}", last_loss);
            println!("âœ¨ Melhoria: {:.2}%", improvement);
        }
        
        println!("\nğŸ§® === DISTRIBUIÃ‡ÃƒO DE OPERAÃ‡Ã•ES ===");
        for (op, count) in &self.operation_counts {
            println!("   {}: {} vezes", op, count);
        }
        
        println!("\nğŸ“ Obrigado por usar o sistema educacional do Mini-GPT!");
        println!("ğŸ“š Esperamos que tenha aprendido como um LLM funciona internamente!");
    }
 }

/// ğŸ“ **IMPLEMENTAÃ‡ÃƒO DOS MÃ‰TODOS AUXILIARES**
///
/// MÃ©todos para compatibilidade com o cÃ³digo existente.
impl EducationalLogger {
    /// ğŸ—ï¸ **CONSTRUTOR SIMPLES (COMPATIBILIDADE)**
    /// 
    /// Cria uma nova instÃ¢ncia do logger com configuraÃ§Ãµes padrÃ£o otimizadas
    /// para aprendizado e debugging de modelos de linguagem.
    /// 
    /// **ConfiguraÃ§Ãµes PadrÃ£o:**
    /// - `verbose: true` - Mostra explicaÃ§Ãµes detalhadas
    /// - `show_tensors: false` - Oculta detalhes de tensores (pode ser verboso)
    /// - `show_attention: false` - Oculta mapas de atenÃ§Ã£o (computacionalmente caro)
    /// - `max_display_tokens: 20` - Limita exibiÃ§Ã£o para evitar spam no terminal
    /// 
    /// **Analogia:** Como configurar um microscÃ³pio - comeÃ§amos com ampliaÃ§Ã£o
    /// moderada e ajustamos conforme necessÃ¡rio.
    pub fn new_simple() -> Self {
        Self {
            operations: Vec::new(),
            timestamps: HashMap::new(),
            stats: LogStats::default(),
            verbosity: 2,
            current_phase: ModelPhase::Initialization,
            training_metrics: Vec::new(),
            operation_counts: HashMap::new(),
            show_tensors: false,
            show_attention: false,
        }
    }
    
    /// ğŸ”Š **CONFIGURAÃ‡ÃƒO DE VERBOSIDADE**
    /// 
    /// Controla o nÃ­vel de detalhamento das explicaÃ§Ãµes educacionais.
    /// 
    /// **ParÃ¢metros:**
    /// - `0` - Modo silencioso, apenas processamento
    /// - `1` - InformaÃ§Ãµes bÃ¡sicas
    /// - `2` - ExplicaÃ§Ãµes completas, diagramas e analogias (padrÃ£o)
    /// - `3` - Debugging detalhado com tensores
    /// 
    /// **Uso Recomendado:**
    /// - `2` para aprendizado e debugging
    /// - `0` para produÃ§Ã£o ou benchmarks
    /// 
    /// **Analogia:** Como o volume de um professor - alto para aprender,
    /// baixo para nÃ£o atrapalhar outros processos.
    pub fn with_verbosity(mut self, verbosity: u8) -> Self {
        self.verbosity = verbosity;
        self
    }
    
    /// ğŸ”¢ **CONFIGURAÃ‡ÃƒO DE VISUALIZAÃ‡ÃƒO DE TENSORES**
    /// 
    /// Controla se deve exibir valores numÃ©ricos detalhados dos tensores.
    /// 
    /// **ParÃ¢metros:**
    /// - `show_tensors: true` - Mostra valores de embeddings, pesos, etc.
    /// - `show_tensors: false` - Oculta detalhes numÃ©ricos (padrÃ£o)
    /// 
    /// **Cuidado:** Tensores podem ter milhares de valores!
    /// Use apenas para debugging especÃ­fico ou tensores pequenos.
    /// 
    /// **Analogia:** Como ver o cÃ³digo fonte de um programa -
    /// Ãºtil para debugging, mas pode ser overwhelming.
    pub fn with_tensor_info(mut self, show_tensors: bool) -> Self {
        self.show_tensors = show_tensors;
        self
    }
    
    /// ğŸ‘ï¸ **CONFIGURAÃ‡ÃƒO DE MAPAS DE ATENÃ‡ÃƒO**
    /// 
    /// Controla se deve visualizar como tokens "prestam atenÃ§Ã£o" uns aos outros.
    /// 
    /// **ParÃ¢metros:**
    /// - `show_attention: true` - Mostra mapas de atenÃ§Ã£o detalhados
    /// - `show_attention: false` - Oculta visualizaÃ§Ãµes de atenÃ§Ã£o (padrÃ£o)
    /// 
    /// **Performance:** Mapas de atenÃ§Ã£o sÃ£o computacionalmente caros
    /// e podem gerar muito output. Use com moderaÃ§Ã£o.
    /// 
    /// **Analogia:** Como rastrear o movimento dos olhos durante leitura -
    /// fascinante, mas pode distrair do conteÃºdo principal.
    pub fn with_attention_maps(mut self, show_attention: bool) -> Self {
        self.show_attention = show_attention;
        self
    }
    
    /// ğŸ“ **PASSO 1: VISUALIZAÃ‡ÃƒO DA TOKENIZAÃ‡ÃƒO**
    /// 
    /// Mostra como o texto Ã© dividido em tokens e convertido em IDs
    pub fn log_tokenization(&self, text: &str, tokens: &[usize], tokenizer: &BPETokenizer) -> Result<()> {
        if self.verbosity == 0 {
            return Ok(());
        }
        
        println!("\nğŸ“ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!("ğŸ“ PASSO 1: TOKENIZAÃ‡ÃƒO - TEXTO â†’ NÃšMEROS");
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        
        println!("ğŸ“– **TEXTO ORIGINAL:**");
        println!("   \"{}\"", text);
        println!();
        
        println!("ğŸ” **PROCESSO DE TOKENIZAÃ‡ÃƒO:**");
        
        // Mostra a divisÃ£o palavra por palavra
        let words: Vec<&str> = text.split_whitespace().collect();
        println!("   1ï¸âƒ£ DivisÃ£o em palavras: {:?}", words);
        println!();
        
        // Mostra os tokens resultantes
        println!("ğŸ”¢ **TOKENS GERADOS:**");
        println!("   Total de tokens: {}", tokens.len());
        println!();
        
        // Tabela detalhada de tokens
        println!("ğŸ“Š **TABELA DE TOKENS:**");
        println!("   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚ Pos â”‚ Token ID â”‚ Token Text                      â”‚");
        println!("   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        
        let display_limit = 20.min(tokens.len());
        
        for (i, &token_id) in tokens.iter().take(display_limit).enumerate() {
            let token_text = tokenizer.decode(&[token_id])
                .unwrap_or_else(|_| "<ERROR>".to_string());
            
            let display_text = if token_text.is_empty() {
                "<SPECIAL>".to_string()
            } else {
                format!("\"{}\"", token_text)
            };
            
            println!("   â”‚ {:3} â”‚ {:8} â”‚ {:31} â”‚", i, token_id, display_text);
        }
        
        if tokens.len() > display_limit {
            println!("   â”‚ ... â”‚   ...    â”‚ ... ({} tokens mais)            â”‚", 
                    tokens.len() - display_limit);
        }
        
        println!("   â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!();
        
        // EstatÃ­sticas
        println!("ğŸ“ˆ **ESTATÃSTICAS:**");
        println!("   â€¢ Caracteres originais: {}", text.len());
        println!("   â€¢ Tokens gerados: {}", tokens.len());
        println!("   â€¢ Taxa de compressÃ£o: {:.2}x", text.len() as f32 / tokens.len() as f32);
        println!();
        
        Ok(())
    }
    
    /// ğŸ”¢ **PASSO 2: VISUALIZAÃ‡ÃƒO DOS EMBEDDINGS**
    /// 
    /// Mostra como tokens sÃ£o convertidos em vetores densos
    pub fn log_embedding_analysis(&self, tokens: &[usize], token_embeddings: &Tensor, _position_embeddings: &Tensor) -> Result<()> {
        if self.verbosity == 0 {
            return Ok(());
        }
        
        println!("ğŸ“ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!("ğŸ”¢ PASSO 2: EMBEDDINGS - NÃšMEROS â†’ VETORES");
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        
        let shape = token_embeddings.shape();
        let seq_len = shape.dims()[0];
        let emb_dim = shape.dims()[1];
        
        println!("ğŸ“ **DIMENSÃ•ES DOS EMBEDDINGS:**");
        println!("   â€¢ SequÃªncia: {} tokens", seq_len);
        println!("   â€¢ DimensÃ£o: {} features por token", emb_dim);
        println!("   â€¢ Total de parÃ¢metros: {} valores", seq_len * emb_dim);
        println!();
        
        println!("ğŸ¯ **CONCEITO: O QUE SÃƒO EMBEDDINGS?**");
        println!("   Embeddings transformam tokens (nÃºmeros discretos) em vetores");
        println!("   densos que capturam significado semÃ¢ntico e sintÃ¡tico.");
        println!();
        println!("   Exemplo conceitual:");
        println!("   Token 'gato' â†’ [0.2, -0.1, 0.8, 0.3, ...] (vetor de {} dims)", emb_dim);
        println!("   Token 'cÃ£o'  â†’ [0.1, -0.2, 0.7, 0.4, ...] (similar a 'gato')");
        println!("   Token 'casa' â†’ [-0.5, 0.9, 0.1, -0.2, ...] (diferente)");
        println!();
        
        if self.show_tensors {
            println!("ğŸ” **VISUALIZAÃ‡ÃƒO DOS EMBEDDINGS (primeiros 5 tokens):**");
            
            let display_tokens = 5.min(seq_len);
            let display_dims = 8.min(emb_dim);
            
            for i in 0..display_tokens {
                println!("   Token {} (ID: {}):", i, tokens.get(i).unwrap_or(&0));
                print!("     [");
                
                for j in 0..display_dims {
                    // Simula valores de embedding (na prÃ¡tica, viriam do tensor real)
                    let val = (i as f32 * 0.1 + j as f32 * 0.05) * if j % 2 == 0 { 1.0 } else { -1.0 };
                    print!("{:7.3}", val);
                    if j < display_dims - 1 {
                        print!(", ");
                    }
                }
                
                if emb_dim > display_dims {
                    print!(", ... +{} dims", emb_dim - display_dims);
                }
                println!("]\n");
            }
        }
        
        println!("ğŸ“ **POSITION EMBEDDINGS:**");
        println!("   Adicionam informaÃ§Ã£o sobre a POSIÃ‡ÃƒO de cada token na sequÃªncia.");
        println!("   Isso permite ao modelo entender ordem e contexto.");
        println!();
        println!("   PosiÃ§Ã£o 0: [primeira palavra da frase]");
        println!("   PosiÃ§Ã£o 1: [segunda palavra da frase]");
        println!("   PosiÃ§Ã£o N: [N-Ã©sima palavra da frase]");
        println!();
        
        println!("ğŸ§® **EMBEDDING FINAL = TOKEN + POSIÃ‡ÃƒO:**");
        println!("   Cada posiÃ§Ã£o recebe: embedding_token + embedding_posiÃ§Ã£o");
        println!("   Isso cria uma representaÃ§Ã£o Ãºnica que combina:");
        println!("   â€¢ O QUE Ã© a palavra (semÃ¢ntica)");
        println!("   â€¢ ONDE estÃ¡ na frase (sintaxe/ordem)");
        println!();
        
        Ok(())
    }
    
    /// ğŸ§  **PASSO 3: VISUALIZAÃ‡ÃƒO DO PROCESSAMENTO TRANSFORMER**
    /// 
    /// Mostra como os blocos Transformer processam as informaÃ§Ãµes
    pub fn log_transformer_processing(&self, layer: usize, input_shape: &[usize], output_shape: &[usize]) -> Result<()> {
        if self.verbosity == 0 {
            return Ok(());
        }
        
        println!("ğŸ“ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!("ğŸ§  PASSO 3: PROCESSAMENTO TRANSFORMER - CAMADA {}", layer);
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        
        println!("ğŸ—ï¸ **ARQUITETURA DO BLOCO TRANSFORMER:**");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚              INPUT EMBEDDINGS               â”‚");
        println!("   â”‚         Shape: {:?}                    â”‚", input_shape);
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚           LAYER NORMALIZATION               â”‚");
        println!("   â”‚        (estabiliza ativaÃ§Ãµes)               â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚         MULTI-HEAD ATTENTION                â”‚");
        println!("   â”‚    (tokens 'conversam' entre si)            â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚          RESIDUAL CONNECTION                â”‚");
        println!("   â”‚         (input + attention)                 â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚           LAYER NORMALIZATION               â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚          FEED-FORWARD NETWORK               â”‚");
        println!("   â”‚      (processamento individual)             â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚          RESIDUAL CONNECTION                â”‚");
        println!("   â”‚         (input + feedforward)               â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚             OUTPUT EMBEDDINGS               â”‚");
        println!("   â”‚         Shape: {:?}                    â”‚", output_shape);
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!();
        
        println!("ğŸ¯ **O QUE ACONTECE NESTA CAMADA:**");
        println!("   1. ğŸ‘ï¸ **AtenÃ§Ã£o**: Cada token 'olha' para outros tokens");
        println!("      e decide quais sÃ£o importantes para seu contexto");
        println!();
        println!("   2. ğŸ½ï¸ **Feed-Forward**: Cada token Ã© processado");
        println!("      individualmente atravÃ©s de uma rede neural");
        println!();
        println!("   3. ğŸ”— **ConexÃµes Residuais**: Preservam informaÃ§Ã£o");
        println!("      original para evitar perda durante processamento");
        println!();
        
        Ok(())
    }
    
    /// ğŸ¯ **PASSO 4: VISUALIZAÃ‡ÃƒO DA PREDIÃ‡ÃƒO**
    /// 
    /// Mostra como o modelo escolhe a prÃ³xima palavra
    pub fn log_prediction(&self, logits: &Tensor, predicted_token: usize, tokenizer: &BPETokenizer, top_k: usize) -> Result<()> {
        if self.verbosity == 0 {
            return Ok(());
        }
        
        println!("ğŸ“ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!("ğŸ¯ PASSO 4: PREDIÃ‡ÃƒO - ESCOLHENDO A PRÃ“XIMA PALAVRA");
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        
        let vocab_size = logits.shape().dims()[logits.shape().dims().len() - 1];
        
        println!("ğŸ§® **LOGITS (PONTUAÃ‡Ã•ES BRUTAS):**");
        println!("   O modelo produz uma pontuaÃ§Ã£o para cada palavra do vocabulÃ¡rio");
        println!("   VocabulÃ¡rio total: {} palavras possÃ­veis", vocab_size);
        println!();
        
        // Converte logits para probabilidades (softmax)
        let _probs = candle_nn::ops::softmax(&logits, candle_core::D::Minus1)?;
        
        println!("ğŸ“Š **TOP {} CANDIDATOS:**", top_k);
        println!("   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚ Pos â”‚ Token ID â”‚ Probabilidade â”‚ Token Text          â”‚");
        println!("   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        
        // Pega os top-k tokens (simulaÃ§Ã£o - em implementaÃ§Ã£o real seria mais complexo)
        for i in 0..top_k.min(10) {
            let token_id = (predicted_token + i) % vocab_size; // SimulaÃ§Ã£o simples
            let prob = if i == 0 { 0.45 } else { 0.55 / (top_k - 1) as f32 }; // SimulaÃ§Ã£o
            
            let token_text = tokenizer.decode(&[token_id])
                .unwrap_or_else(|_| "<ERROR>".to_string());
            
            let display_text = if token_text.is_empty() {
                "<SPECIAL>".to_string()
            } else {
                format!("\"{}\"", token_text)
            };
            
            let marker = if i == 0 { "ğŸ‘‘" } else { "  " };
            
            println!("   â”‚{} {:2} â”‚ {:8} â”‚ {:10.1}% â”‚ {:19} â”‚", 
                    marker, i + 1, token_id, prob * 100.0, display_text);
        }
        
        println!("   â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!();
        
        let predicted_text = tokenizer.decode(&[predicted_token])
            .unwrap_or_else(|_| "<ERROR>".to_string());
        
        println!("ğŸ† **PALAVRA ESCOLHIDA:**");
        println!("   Token ID: (demonstrativo)");
        println!("   Texto: \"{}\"", predicted_text);
        println!();
        
        println!("ğŸ² **PROCESSO DE SELEÃ‡ÃƒO:**");
        println!("   1. ğŸ§® Modelo calcula pontuaÃ§Ã£o para cada palavra");
        println!("   2. ğŸ“Š Softmax converte pontuaÃ§Ãµes em probabilidades");
        println!("   3. ğŸ¯ Amostragem escolhe palavra baseada nas probabilidades");
        println!("   4. ğŸ”„ Processo se repete para prÃ³xima palavra");
        println!();
        
        Ok(())
    }
    
    /// ğŸ“‹ **RESUMO COMPLETO DO PROCESSO**
    /// 
    /// Mostra um resumo de todo o pipeline de processamento
    pub fn log_process_summary(&self, input_text: &str, output_text: &str, total_tokens: usize, processing_time: f32) -> Result<()> {
        if self.verbosity == 0 {
            return Ok(());
        }
        
        println!("ğŸ“ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!("ğŸ“‹ RESUMO COMPLETO DO PROCESSO LLM");
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        
        println!("ğŸ“ **ENTRADA:**");
        println!("   \"{}\"", input_text);
        println!();
        
        println!("ğŸ”„ **PIPELINE DE PROCESSAMENTO:**");
        println!("   1ï¸âƒ£ TokenizaÃ§Ã£o    â†’ {} tokens", total_tokens);
        println!("   2ï¸âƒ£ Embeddings     â†’ Vetores de {} dimensÃµes", "512"); // Placeholder
        println!("   3ï¸âƒ£ Transformer    â†’ {} camadas de processamento", "6"); // Placeholder
        println!("   4ï¸âƒ£ PrediÃ§Ã£o      â†’ Probabilidades sobre vocabulÃ¡rio");
        println!("   5ï¸âƒ£ DecodificaÃ§Ã£o â†’ Texto final");
        println!();
        
        println!("ğŸ¯ **SAÃDA:**");
        println!("   \"{}\"", output_text);
        println!();
        
        println!("â±ï¸ **PERFORMANCE:**");
        println!("   Tempo total: {:.2}ms", processing_time * 1000.0);
        println!("   Tokens/segundo: {:.0}", total_tokens as f32 / processing_time);
        println!();
        
        println!("ğŸ‰ **PROCESSO CONCLUÃDO COM SUCESSO!**");
        println!("   O modelo transformou texto de entrada em texto de saÃ­da");
        println!("   atravÃ©s de representaÃ§Ãµes numÃ©ricas e processamento neural.");
        println!();
        
        Ok(())
    }
}

/// ğŸ¯ **IMPLEMENTAÃ‡ÃƒO DO TRAIT DEFAULT**
/// 
/// Fornece uma instÃ¢ncia padrÃ£o do EducationalLogger usando as mesmas
/// configuraÃ§Ãµes do construtor `new()`.
/// 
/// **Uso:** Permite criar o logger usando `EducationalLogger::default()`
/// ou em contextos que requerem o trait Default (como structs derivadas).
/// 
/// **EquivalÃªncia:** `EducationalLogger::default()` == `EducationalLogger::new(2)`
/// 
/// **Analogia:** Como ter um "modo automÃ¡tico" em uma cÃ¢mera -
/// configuraÃ§Ãµes sensatas para a maioria dos casos de uso.
impl Default for EducationalLogger {
    fn default() -> Self {
        Self::new(2)
    }
}