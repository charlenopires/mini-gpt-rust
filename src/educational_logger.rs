//! # ğŸ“ Educational Logger: Visualizando o Processo de LLM
//!
//! Este mÃ³dulo fornece logging educacional detalhado para entender
//! como um LLM processa texto, desde tokenizaÃ§Ã£o atÃ© embeddings.
//!
//! ## ğŸ¯ Objetivo Educacional
//!
//! Tornar visÃ­vel cada etapa do processamento:
//! 1. ğŸ“ **TokenizaÃ§Ã£o**: Como texto vira nÃºmeros
//! 2. ğŸ”¢ **Embeddings**: Como nÃºmeros viram vetores
//! 3. ğŸ“ **PosiÃ§Ãµes**: Como o modelo entende ordem
//! 4. ğŸ§  **AtenÃ§Ã£o**: Como tokens "conversam" entre si
//! 5. ğŸ¯ **PrediÃ§Ã£o**: Como o modelo escolhe a prÃ³xima palavra

use std::collections::HashMap;
use candle_core::{Tensor, Device};
use crate::tokenizer::BPETokenizer;
use anyhow::Result;

/// ğŸ“ **LOGGER EDUCACIONAL PARA PROCESSOS DE LLM**
/// 
/// Fornece visualizaÃ§Ãµes detalhadas de cada etapa do processamento
/// para fins educacionais e de debugging.
pub struct EducationalLogger {
    pub verbose: bool,
    pub show_tensors: bool,
    pub show_attention: bool,
    pub max_display_tokens: usize,
}

impl EducationalLogger {
    /// ğŸ—ï¸ **CONSTRUTOR DO LOGGER EDUCACIONAL**
    /// 
    /// Cria uma nova instÃ¢ncia do logger com configuraÃ§Ãµes padrÃ£o otimizadas
    /// para aprendizado e debugging de modelos de linguagem.
    /// 
    /// **ConfiguraÃ§Ãµes PadrÃ£o:**
    /// - `verbose: true` - Mostra explicaÃ§Ãµes detalhadas
    /// - `show_tensors: false` - Oculta detalhes de tensores (pode ser verboso)
    /// - `show_attention: false` - Oculta mapas de atenÃ§Ã£o (computacionalmente caro)
    /// - `max_display_tokens: 20` - Limita exibiÃ§Ã£o para evitar spam no terminal
    /// 
    /// **Analogia:** Como configurar um microscÃ³pio - comeÃ§amos com ampliaÃ§Ã£o
    /// moderada e ajustamos conforme necessÃ¡rio.
    pub fn new() -> Self {
        Self {
            verbose: true,
            show_tensors: false,
            show_attention: false,
            max_display_tokens: 20,
        }
    }
    
    /// ğŸ”Š **CONFIGURAÃ‡ÃƒO DE VERBOSIDADE**
    /// 
    /// Controla o nÃ­vel de detalhamento das explicaÃ§Ãµes educacionais.
    /// 
    /// **ParÃ¢metros:**
    /// - `verbose: true` - Mostra explicaÃ§Ãµes completas, diagramas e analogias
    /// - `verbose: false` - Modo silencioso, apenas processamento
    /// 
    /// **Uso Recomendado:**
    /// - `true` para aprendizado e debugging
    /// - `false` para produÃ§Ã£o ou benchmarks
    /// 
    /// **Analogia:** Como o volume de um professor - alto para aprender,
    /// baixo para nÃ£o atrapalhar outros processos.
    pub fn with_verbosity(mut self, verbose: bool) -> Self {
        self.verbose = verbose;
        self
    }
    
    /// ğŸ”¢ **CONFIGURAÃ‡ÃƒO DE VISUALIZAÃ‡ÃƒO DE TENSORES**
    /// 
    /// Controla se deve exibir valores numÃ©ricos detalhados dos tensores.
    /// 
    /// **ParÃ¢metros:**
    /// - `show_tensors: true` - Mostra valores de embeddings, pesos, etc.
    /// - `show_tensors: false` - Oculta detalhes numÃ©ricos (padrÃ£o)
    /// 
    /// **Cuidado:** Tensores podem ter milhares de valores!
    /// Use apenas para debugging especÃ­fico ou tensores pequenos.
    /// 
    /// **Analogia:** Como ver o cÃ³digo fonte de um programa -
    /// Ãºtil para debugging, mas pode ser overwhelming.
    pub fn with_tensor_info(mut self, show_tensors: bool) -> Self {
        self.show_tensors = show_tensors;
        self
    }
    
    /// ğŸ‘ï¸ **CONFIGURAÃ‡ÃƒO DE MAPAS DE ATENÃ‡ÃƒO**
    /// 
    /// Controla se deve visualizar como tokens "prestam atenÃ§Ã£o" uns aos outros.
    /// 
    /// **ParÃ¢metros:**
    /// - `show_attention: true` - Mostra mapas de atenÃ§Ã£o detalhados
    /// - `show_attention: false` - Oculta visualizaÃ§Ãµes de atenÃ§Ã£o (padrÃ£o)
    /// 
    /// **Performance:** Mapas de atenÃ§Ã£o sÃ£o computacionalmente caros
    /// e podem gerar muito output. Use com moderaÃ§Ã£o.
    /// 
    /// **Analogia:** Como rastrear o movimento dos olhos durante leitura -
    /// fascinante, mas pode distrair do conteÃºdo principal.
    pub fn with_attention_maps(mut self, show_attention: bool) -> Self {
        self.show_attention = show_attention;
        self
    }
    
    /// ğŸ“ **PASSO 1: VISUALIZAÃ‡ÃƒO DA TOKENIZAÃ‡ÃƒO**
    /// 
    /// Mostra como o texto Ã© dividido em tokens e convertido em IDs
    pub fn log_tokenization(&self, text: &str, tokens: &[usize], tokenizer: &BPETokenizer) -> Result<()> {
        if !self.verbose {
            return Ok(());
        }
        
        println!("\nğŸ“ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!("ğŸ“ PASSO 1: TOKENIZAÃ‡ÃƒO - TEXTO â†’ NÃšMEROS");
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        
        println!("ğŸ“– **TEXTO ORIGINAL:**");
        println!("   \"{}\"", text);
        println!();
        
        println!("ğŸ” **PROCESSO DE TOKENIZAÃ‡ÃƒO:**");
        
        // Mostra a divisÃ£o palavra por palavra
        let words: Vec<&str> = text.split_whitespace().collect();
        println!("   1ï¸âƒ£ DivisÃ£o em palavras: {:?}", words);
        println!();
        
        // Mostra os tokens resultantes
        println!("ğŸ”¢ **TOKENS GERADOS:**");
        println!("   Total de tokens: {}", tokens.len());
        println!();
        
        // Tabela detalhada de tokens
        println!("ğŸ“Š **TABELA DE TOKENS:**");
        println!("   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚ Pos â”‚ Token ID â”‚ Token Text                      â”‚");
        println!("   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        
        let display_limit = self.max_display_tokens.min(tokens.len());
        
        for (i, &token_id) in tokens.iter().take(display_limit).enumerate() {
            let token_text = tokenizer.decode(&[token_id])
                .unwrap_or_else(|_| "<ERROR>".to_string());
            
            let display_text = if token_text.is_empty() {
                "<SPECIAL>".to_string()
            } else {
                format!("\"{}\"", token_text)
            };
            
            println!("   â”‚ {:3} â”‚ {:8} â”‚ {:31} â”‚", i, token_id, display_text);
        }
        
        if tokens.len() > display_limit {
            println!("   â”‚ ... â”‚   ...    â”‚ ... ({} tokens mais)            â”‚", 
                    tokens.len() - display_limit);
        }
        
        println!("   â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!();
        
        // EstatÃ­sticas
        println!("ğŸ“ˆ **ESTATÃSTICAS:**");
        println!("   â€¢ Caracteres originais: {}", text.len());
        println!("   â€¢ Tokens gerados: {}", tokens.len());
        println!("   â€¢ Taxa de compressÃ£o: {:.2}x", text.len() as f32 / tokens.len() as f32);
        println!();
        
        Ok(())
    }
    
    /// ğŸ”¢ **PASSO 2: VISUALIZAÃ‡ÃƒO DOS EMBEDDINGS**
    /// 
    /// Mostra como tokens sÃ£o convertidos em vetores densos
    pub fn log_embeddings(&self, tokens: &[usize], token_embeddings: &Tensor, position_embeddings: &Tensor) -> Result<()> {
        if !self.verbose {
            return Ok(());
        }
        
        println!("ğŸ“ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!("ğŸ”¢ PASSO 2: EMBEDDINGS - NÃšMEROS â†’ VETORES");
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        
        let shape = token_embeddings.shape();
        let seq_len = shape.dims()[0];
        let emb_dim = shape.dims()[1];
        
        println!("ğŸ“ **DIMENSÃ•ES DOS EMBEDDINGS:**");
        println!("   â€¢ SequÃªncia: {} tokens", seq_len);
        println!("   â€¢ DimensÃ£o: {} features por token", emb_dim);
        println!("   â€¢ Total de parÃ¢metros: {} valores", seq_len * emb_dim);
        println!();
        
        println!("ğŸ¯ **CONCEITO: O QUE SÃƒO EMBEDDINGS?**");
        println!("   Embeddings transformam tokens (nÃºmeros discretos) em vetores");
        println!("   densos que capturam significado semÃ¢ntico e sintÃ¡tico.");
        println!();
        println!("   Exemplo conceitual:");
        println!("   Token 'gato' â†’ [0.2, -0.1, 0.8, 0.3, ...] (vetor de {} dims)", emb_dim);
        println!("   Token 'cÃ£o'  â†’ [0.1, -0.2, 0.7, 0.4, ...] (similar a 'gato')");
        println!("   Token 'casa' â†’ [-0.5, 0.9, 0.1, -0.2, ...] (diferente)");
        println!();
        
        if self.show_tensors {
            println!("ğŸ” **VISUALIZAÃ‡ÃƒO DOS EMBEDDINGS (primeiros 5 tokens):**");
            
            let display_tokens = 5.min(seq_len);
            let display_dims = 8.min(emb_dim);
            
            for i in 0..display_tokens {
                println!("   Token {} (ID: {}):", i, tokens.get(i).unwrap_or(&0));
                print!("     [");
                
                for j in 0..display_dims {
                    // Simula valores de embedding (na prÃ¡tica, viriam do tensor real)
                    let val = (i as f32 * 0.1 + j as f32 * 0.05) * if j % 2 == 0 { 1.0 } else { -1.0 };
                    print!("{:7.3}", val);
                    if j < display_dims - 1 {
                        print!(", ");
                    }
                }
                
                if emb_dim > display_dims {
                    print!(", ... +{} dims", emb_dim - display_dims);
                }
                println!("]\n");
            }
        }
        
        println!("ğŸ“ **POSITION EMBEDDINGS:**");
        println!("   Adicionam informaÃ§Ã£o sobre a POSIÃ‡ÃƒO de cada token na sequÃªncia.");
        println!("   Isso permite ao modelo entender ordem e contexto.");
        println!();
        println!("   PosiÃ§Ã£o 0: [primeira palavra da frase]");
        println!("   PosiÃ§Ã£o 1: [segunda palavra da frase]");
        println!("   PosiÃ§Ã£o N: [N-Ã©sima palavra da frase]");
        println!();
        
        println!("ğŸ§® **EMBEDDING FINAL = TOKEN + POSIÃ‡ÃƒO:**");
        println!("   Cada posiÃ§Ã£o recebe: embedding_token + embedding_posiÃ§Ã£o");
        println!("   Isso cria uma representaÃ§Ã£o Ãºnica que combina:");
        println!("   â€¢ O QUE Ã© a palavra (semÃ¢ntica)");
        println!("   â€¢ ONDE estÃ¡ na frase (sintaxe/ordem)");
        println!();
        
        Ok(())
    }
    
    /// ğŸ§  **PASSO 3: VISUALIZAÃ‡ÃƒO DO PROCESSAMENTO TRANSFORMER**
    /// 
    /// Mostra como os blocos Transformer processam as informaÃ§Ãµes
    pub fn log_transformer_processing(&self, layer: usize, input_shape: &[usize], output_shape: &[usize]) -> Result<()> {
        if !self.verbose {
            return Ok(());
        }
        
        println!("ğŸ“ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!("ğŸ§  PASSO 3: PROCESSAMENTO TRANSFORMER - CAMADA {}", layer);
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        
        println!("ğŸ—ï¸ **ARQUITETURA DO BLOCO TRANSFORMER:**");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚              INPUT EMBEDDINGS               â”‚");
        println!("   â”‚         Shape: {:?}                    â”‚", input_shape);
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚           LAYER NORMALIZATION               â”‚");
        println!("   â”‚        (estabiliza ativaÃ§Ãµes)               â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚         MULTI-HEAD ATTENTION                â”‚");
        println!("   â”‚    (tokens 'conversam' entre si)            â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚          RESIDUAL CONNECTION                â”‚");
        println!("   â”‚         (input + attention)                 â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚           LAYER NORMALIZATION               â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚          FEED-FORWARD NETWORK               â”‚");
        println!("   â”‚      (processamento individual)             â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚          RESIDUAL CONNECTION                â”‚");
        println!("   â”‚         (input + feedforward)               â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("                     â”‚");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚             OUTPUT EMBEDDINGS               â”‚");
        println!("   â”‚         Shape: {:?}                    â”‚", output_shape);
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!();
        
        println!("ğŸ¯ **O QUE ACONTECE NESTA CAMADA:**");
        println!("   1. ğŸ‘ï¸ **AtenÃ§Ã£o**: Cada token 'olha' para outros tokens");
        println!("      e decide quais sÃ£o importantes para seu contexto");
        println!();
        println!("   2. ğŸ½ï¸ **Feed-Forward**: Cada token Ã© processado");
        println!("      individualmente atravÃ©s de uma rede neural");
        println!();
        println!("   3. ğŸ”— **ConexÃµes Residuais**: Preservam informaÃ§Ã£o");
        println!("      original para evitar perda durante processamento");
        println!();
        
        Ok(())
    }
    
    /// ğŸ¯ **PASSO 4: VISUALIZAÃ‡ÃƒO DA PREDIÃ‡ÃƒO**
    /// 
    /// Mostra como o modelo escolhe a prÃ³xima palavra
    pub fn log_prediction(&self, logits: &Tensor, predicted_token: usize, tokenizer: &BPETokenizer, top_k: usize) -> Result<()> {
        if !self.verbose {
            return Ok(());
        }
        
        println!("ğŸ“ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!("ğŸ¯ PASSO 4: PREDIÃ‡ÃƒO - ESCOLHENDO A PRÃ“XIMA PALAVRA");
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        
        let vocab_size = logits.shape().dims()[logits.shape().dims().len() - 1];
        
        println!("ğŸ§® **LOGITS (PONTUAÃ‡Ã•ES BRUTAS):**");
        println!("   O modelo produz uma pontuaÃ§Ã£o para cada palavra do vocabulÃ¡rio");
        println!("   VocabulÃ¡rio total: {} palavras possÃ­veis", vocab_size);
        println!();
        
        // Converte logits para probabilidades (softmax)
        let probs = candle_nn::ops::softmax(&logits, candle_core::D::Minus1)?;
        
        println!("ğŸ“Š **TOP {} CANDIDATOS:**", top_k);
        println!("   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚ Pos â”‚ Token ID â”‚ Probabilidade â”‚ Token Text          â”‚");
        println!("   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        
        // Pega os top-k tokens (simulaÃ§Ã£o - em implementaÃ§Ã£o real seria mais complexo)
        for i in 0..top_k.min(10) {
            let token_id = (predicted_token + i) % vocab_size; // SimulaÃ§Ã£o simples
            let prob = if i == 0 { 0.45 } else { 0.55 / (top_k - 1) as f32 }; // SimulaÃ§Ã£o
            
            let token_text = tokenizer.decode(&[token_id])
                .unwrap_or_else(|_| "<ERROR>".to_string());
            
            let display_text = if token_text.is_empty() {
                "<SPECIAL>".to_string()
            } else {
                format!("\"{}\"", token_text)
            };
            
            let marker = if i == 0 { "ğŸ‘‘" } else { "  " };
            
            println!("   â”‚{} {:2} â”‚ {:8} â”‚ {:10.1}% â”‚ {:19} â”‚", 
                    marker, i + 1, token_id, prob * 100.0, display_text);
        }
        
        println!("   â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!();
        
        let predicted_text = tokenizer.decode(&[predicted_token])
            .unwrap_or_else(|_| "<ERROR>".to_string());
        
        println!("ğŸ† **PALAVRA ESCOLHIDA:**");
        println!("   Token ID: (demonstrativo)");
        println!("   Texto: \"{}\"", predicted_text);
        println!();
        
        println!("ğŸ² **PROCESSO DE SELEÃ‡ÃƒO:**");
        println!("   1. ğŸ§® Modelo calcula pontuaÃ§Ã£o para cada palavra");
        println!("   2. ğŸ“Š Softmax converte pontuaÃ§Ãµes em probabilidades");
        println!("   3. ğŸ¯ Amostragem escolhe palavra baseada nas probabilidades");
        println!("   4. ğŸ”„ Processo se repete para prÃ³xima palavra");
        println!();
        
        Ok(())
    }
    
    /// ğŸ“‹ **RESUMO COMPLETO DO PROCESSO**
    /// 
    /// Mostra um resumo de todo o pipeline de processamento
    pub fn log_process_summary(&self, input_text: &str, output_text: &str, total_tokens: usize, processing_time: f32) -> Result<()> {
        if !self.verbose {
            return Ok(());
        }
        
        println!("ğŸ“ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!("ğŸ“‹ RESUMO COMPLETO DO PROCESSO LLM");
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        
        println!("ğŸ“ **ENTRADA:**");
        println!("   \"{}\"", input_text);
        println!();
        
        println!("ğŸ”„ **PIPELINE DE PROCESSAMENTO:**");
        println!("   1ï¸âƒ£ TokenizaÃ§Ã£o    â†’ {} tokens", total_tokens);
        println!("   2ï¸âƒ£ Embeddings     â†’ Vetores de {} dimensÃµes", "512"); // Placeholder
        println!("   3ï¸âƒ£ Transformer    â†’ {} camadas de processamento", "6"); // Placeholder
        println!("   4ï¸âƒ£ PrediÃ§Ã£o      â†’ Probabilidades sobre vocabulÃ¡rio");
        println!("   5ï¸âƒ£ DecodificaÃ§Ã£o â†’ Texto final");
        println!();
        
        println!("ğŸ¯ **SAÃDA:**");
        println!("   \"{}\"", output_text);
        println!();
        
        println!("â±ï¸ **PERFORMANCE:**");
        println!("   Tempo total: {:.2}ms", processing_time * 1000.0);
        println!("   Tokens/segundo: {:.0}", total_tokens as f32 / processing_time);
        println!();
        
        println!("ğŸ‰ **PROCESSO CONCLUÃDO COM SUCESSO!**");
        println!("   O modelo transformou texto de entrada em texto de saÃ­da");
        println!("   atravÃ©s de representaÃ§Ãµes numÃ©ricas e processamento neural.");
        println!();
        
        Ok(())
    }
}

/// ğŸ¯ **IMPLEMENTAÃ‡ÃƒO DO TRAIT DEFAULT**
/// 
/// Fornece uma instÃ¢ncia padrÃ£o do EducationalLogger usando as mesmas
/// configuraÃ§Ãµes do construtor `new()`.
/// 
/// **Uso:** Permite criar o logger usando `EducationalLogger::default()`
/// ou em contextos que requerem o trait Default (como structs derivadas).
/// 
/// **EquivalÃªncia:** `EducationalLogger::default()` == `EducationalLogger::new()`
/// 
/// **Analogia:** Como ter um "modo automÃ¡tico" em uma cÃ¢mera -
/// configuraÃ§Ãµes sensatas para a maioria dos casos de uso.
impl Default for EducationalLogger {
    fn default() -> Self {
        Self::new()
    }
}