//! # Mini-GPT: Um Large Language Model (LLM) Educacional em Rust
//! 
//! ## ğŸ§  O que Ã© um Large Language Model?
//! 
//! Imagine que estamos construindo um "cÃ©rebro artificial" que aprende a escrever em portuguÃªs.
//! Como uma crianÃ§a aprendendo a falar, nosso modelo vai observar padrÃµes no texto e tentar
//! reproduzi-los. Mas como exatamente isso funciona?
//! 
//! ### ğŸ“š Processo de Aprendizado (similar ao cÃ©rebro humano):
//! 1. **TokenizaÃ§Ã£o**: Quebra o texto em pedaÃ§os menores (tokens) - como sÃ­labas para uma crianÃ§a
//! 2. **Embeddings**: Converte palavras em nÃºmeros que o computador entende - cada palavra vira um vetor
//! 3. **AtenÃ§Ã£o**: O modelo aprende quais palavras sÃ£o importantes para o contexto - como focar na conversa
//! 4. **Transformers**: Arquitetura que processa sequÃªncias de texto de forma paralela e eficiente
//! 5. **Treinamento**: Ajusta milhÃµes de parÃ¢metros para prever a prÃ³xima palavra corretamente
//! 
//! ### ğŸ”¬ Conceitos Fundamentais:
//! - **Tokens**: Unidades bÃ¡sicas de texto (palavras, subpalavras ou caracteres)
//! - **Embeddings**: RepresentaÃ§Ãµes vetoriais densas que capturam significado semÃ¢ntico
//! - **AtenÃ§Ã£o**: Mecanismo que permite ao modelo focar em partes relevantes da entrada
//! - **Backpropagation**: Algoritmo que ajusta pesos da rede neural baseado nos erros
//! - **Gradient Descent**: MÃ©todo de otimizaÃ§Ã£o que minimiza a funÃ§Ã£o de perda iterativamente

use anyhow::Result;
use clap::{Parser, Subcommand};
use std::path::PathBuf;

mod tokenizer;
mod attention;
mod transformer;
mod model;
mod training;
mod educational_logger;
mod kernels;

use model::{MiniGPT, CheckpointMetadata};
use training::Trainer;
use educational_logger::EducationalLogger;
use kernels::{FusionBenchmark, FusionConfig};

/// ğŸ–¥ï¸ **INTERFACE DE LINHA DE COMANDO (CLI)**
/// 
/// Define a estrutura principal da aplicaÃ§Ã£o usando o crate `clap`
/// para parsing automÃ¡tico de argumentos e geraÃ§Ã£o de help.
/// 
/// ## ğŸ¯ Funcionalidades DisponÃ­veis:
/// 1. **Train**: Treina o modelo do zero com dados fornecidos
/// 2. **Generate**: Gera texto a partir de um prompt
/// 3. **Chat**: Modo interativo de conversaÃ§Ã£o
/// 
/// ## ğŸ“– Exemplo de Uso:
/// ```bash
/// # Treinar o modelo
/// cargo run -- train --data corpus.txt --epochs 50
/// 
/// # Gerar texto
/// cargo run -- generate --prompt "Era uma vez" --max-tokens 100
/// 
/// # Modo chat interativo
/// cargo run -- chat
/// ```
#[derive(Parser)]
#[command(name = "mini-gpt")]
#[command(about = "Mini-GPT: LLM Educacional em Rust ğŸ¦€ğŸ§ ", long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Commands,  // ğŸ¯ Comando principal a ser executado
}

/// ğŸ® **COMANDOS DISPONÃVEIS**
/// 
/// Enumera todas as operaÃ§Ãµes possÃ­veis que o usuÃ¡rio pode realizar
/// com o Mini-GPT, cada uma com seus prÃ³prios parÃ¢metros especÃ­ficos.
#[derive(Subcommand)]
enum Commands {
    /// ğŸ“ **TREINAMENTO: Ensinar o modelo a escrever**
    /// 
    /// Treina o modelo do zero usando um corpus de texto em portuguÃªs.
    /// O processo envolve:
    /// 1. Carregar e tokenizar o dataset
    /// 2. Dividir em batches para processamento eficiente
    /// 3. Executar forward/backward passes
    /// 4. Otimizar parÃ¢metros usando Adam
    /// 5. Salvar checkpoints periodicamente
    Train {
        /// ğŸ“ Caminho para o arquivo de dados de treinamento
        /// Deve conter texto em portuguÃªs, preferencialmente limpo e bem formatado
        #[arg(short, long, default_value = "data/corpus_pt_br.txt")]
        data: PathBuf,
        
        /// ğŸ”„ NÃºmero de Ã©pocas de treinamento
        /// Uma Ã©poca = uma passada completa pelo dataset
        /// Mais Ã©pocas = melhor aprendizado, mas risco de overfitting
        #[arg(short, long, default_value = "100")]
        epochs: usize,
    },
    
    /// ğŸ¨ **GERAÃ‡ÃƒO: Criar texto criativo**
    /// 
    /// Gera texto a partir de um prompt inicial usando o modelo treinado.
    /// O processo utiliza:
    /// 1. TokenizaÃ§Ã£o do prompt de entrada
    /// 2. GeraÃ§Ã£o autoregressiva (uma palavra por vez)
    /// 3. Sampling com temperatura para controlar criatividade
    /// 4. DecodificaÃ§Ã£o de volta para texto legÃ­vel
    Generate {
        /// ğŸ’­ Prompt inicial para geraÃ§Ã£o de texto
        /// Exemplo: "Era uma vez", "O futuro da tecnologia", etc.
        #[arg(short, long)]
        prompt: String,
        
        /// ğŸ¯ NÃºmero mÃ¡ximo de tokens a gerar
        /// Controla o comprimento do texto gerado
        /// 1 token â‰ˆ 0.75 palavras em portuguÃªs
        #[arg(short, long, default_value = "100")]
        max_tokens: usize,
        
        /// ğŸ“š Ativa logs educacionais detalhados
        #[arg(long, help = "Ativa logs educacionais detalhados")]
        educational: bool,
        
        /// ğŸ” Mostra informaÃ§Ãµes de tensores
        #[arg(long, help = "Mostra informaÃ§Ãµes de tensores")]
        show_tensors: bool,
    },
    
    /// ğŸ’¬ **CHAT: ConversaÃ§Ã£o interativa**
    /// 
    /// Modo interativo onde vocÃª pode conversar com o modelo
    /// em tempo real, simulando um chatbot inteligente.
    /// 
    /// ## ğŸ® Como usar:
    /// - Digite suas mensagens e pressione Enter
    /// - Digite 'quit' ou 'exit' para sair
    /// - O modelo mantÃ©m contexto da conversa
    Chat {
        /// ğŸ“š Ativa logs educacionais detalhados
        #[arg(long, help = "Ativa logs educacionais detalhados")]
        educational: bool,
        
        /// ğŸ” Mostra informaÃ§Ãµes de tensores
        #[arg(long, help = "Mostra informaÃ§Ãµes de tensores")]
        show_tensors: bool,
    },
    
    /// ğŸ“‚ **LOAD: Carregar modelo de checkpoint**
    /// 
    /// Carrega um modelo previamente treinado de um arquivo SafeTensors
    /// e permite gerar texto ou iniciar chat com o modelo carregado.
    /// 
    /// ## ğŸ¯ **Modos de Carregamento:**
    /// 1. **Direto**: Especifica caminho exato do checkpoint
    /// 2. **Interativo**: Lista checkpoints disponÃ­veis para seleÃ§Ã£o
    /// 3. **AutomÃ¡tico**: Carrega o melhor checkpoint (menor loss)
    /// 4. **Por Nome**: Busca checkpoint por nome/padrÃ£o
    /// 
    /// ## ğŸ“Š **Filtros DisponÃ­veis:**
    /// - Por data de criaÃ§Ã£o (mais recente/antigo)
    /// - Por performance (menor/maior loss)
    /// - Por step de treinamento
    /// - Por descriÃ§Ã£o/tags
    Load {
        /// ğŸ“ Caminho para o arquivo de checkpoint (.safetensors)
        /// Se nÃ£o especificado, entra em modo interativo
        #[arg(short, long)]
        checkpoint: Option<PathBuf>,
        
        /// ğŸ“‚ DiretÃ³rio para buscar checkpoints (modo interativo)
        #[arg(short, long, default_value = "models")]
        dir: PathBuf,
        
        /// ğŸ¯ Carrega automaticamente o melhor checkpoint (menor loss)
        #[arg(long, help = "Carrega automaticamente o checkpoint com menor loss")]
        best: bool,
        
        /// ğŸ“… Carrega o checkpoint mais recente
        #[arg(long, help = "Carrega o checkpoint mais recente por timestamp")]
        latest: bool,
        
        /// ğŸ” Busca checkpoint por nome/padrÃ£o
        #[arg(long, help = "Busca checkpoint que contenha este padrÃ£o no nome")]
        name_pattern: Option<String>,
        
        /// ğŸ“Š Filtra por loss mÃ¡ximo
        #[arg(long, help = "Carrega apenas checkpoints com loss menor que este valor")]
        max_loss: Option<f32>,
        
        /// ğŸ”¢ Filtra por step mÃ­nimo de treinamento
        #[arg(long, help = "Carrega apenas checkpoints com step maior que este valor")]
        min_step: Option<usize>,
        
        /// ğŸ’­ Prompt para geraÃ§Ã£o (opcional)
        #[arg(short, long)]
        prompt: Option<String>,
        
        /// ğŸ¯ NÃºmero mÃ¡ximo de tokens a gerar
        #[arg(short, long, default_value = "100")]
        max_tokens: usize,
        
        /// ğŸ’¬ Inicia modo chat apÃ³s carregar
        #[arg(long, help = "Inicia modo chat interativo")]
        chat: bool,
        
        /// ğŸ“š Ativa logs educacionais detalhados
        #[arg(long, help = "Ativa logs educacionais detalhados")]
        educational: bool,
        
        /// ğŸ” Mostra informaÃ§Ãµes detalhadas do checkpoint antes de carregar
        #[arg(long, help = "Exibe metadados detalhados do checkpoint")]
        info: bool,
    },
    
    /// ğŸ“‹ **LIST: Listar checkpoints disponÃ­veis**
    /// 
    /// Lista todos os checkpoints disponÃ­veis em um diretÃ³rio
    /// com informaÃ§Ãµes sobre timestamp, loss e configuraÃ§Ã£o.
    List {
        /// ğŸ“ DiretÃ³rio para buscar checkpoints
        #[arg(short, long, default_value = "models")]
        dir: PathBuf,
    },
    
    /// âš¡ **BENCHMARK: Testar performance de kernel fusion**
    /// 
    /// Executa benchmarks para medir ganhos de performance
    /// das otimizaÃ§Ãµes de kernel fusion em diferentes cenÃ¡rios.
    Benchmark {
        /// ğŸ”¢ Tamanho do batch para teste
        #[arg(long, default_value = "4")]
        batch_size: usize,
        
        /// ğŸ“ Comprimento da sequÃªncia
        #[arg(long, default_value = "128")]
        seq_len: usize,
        
        /// ğŸ§® DimensÃ£o do modelo
        #[arg(long, default_value = "512")]
        d_model: usize,
        
        /// ğŸ”„ NÃºmero de iteraÃ§Ãµes para benchmark
        #[arg(long, default_value = "100")]
        iterations: usize,
        
        /// ğŸ¯ Tipo de benchmark (attention, feedforward, all)
        #[arg(long, default_value = "all")]
        benchmark_type: String,
    },
}

/// ğŸš€ **FUNÃ‡ÃƒO PRINCIPAL: PONTO DE ENTRADA DA APLICAÃ‡ÃƒO**
/// 
/// Esta Ã© a funÃ§Ã£o principal que coordena toda a execuÃ§Ã£o do Mini-GPT.
/// Funciona como um "maestro" que dirige a orquestra de funcionalidades.
/// 
/// ## ğŸ¯ Responsabilidades:
/// 1. **Parsing CLI**: Interpreta argumentos da linha de comando
/// 2. **InicializaÃ§Ã£o**: Configura dispositivo de computaÃ§Ã£o (CPU/GPU)
/// 3. **Roteamento**: Direciona para a funÃ§Ã£o apropriada baseada no comando
/// 4. **Tratamento de Erros**: Propaga erros usando o tipo `Result`
/// 
/// ## ğŸ–¥ï¸ **SeleÃ§Ã£o de Dispositivo:**
/// 
/// ### ğŸ’» **CPU (PadrÃ£o):**
/// - **Vantagens**: Compatibilidade universal, debugging mais fÃ¡cil
/// - **Desvantagens**: Mais lento para operaÃ§Ãµes matriciais grandes
/// - **Uso**: Ideal para desenvolvimento e modelos pequenos
/// 
/// ### ğŸš€ **GPU (Metal/CUDA):**
/// - **Vantagens**: ParalelizaÃ§Ã£o massiva, muito mais rÃ¡pido
/// - **Desvantagens**: Requer hardware especÃ­fico, mais complexo
/// - **Uso**: Essencial para modelos grandes e treinamento intensivo
/// 
/// ## ğŸ“Š **Performance Esperada:**
/// ```text
/// CPU (M3):     ~1000 tokens/segundo (geraÃ§Ã£o)
/// GPU (Metal):  ~5000 tokens/segundo (geraÃ§Ã£o)
/// GPU (CUDA):   ~10000 tokens/segundo (geraÃ§Ã£o)
/// ```
fn main() -> Result<()> {
    // ğŸ® **PARSING DOS ARGUMENTOS CLI**
    // Usa o crate `clap` para interpretar automaticamente
    // os argumentos passados na linha de comando
    let cli = Cli::parse();
    
    // ğŸ¯ **ROTEAMENTO BASEADO NO COMANDO**
    // Pattern matching para executar a funÃ§Ã£o apropriada
    // baseada no comando escolhido pelo usuÃ¡rio
    match cli.command {
        // ğŸ“ **MODO TREINAMENTO**
        // Treina o modelo do zero usando dados fornecidos
        Commands::Train { data, epochs } => {
            // ğŸš€ **DISPOSITIVO PARA TREINAMENTO: METAL GPU ARM APPLE**
        // Prioriza Metal GPU para mÃ¡xima performance no ARM Apple
            let device = match candle_core::Device::new_metal(0) {
                Ok(metal_device) => {
                    println!("ğŸš€ Usando dispositivo: Metal GPU");
                    println!("âš¡ AceleraÃ§Ã£o de hardware ativada para treinamento!");
                    metal_device
                }
                Err(e) => {
                    println!("âš ï¸  Metal GPU nÃ£o disponÃ­vel ({}), usando CPU", e);
                    candle_core::Device::Cpu
                }
            };
            println!("ğŸ“š Iniciando treinamento com dados de: {:?}", data);
            println!("ğŸ”„ Ã‰pocas configuradas: {}", epochs);
            train_model(data, epochs, &device)?
        }
        
        // ğŸ¨ **MODO GERAÃ‡ÃƒO**
        // Gera texto criativo a partir de um prompt
        Commands::Generate { prompt, max_tokens, educational, show_tensors } => {
            // ğŸš€ **DISPOSITIVO PARA GERAÃ‡ÃƒO: METAL GPU ARM APPLE**
            // Prioriza Metal GPU para geraÃ§Ã£o ultra-rÃ¡pida
            let device = match candle_core::Device::new_metal(0) {
                Ok(metal_device) => {
                    println!("ğŸš€ Usando dispositivo: Metal GPU");
                    println!("âš¡ GeraÃ§Ã£o acelerada por hardware ativada!");
                    metal_device
                }
                Err(e) => {
                    println!("âš ï¸  Metal GPU nÃ£o disponÃ­vel ({}), usando CPU", e);
                    candle_core::Device::Cpu
                }
            };
            println!("ğŸš€ Usando dispositivo: {:?}", device);
            println!("âœ¨ Gerando texto a partir de: '{}'", prompt);
            println!("ğŸ¯ MÃ¡ximo de tokens: {}", max_tokens);
            generate_text(&prompt, max_tokens, &device, educational, show_tensors)?
        }
        
        // ğŸ’¬ **MODO CHAT INTERATIVO**
        // Permite conversaÃ§Ã£o em tempo real com o modelo
        Commands::Chat { educational, show_tensors } => {
            // ğŸš€ **DISPOSITIVO PARA CHAT: GPU (com fallback para CPU)**
            // Prioriza GPU para melhor performance em chat interativo
            let device = match candle_core::Device::new_metal(0) {
                Ok(metal_device) => {
                    println!("ğŸš€ Usando dispositivo: Metal GPU (aceleraÃ§Ã£o de hardware)");
                    metal_device
                }
                Err(_) => {
                    // Fallback para CUDA se Metal nÃ£o estiver disponÃ­vel
                    match candle_core::Device::new_cuda(0) {
                        Ok(cuda_device) => {
                            println!("ğŸš€ Usando dispositivo: CUDA GPU (aceleraÃ§Ã£o de hardware)");
                            cuda_device
                        }
                        Err(_) => {
                            println!("âš ï¸  GPU nÃ£o disponÃ­vel, usando CPU");
                            candle_core::Device::Cpu
                        }
                    }
                }
            };
            
            println!("ğŸ’¬ Modo chat ativado! Digite 'quit' ou 'exit' para terminar.");
            println!("ğŸ¤– Aguardando suas mensagens...");
            interactive_chat(&device, educational, show_tensors)?
        }
        
        // ğŸ“‚ **MODO CARREGAMENTO DE MODELO**
        // Carrega modelo de checkpoint e executa geraÃ§Ã£o ou chat
        Commands::Load { 
            checkpoint, 
            dir, 
            best, 
            latest, 
            name_pattern, 
            max_loss, 
            min_step, 
            prompt, 
            max_tokens, 
            chat, 
            educational, 
            info 
        } => {
            let device = match candle_core::Device::new_metal(0) {
                Ok(metal_device) => {
                    println!("ğŸš€ Usando dispositivo: Metal GPU");
                    println!("âš¡ Kernel fusion ativado para mÃ¡xima performance!");
                    metal_device
                }
                Err(e) => {
                    println!("âš ï¸  Metal GPU nÃ£o disponÃ­vel ({}), usando CPU", e);
                    candle_core::Device::Cpu
                }
            };
            
            // ğŸ¯ **SELEÃ‡ÃƒO INTELIGENTE DE CHECKPOINT**
            let selected_checkpoint = select_checkpoint(
                checkpoint,
                &dir,
                best,
                latest,
                name_pattern,
                max_loss,
                min_step,
                info
            )?;
            
            load_and_run_model(selected_checkpoint, prompt, max_tokens, chat, educational, &device)?
        }
        
        // ğŸ“‹ **MODO LISTAGEM DE CHECKPOINTS**
        // Lista todos os checkpoints disponÃ­veis
        Commands::List { dir } => {
            list_checkpoints(dir)?
        }
        
        // âš¡ **MODO BENCHMARK DE KERNEL FUSION**
        // Testa performance das otimizaÃ§Ãµes
        Commands::Benchmark { batch_size, seq_len, d_model, iterations, benchmark_type } => {
            let device = match candle_core::Device::new_metal(0) {
                Ok(metal_device) => {
                    println!("ğŸš€ Usando dispositivo: Metal GPU para benchmark");
                    metal_device
                }
                Err(e) => {
                    println!("âš ï¸  Metal GPU nÃ£o disponÃ­vel ({}), usando CPU", e);
                    candle_core::Device::Cpu
                }
            };
            run_kernel_fusion_benchmark(batch_size, seq_len, d_model, iterations, &benchmark_type, &device)?
        }
    }
    
    // âœ… **FINALIZAÃ‡ÃƒO BEM-SUCEDIDA**
    // Retorna Ok(()) indicando que tudo correu bem
    println!("âœ¨ ExecuÃ§Ã£o concluÃ­da com sucesso!");
    Ok(())
}

/// ğŸ“‚ **CARREGAMENTO E EXECUÃ‡ÃƒO DE MODELO**
/// 
/// Carrega um modelo de checkpoint e executa geraÃ§Ã£o ou chat
fn load_and_run_model(
    checkpoint_path: PathBuf,
    prompt: Option<String>,
    max_tokens: usize,
    chat_mode: bool,
    educational: bool,
    device: &candle_core::Device,
) -> Result<()> {
    println!("ğŸ“‚ Carregando modelo de: {:?}", checkpoint_path);
    
    // Carregar modelo do checkpoint
    let (model, metadata) = MiniGPT::load_from_checkpoint(&checkpoint_path, device)
        .map_err(|e| anyhow::anyhow!("Erro ao carregar checkpoint: {}", e))?;
    println!("âœ… Modelo carregado com sucesso! (Step: {:?})", metadata.training_step);
    
    if chat_mode {
        // Modo chat interativo
        println!("ğŸ’¬ Iniciando chat com modelo carregado...");
        interactive_chat_with_model(&model, educational)
    } else if let Some(prompt_text) = prompt {
        // GeraÃ§Ã£o de texto
        println!("ğŸ¨ Gerando texto a partir do prompt...");
        generate_text_with_model(&model, &prompt_text, max_tokens, educational)
    } else {
        println!("âš ï¸  Especifique um prompt (-p) ou use modo chat (--chat)");
        Ok(())
    }
}

/// ğŸ¯ **SELEÃ‡ÃƒO INTELIGENTE DE CHECKPOINT**
/// 
/// Implementa lÃ³gica avanÃ§ada para seleÃ§Ã£o de checkpoints baseada em critÃ©rios
/// especÃ­ficos como performance, data, nome e filtros customizados.
/// 
/// ## ğŸ§  **Algoritmo de SeleÃ§Ã£o:**
/// 1. **Modo Direto**: Se caminho especÃ­fico fornecido, usa diretamente
/// 2. **Modo AutomÃ¡tico**: Aplica filtros e critÃ©rios de ordenaÃ§Ã£o
/// 3. **Modo Interativo**: Apresenta lista filtrada para seleÃ§Ã£o manual
/// 
/// ## ğŸ“Š **CritÃ©rios de PriorizaÃ§Ã£o:**
/// - **Best**: Menor loss (melhor performance)
/// - **Latest**: Timestamp mais recente
/// - **Pattern**: CorrespondÃªncia de nome/descriÃ§Ã£o
/// - **Filtros**: Loss mÃ¡ximo, step mÃ­nimo
fn select_checkpoint(
    direct_path: Option<PathBuf>,
    search_dir: &PathBuf,
    auto_best: bool,
    auto_latest: bool,
    name_pattern: Option<String>,
    max_loss_filter: Option<f32>,
    min_step_filter: Option<usize>,
    show_info: bool,
) -> Result<PathBuf> {
    // ğŸ¯ **MODO DIRETO: Caminho especÃ­fico fornecido**
    if let Some(path) = direct_path {
        if !path.exists() {
            return Err(anyhow::anyhow!("âŒ Checkpoint nÃ£o encontrado: {:?}", path));
        }
        
        if show_info {
            println!("ğŸ“‹ Carregando checkpoint especÃ­fico: {:?}", path);
            // Carrega apenas para mostrar informaÃ§Ãµes, sem usar o modelo
            if let Ok((_, metadata)) = MiniGPT::load_from_checkpoint(&path, &candle_core::Device::Cpu) {
                display_checkpoint_info(&path, &metadata);
            }
        }
        
        return Ok(path);
    }
    
    // ğŸ“‚ **BUSCA E FILTRAGEM DE CHECKPOINTS**
    println!("ğŸ” Buscando checkpoints em: {:?}", search_dir);
    
    let mut checkpoints = MiniGPT::list_checkpoints(search_dir)
        .map_err(|e| anyhow::anyhow!("Erro ao listar checkpoints: {}", e))?;
    
    if checkpoints.is_empty() {
        return Err(anyhow::anyhow!("ğŸ“­ Nenhum checkpoint encontrado em {:?}", search_dir));
    }
    
    println!("ğŸ“Š Encontrados {} checkpoints", checkpoints.len());
    
    // ğŸ” **APLICAÃ‡ÃƒO DE FILTROS**
    
    // Filtro por padrÃ£o de nome
    if let Some(pattern) = &name_pattern {
        checkpoints.retain(|(path, metadata)| {
            let filename = std::path::Path::new(path)
                .file_name()
                .unwrap_or_default()
                .to_string_lossy()
                .to_lowercase();
            
            let description = metadata.description
                .as_ref()
                .map(|d| d.to_lowercase())
                .unwrap_or_default();
            
            let pattern_lower = pattern.to_lowercase();
            filename.contains(&pattern_lower) || description.contains(&pattern_lower)
        });
        
        println!("ğŸ” ApÃ³s filtro por padrÃ£o '{}': {} checkpoints", pattern, checkpoints.len());
    }
    
    // Filtro por loss mÃ¡ximo
    if let Some(max_loss) = max_loss_filter {
        checkpoints.retain(|(_, metadata)| {
            metadata.loss.map_or(false, |loss| loss <= max_loss)
        });
        
        println!("ğŸ“Š ApÃ³s filtro por loss â‰¤ {}: {} checkpoints", max_loss, checkpoints.len());
    }
    
    // Filtro por step mÃ­nimo
    if let Some(min_step) = min_step_filter {
        checkpoints.retain(|(_, metadata)| {
            metadata.training_step.map_or(false, |step| step >= min_step)
        });
        
        println!("ğŸ”¢ ApÃ³s filtro por step â‰¥ {}: {} checkpoints", min_step, checkpoints.len());
    }
    
    if checkpoints.is_empty() {
        return Err(anyhow::anyhow!("âŒ Nenhum checkpoint atende aos critÃ©rios especificados"));
    }
    
    // ğŸ¯ **SELEÃ‡ÃƒO AUTOMÃTICA**
    
    if auto_best {
        // Seleciona checkpoint com menor loss
        checkpoints.sort_by(|a, b| {
            let loss_a = a.1.loss.unwrap_or(f32::INFINITY);
            let loss_b = b.1.loss.unwrap_or(f32::INFINITY);
            loss_a.partial_cmp(&loss_b).unwrap_or(std::cmp::Ordering::Equal)
        });
        
        let (best_path, best_metadata) = &checkpoints[0];
        println!("ğŸ† Selecionado melhor checkpoint (loss: {:?}): {}", 
                best_metadata.loss, 
                std::path::Path::new(best_path).file_name().unwrap().to_string_lossy());
        
        if show_info {
            display_checkpoint_info(&PathBuf::from(best_path), best_metadata);
        }
        
        return Ok(PathBuf::from(best_path));
    }
    
    if auto_latest {
        // Seleciona checkpoint mais recente
        checkpoints.sort_by(|a, b| b.1.timestamp.cmp(&a.1.timestamp));
        
        let (latest_path, latest_metadata) = &checkpoints[0];
        println!("ğŸ“… Selecionado checkpoint mais recente: {}", 
                std::path::Path::new(latest_path).file_name().unwrap().to_string_lossy());
        
        if show_info {
            display_checkpoint_info(&PathBuf::from(latest_path), latest_metadata);
        }
        
        return Ok(PathBuf::from(latest_path));
    }
    
    // ğŸ® **MODO INTERATIVO: SeleÃ§Ã£o manual**
    println!("\nğŸ® Modo de seleÃ§Ã£o interativa ativado!");
    println!("{}", "=".repeat(80));
    
    // Ordena por loss (melhor primeiro) para apresentaÃ§Ã£o
    checkpoints.sort_by(|a, b| {
        let loss_a = a.1.loss.unwrap_or(f32::INFINITY);
        let loss_b = b.1.loss.unwrap_or(f32::INFINITY);
        loss_a.partial_cmp(&loss_b).unwrap_or(std::cmp::Ordering::Equal)
    });
    
    for (i, (path, metadata)) in checkpoints.iter().enumerate() {
        let filename = std::path::Path::new(path).file_name().unwrap().to_string_lossy();
        println!("{}. ğŸ“ {}", i + 1, filename);
        println!("   ğŸ“Š Loss: {:?} | ğŸ“… {}", metadata.loss, metadata.timestamp);
        
        if let Some(step) = metadata.training_step {
            println!("   ğŸ”¢ Step: {}", step);
        }
        
        if let Some(desc) = &metadata.description {
            println!("   ğŸ“ {}", desc);
        }
        
        println!();
    }
    
    println!("Digite o nÃºmero do checkpoint desejado (1-{}) ou 'q' para cancelar:", checkpoints.len());
    
    use std::io::{self, Write};
    loop {
        print!("ğŸ¯ Sua escolha: ");
        io::stdout().flush()?;
        
        let mut input = String::new();
        io::stdin().read_line(&mut input)?;
        let input = input.trim();
        
        if input.eq_ignore_ascii_case("q") || input.eq_ignore_ascii_case("quit") {
            return Err(anyhow::anyhow!("âŒ SeleÃ§Ã£o cancelada pelo usuÃ¡rio"));
        }
        
        if let Ok(choice) = input.parse::<usize>() {
            if choice >= 1 && choice <= checkpoints.len() {
                let (selected_path, selected_metadata) = &checkpoints[choice - 1];
                println!("âœ… Checkpoint selecionado: {}", 
                        std::path::Path::new(selected_path).file_name().unwrap().to_string_lossy());
                
                if show_info {
                    display_checkpoint_info(&PathBuf::from(selected_path), selected_metadata);
                }
                
                return Ok(PathBuf::from(selected_path));
            }
        }
        
        println!("âŒ OpÃ§Ã£o invÃ¡lida. Digite um nÃºmero entre 1 e {} ou 'q' para cancelar.", checkpoints.len());
    }
}

/// ğŸ“‹ **EXIBIÃ‡ÃƒO DE INFORMAÃ‡Ã•ES DETALHADAS DO CHECKPOINT**
/// 
/// Mostra metadados completos de um checkpoint especÃ­fico
fn display_checkpoint_info(path: &PathBuf, metadata: &CheckpointMetadata) {
    println!("\nğŸ“‹ InformaÃ§Ãµes Detalhadas do Checkpoint");
    println!("{}", "=".repeat(50));
    println!("ğŸ“ Arquivo: {}", path.file_name().unwrap().to_string_lossy());
    println!("ğŸ“‚ Caminho: {:?}", path);
    println!("ğŸ“… Timestamp: {}", metadata.timestamp);
    println!("ğŸ”§ VersÃ£o: {}", metadata.version);
    
    if let Some(loss) = metadata.loss {
        println!("ğŸ“Š Loss: {:.6}", loss);
    }
    
    if let Some(step) = metadata.training_step {
        println!("ğŸ”¢ Training Step: {}", step);
    }
    
    if let Some(desc) = &metadata.description {
        println!("ğŸ“ DescriÃ§Ã£o: {}", desc);
    }
    
    // InformaÃ§Ãµes do arquivo
    if let Ok(file_metadata) = std::fs::metadata(path) {
        let size_mb = file_metadata.len() as f64 / (1024.0 * 1024.0);
        println!("ğŸ’¾ Tamanho: {:.2} MB", size_mb);
    }
    
    println!("{}", "=".repeat(50));
}

/// ğŸ“‹ **LISTAGEM DE CHECKPOINTS**
/// 
/// Lista todos os checkpoints disponÃ­veis em um diretÃ³rio
fn list_checkpoints(dir: PathBuf) -> Result<()> {
    println!("ğŸ“‹ Listando checkpoints em: {:?}", dir);
    
    let checkpoints = MiniGPT::list_checkpoints(&dir)
        .map_err(|e| anyhow::anyhow!("Erro ao listar checkpoints: {}", e))?;
    
    if checkpoints.is_empty() {
        println!("ğŸ“­ Nenhum checkpoint encontrado no diretÃ³rio.");
        return Ok(());
    }
    
    println!("\nğŸ“Š Checkpoints encontrados:");
    println!("{}", "-".repeat(80));
    
    for (i, (path, metadata)) in checkpoints.iter().enumerate() {
        println!("{}. ğŸ“ {}", i + 1, std::path::Path::new(path).file_name().unwrap().to_string_lossy());
        println!("   ğŸ“… Timestamp: {}", metadata.timestamp);
        println!("   ğŸ“Š Loss: {:?}", metadata.loss);
        println!("   ğŸ”§ VersÃ£o: {}", metadata.version);
        
        if let Some(description) = &metadata.description {
            println!("   ğŸ“ DescriÃ§Ã£o: {}", description);
        }
        
        println!();
    }
    
    Ok(())
}

/// âš¡ **BENCHMARK DE KERNEL FUSION**
/// 
/// Executa benchmarks para medir ganhos de performance
fn run_kernel_fusion_benchmark(
    batch_size: usize,
    seq_len: usize,
    d_model: usize,
    iterations: usize,
    benchmark_type: &str,
    device: &candle_core::Device,
) -> Result<()> {
    println!("âš¡ Executando benchmark de kernel fusion...");
    println!("ğŸ“Š ConfiguraÃ§Ã£o:");
    println!("   ğŸ”¢ Batch size: {}", batch_size);
    println!("   ğŸ“ Sequence length: {}", seq_len);
    println!("   ğŸ§® Model dimension: {}", d_model);
    println!("   ğŸ”„ Iterations: {}", iterations);
    println!("   ğŸ¯ Type: {}", benchmark_type);
    println!();
    
    let fusion_config = FusionConfig {
        enable_attention_fusion: true,
        enable_feedforward_fusion: true,
        enable_memory_optimization: true,
        fusion_threshold: 512,
    };
    
    let benchmark = FusionBenchmark::new(fusion_config, device.clone());
    
    match benchmark_type {
        "attention" => {
            let results = benchmark.benchmark_attention(batch_size, seq_len, d_model, iterations)?;
            println!("ğŸ¯ Resultados do Benchmark de AtenÃ§Ã£o:");
            println!("   âš¡ Fusionado: {:.2}ms (mÃ©dia)", results.fused_time_ms);
            println!("   ğŸŒ NÃ£o-fusionado: {:.2}ms (mÃ©dia)", results.unfused_time_ms);
            println!("   ğŸš€ Speedup: {:.2}x", results.speedup);
            println!("   ğŸ’¾ Economia de memÃ³ria: {:.1}%", results.memory_saved_percent);
        }
        "feedforward" => {
            let results = benchmark.benchmark_feedforward(batch_size, seq_len, d_model, iterations)?;
            println!("ğŸ¯ Resultados do Benchmark de Feed-Forward:");
            println!("   âš¡ Fusionado: {:.2}ms (mÃ©dia)", results.fused_time_ms);
            println!("   ğŸŒ NÃ£o-fusionado: {:.2}ms (mÃ©dia)", results.unfused_time_ms);
            println!("   ğŸš€ Speedup: {:.2}x", results.speedup);
            println!("   ğŸ’¾ Economia de memÃ³ria: {:.1}%", results.memory_saved_percent);
        }
        "all" => {
            println!("ğŸ¯ Executando benchmark completo...");
            
            let attention_results = benchmark.benchmark_attention(batch_size, seq_len, d_model, iterations)?;
            println!("\nğŸ“Š AtenÃ§Ã£o Multi-Head:");
            println!("   âš¡ Fusionado: {:.2}ms", attention_results.fused_time_ms);
            println!("   ğŸŒ NÃ£o-fusionado: {:.2}ms", attention_results.unfused_time_ms);
            println!("   ğŸš€ Speedup: {:.2}x", attention_results.speedup);
            
            let ff_results = benchmark.benchmark_feedforward(batch_size, seq_len, d_model, iterations)?;
            println!("\nğŸ“Š Feed-Forward:");
            println!("   âš¡ Fusionado: {:.2}ms", ff_results.fused_time_ms);
            println!("   ğŸŒ NÃ£o-fusionado: {:.2}ms", ff_results.unfused_time_ms);
            println!("   ğŸš€ Speedup: {:.2}x", ff_results.speedup);
            
            let total_speedup = (attention_results.speedup + ff_results.speedup) / 2.0;
            println!("\nğŸ† Speedup mÃ©dio total: {:.2}x", total_speedup);
        }
        _ => {
            println!("âŒ Tipo de benchmark invÃ¡lido. Use: attention, feedforward, ou all");
        }
    }
    
    Ok(())
}

/// ğŸ¨ **GERAÃ‡ÃƒO DE TEXTO COM MODELO CARREGADO**
fn generate_text_with_model(
    _model: &MiniGPT,
    prompt: &str,
    max_tokens: usize,
    educational: bool,
) -> Result<()> {
    // ImplementaÃ§Ã£o simplificada - na prÃ¡tica, vocÃª precisaria
    // implementar a lÃ³gica de geraÃ§Ã£o usando o modelo carregado
    println!("ğŸ¨ Gerando texto com modelo carregado...");
    println!("ğŸ’­ Prompt: {}", prompt);
    println!("ğŸ¯ Max tokens: {}", max_tokens);
    
    if educational {
        println!("ğŸ“š Modo educacional ativado");
    }
    
    // TODO: Implementar geraÃ§Ã£o real
    println!("âš ï¸  GeraÃ§Ã£o com modelo carregado ainda nÃ£o implementada");
    
    Ok(())
}

/// ğŸ’¬ **CHAT INTERATIVO COM MODELO CARREGADO**
fn interactive_chat_with_model(
    _model: &MiniGPT,
    educational: bool,
) -> Result<()> {
    // ImplementaÃ§Ã£o simplificada - na prÃ¡tica, vocÃª precisaria
    // implementar a lÃ³gica de chat usando o modelo carregado
    println!("ğŸ’¬ Chat interativo com modelo carregado...");
    
    if educational {
        println!("ğŸ“š Modo educacional ativado");
    }
    
    // TODO: Implementar chat real
    println!("âš ï¸  Chat com modelo carregado ainda nÃ£o implementado");
    
    Ok(())
}

/// ğŸ“ **FUNÃ‡ÃƒO DE TREINAMENTO DO MODELO**
/// 
/// Esta funÃ§Ã£o implementa o processo completo de treinamento de um modelo de linguagem.
/// Ã‰ como ensinar uma crianÃ§a a escrever: mostramos exemplos e ela aprende os padrÃµes.
/// 
/// ## ğŸ“‹ Etapas do Treinamento:
/// 
/// ### 1. **TokenizaÃ§Ã£o** ğŸ“
/// - Converte texto bruto em tokens (unidades processÃ¡veis)
/// - Similar a como dividimos frases em palavras para entender
/// - Cada token recebe um ID numÃ©rico Ãºnico
/// 
/// ### 2. **CriaÃ§Ã£o do Modelo** ğŸ§ 
/// - Inicializa a arquitetura Transformer com pesos aleatÃ³rios
/// - Define o tamanho do vocabulÃ¡rio baseado nos tokens encontrados
/// - Cria camadas de atenÃ§Ã£o, embeddings e redes neurais
/// 
/// ### 3. **Processo de Treinamento** ğŸ¯
/// - **Forward Pass**: Modelo faz prediÃ§Ãµes sobre prÃ³xima palavra
/// - **Loss Calculation**: Compara prediÃ§Ã£o com resposta correta
/// - **Backpropagation**: Calcula gradientes (direÃ§Ã£o para melhorar)
/// - **Optimization**: Ajusta pesos usando gradiente descendente
/// - Repete por vÃ¡rias Ã©pocas atÃ© convergir
fn train_model(data_path: PathBuf, epochs: usize, device: &candle_core::Device) -> Result<()> {
    // ğŸ“– **ETAPA 1: CARREGAMENTO DOS DADOS**
    // LÃª o arquivo de texto que serÃ¡ usado como corpus de treinamento
    // Este texto contÃ©m os padrÃµes que o modelo vai aprender
    let text = std::fs::read_to_string(data_path)?;
    
    // ğŸ”¤ **ETAPA 2: TOKENIZAÃ‡ÃƒO**
    // O tokenizador converte texto em nÃºmeros que o modelo pode processar
    // BPE (Byte Pair Encoding) Ã© eficiente para vocabulÃ¡rios grandes
    // Processo similar a criar um "dicionÃ¡rio" onde cada palavra/subpalavra tem um nÃºmero
    //
    // ## ğŸ¯ **Como funciona o BPE (Byte Pair Encoding):**
    // 1. ComeÃ§a com caracteres individuais
    // 2. Encontra pares de bytes mais frequentes
    // 3. Substitui pares por novos tokens
    // 4. Repete atÃ© atingir tamanho de vocabulÃ¡rio desejado
    //
    // **Exemplo prÃ¡tico:**
    // - Texto: "Brasil Brasil brasileiro"
    // - Passo 1: ['B', 'r', 'a', 's', 'i', 'l', ' ', ...]
    // - Passo 2: Encontra 'Br' frequente â†’ token 256
    // - Passo 3: Encontra 'as' frequente â†’ token 257
    // - Resultado: [256, 257, 'i', 'l', ' ', 256, 257, 'i', 'l', ...]
    let mut tokenizer = tokenizer::BPETokenizer::new(1000)?; // 1000 tokens de vocabulÃ¡rio
    tokenizer.train(&text)?;  // Analisa o texto e cria vocabulÃ¡rio
    
    // ğŸ”¢ **CONVERSÃƒO TEXTO â†’ NÃšMEROS**
    // Transforma todo o corpus em sequÃªncia de IDs numÃ©ricos
    // Estes nÃºmeros sÃ£o o que o modelo realmente "vÃª" durante o treinamento
    //
    // ## ğŸ“Š **Exemplo de TokenizaÃ§Ã£o:**
    // - Texto: "O Brasil Ã© um paÃ­s"
    // - Tokens: [15, 42, 89, 156, 203]
    // - Cada nÃºmero representa uma palavra/subpalavra no vocabulÃ¡rio
    let tokens = tokenizer.encode(&text)?;
    println!("ğŸ“Š Total de tokens: {}", tokens.len());
    
    // ğŸ§  **ETAPA 3: CONFIGURAÃ‡ÃƒO E CRIAÃ‡ÃƒO DO MODELO**
    // Define a arquitetura do Transformer - como o "DNA" do modelo
    //
    // ## ğŸ“Š **ParÃ¢metros Explicados:**
    // - `vocab_size`: Tamanho do vocabulÃ¡rio (quantas palavras/tokens Ãºnicos)
    // - `n_embd`: DimensÃ£o dos embeddings (128 = modelo pequeno educacional)
    // - `n_head`: CabeÃ§as de atenÃ§Ã£o (4 = permite focar em 4 aspectos diferentes)
    // - `n_layer`: Camadas Transformer (4 = profundidade moderada)
    // - `block_size`: Contexto mÃ¡ximo (64 tokens = ~48 palavras em portuguÃªs)
    // - `dropout`: RegularizaÃ§Ã£o para evitar overfitting
    let config = model::GPTConfig {
        vocab_size: tokenizer.vocab_size(),  // Quantas palavras diferentes o modelo conhece
        n_embd: 128,      // DimensÃ£o dos embeddings (pequena para educacional)
        n_head: 4,        // 4 cabeÃ§as de atenÃ§Ã£o (permite focar em aspectos diferentes)
        n_layer: 4,       // 4 camadas transformer (profundidade do modelo)
        block_size: 64,   // Contexto de 64 tokens (quantas palavras o modelo "lembra")
        dropout: 0.1,     // RegularizaÃ§Ã£o para evitar overfitting
    };
    
    // ğŸ—ï¸ **CONSTRUÃ‡ÃƒO DA ARQUITETURA NEURAL**
    // Cria todas as camadas, pesos e conexÃµes do modelo
    // Inicialmente com valores aleatÃ³rios - o treinamento vai ajustÃ¡-los
    //
    // ## ğŸ§  **Componentes Criados:**
    // - Embeddings de tokens e posiÃ§Ãµes
    // - Camadas Transformer com atenÃ§Ã£o multi-head
    // - Redes feed-forward
    // - Layer normalization
    // - CabeÃ§a de linguagem para prediÃ§Ã£o
    let model = MiniGPT::new(config, device).map_err(|e| anyhow::anyhow!("{}", e))?;
    println!("ğŸ§  Modelo criado com {} parÃ¢metros", model.num_parameters());
    
    // ğŸ¯ **ETAPA 4: TREINAMENTO PROPRIAMENTE DITO**
    // Aqui acontece a "mÃ¡gica": o modelo aprende padrÃµes atravÃ©s de:
    // - MÃºltiplas passadas pelos dados (Ã©pocas)
    // - Ajuste gradual dos pesos neurais via backpropagation
    // - MinimizaÃ§Ã£o da funÃ§Ã£o de perda (cross-entropy loss)
    //
    // ## ğŸ”„ **Processo de Treinamento:**
    // 1. **Forward Pass**: Modelo faz prediÃ§Ãµes sobre prÃ³xima palavra
    // 2. **Loss Calculation**: Compara prediÃ§Ã£o com resposta correta
    // 3. **Backpropagation**: Calcula gradientes (direÃ§Ã£o para melhorar)
    // 4. **Optimization**: Ajusta pesos usando gradiente descendente
    // 5. Repete por vÃ¡rias Ã©pocas atÃ© convergir
    let mut trainer = Trainer::new(model, tokenizer, device.clone());
    trainer.train(&tokens, epochs).map_err(|e| anyhow::anyhow!("{}", e))?;
    
    // ğŸ’¾ **ETAPA 5: PERSISTÃŠNCIA DO MODELO TREINADO**
    // Salva os pesos aprendidos para uso posterior
    // Formato SafeTensors Ã© seguro e eficiente para modelos ML
    //
    // ## ğŸ’¾ **Processo de Salvamento:**
    // 1. Serializa todos os tensores do modelo
    // 2. Salva metadados da arquitetura
    // 3. Cria checkpoint para recuperaÃ§Ã£o
    // 4. Valida integridade dos dados salvos
    trainer.save("models/mini_gpt.safetensors").map_err(|e| anyhow::anyhow!("{}", e))?;
    println!("ğŸ’¾ Modelo salvo com sucesso!");
    
    Ok(())
}

/// ğŸ¨ **FUNÃ‡ÃƒO DE GERAÃ‡ÃƒO DE TEXTO**
/// 
/// Esta funÃ§Ã£o demonstra como um modelo treinado pode gerar texto novo.
/// Ã‰ como pedir para uma pessoa continuar uma frase - ela usa o que aprendeu
/// para criar algo coerente e contextualmente apropriado.
/// 
/// ## ğŸ”„ Processo de GeraÃ§Ã£o:
/// 
/// ### 1. **TokenizaÃ§Ã£o do Prompt** ğŸ“
/// - Converte o texto inicial em tokens que o modelo entende
/// - Mesmo processo usado no treinamento
/// 
/// ### 2. **InferÃªncia Autoregressiva** ğŸ”®
/// - Modelo prediz prÃ³ximo token baseado no contexto
/// - Adiciona prediÃ§Ã£o ao contexto e repete
/// - Processo iterativo atÃ© atingir limite de tokens
/// 
/// ### 3. **Sampling com Temperatura** ğŸŒ¡ï¸
/// - Controla criatividade vs. determinismo
/// - Temperatura baixa = mais conservador
/// - Temperatura alta = mais criativo/aleatÃ³rio
/// 
/// ## ğŸ¯ ParÃ¢metros:
/// - `prompt`: Texto inicial para comeÃ§ar a geraÃ§Ã£o
/// - `max_tokens`: Limite mÃ¡ximo de tokens a gerar
/// - `device`: Dispositivo de computaÃ§Ã£o (CPU/GPU)
/// 
/// ## ğŸ“Š **Processo Detalhado:**
/// ```text
/// 1. Tokenizar prompt: "Era uma vez" â†’ [15, 42, 89]
/// 2. Forward pass: modelo prediz distribuiÃ§Ã£o do prÃ³ximo token
/// 3. Sampling: escolhe token baseado na distribuiÃ§Ã£o + temperatura
/// 4. Adicionar ao contexto: [15, 42, 89, 156]
/// 5. Repetir atÃ© max_tokens ou token de fim
/// ```
fn generate_text(prompt: &str, max_tokens: usize, device: &candle_core::Device, educational: bool, show_tensors: bool) -> Result<()> {
    use std::time::Instant;
    
    let start_time = Instant::now();
    
    // ğŸ“ **INICIALIZAÃ‡ÃƒO DO LOGGER EDUCACIONAL**
    let verbosity_level = if educational { if show_tensors { 3 } else { 2 } } else { 0 };
    let logger = EducationalLogger::new(verbosity_level);
    
    // ğŸ”§ **ETAPA 1: INICIALIZAÃ‡ÃƒO DO TOKENIZADOR**
    // 
    // âš ï¸ **NOTA IMPORTANTE**: Em produÃ§Ã£o, carregarÃ­amos o tokenizador
    // exato usado durante o treinamento para garantir consistÃªncia.
    // Aqui criamos um novo apenas para demonstraÃ§Ã£o educacional.
    let mut tokenizer = tokenizer::BPETokenizer::new(1000)?;
    
    // ğŸ“š **ETAPA 2: TREINAMENTO RÃPIDO DO TOKENIZADOR**
    // 
    // Para demonstraÃ§Ã£o, treina com um corpus pequeno em portuguÃªs.
    // Em produÃ§Ã£o, usarÃ­amos o mesmo vocabulÃ¡rio do treinamento.
    // 
    // ## ğŸ¯ **Por que ConsistÃªncia Ã© Crucial:**
    // - Tokens diferentes = embeddings diferentes
    // - Modelo nÃ£o reconhece tokens "novos"
    // - Pode gerar texto incoerente ou falhar
    // 
    // ğŸ“š **CORPUS DE DEMONSTRAÃ‡ÃƒO EM PORTUGUÃŠS BRASILEIRO**
    // 
    // Este corpus pequeno serve apenas para demonstraÃ§Ã£o educacional.
    // Em produÃ§Ã£o, usarÃ­amos:
    // - Datasets gigantes (GB ou TB de texto)
    // - Texto limpo e prÃ©-processado
    // - MÃºltiplos domÃ­nios (notÃ­cias, literatura, web, etc.)
    // - Balanceamento de tÃ³picos e estilos
    let sample_text = "O Brasil Ã© um paÃ­s tropical. A inteligÃªncia artificial estÃ¡ revolucionando o mundo. \
                      A programaÃ§Ã£o em Rust Ã© segura e eficiente. O aprendizado de mÃ¡quina utiliza dados para fazer previsÃµes.";
    
    // ğŸ¯ **TREINAMENTO DO TOKENIZADOR BPE**
    // 
    // ## ğŸ”¤ **Como funciona o BPE (Byte Pair Encoding):**
    // 1. ComeÃ§a com caracteres individuais
    // 2. Encontra pares de bytes mais frequentes
    // 3. Substitui pares por novos tokens
    // 4. Repete atÃ© atingir tamanho de vocabulÃ¡rio desejado
    // 
    // **Exemplo prÃ¡tico:**
    // - Texto: "Brasil Brasil brasileiro"
    // - Passo 1: ['B', 'r', 'a', 's', 'i', 'l', ' ', ...]
    // - Passo 2: Encontra 'Br' frequente â†’ token 256
    // - Passo 3: Encontra 'as' frequente â†’ token 257
    // - Resultado: [256, 257, 'i', 'l', ' ', 256, 257, 'i', 'l', ...]
    tokenizer.train(sample_text)?;
    
    // ğŸ“ **LOG EDUCACIONAL: TOKENIZAÃ‡ÃƒO**
    // 
    // Mostra como o texto foi dividido em tokens para ajudar
    // a entender o processo de tokenizaÃ§Ã£o. Ãštil para:
    // - Debugar problemas de tokenizaÃ§Ã£o
    // - Entender como o modelo "vÃª" o texto
    // - Otimizar prompts para melhor performance
    if educational {
        let tokens = tokenizer.encode(prompt)?;
        logger.log_tokenization(prompt, &tokens, &tokenizer)?;
    }
    
    // ğŸ—ï¸ **ETAPA 3: RECRIAÃ‡ÃƒO DA ARQUITETURA DO MODELO**
    // 
    // âš ï¸ **CRÃTICO**: A arquitetura deve ser IDÃŠNTICA Ã  usada no treinamento!
    // Qualquer diferenÃ§a (n_embd, n_head, n_layer) causarÃ¡ erro de carregamento.
    // 
    // ## ğŸ“Š **ParÃ¢metros Explicados:**
    // - `vocab_size`: Tamanho do vocabulÃ¡rio (quantas palavras/tokens Ãºnicos)
    // - `n_embd`: DimensÃ£o dos embeddings (128 = modelo pequeno educacional)
    // - `n_head`: CabeÃ§as de atenÃ§Ã£o (4 = permite focar em 4 aspectos diferentes)
    // - `n_layer`: Camadas Transformer (4 = profundidade moderada)
    // - `block_size`: Contexto mÃ¡ximo (64 tokens = ~48 palavras em portuguÃªs)
    // - `dropout`: 0.0 durante inferÃªncia (sem regularizaÃ§Ã£o)
    let config = model::GPTConfig {
        vocab_size: tokenizer.vocab_size(),  // Baseado no vocabulÃ¡rio treinado
        n_embd: 128,      // Embeddings de 128 dimensÃµes
        n_head: 4,        // 4 cabeÃ§as de atenÃ§Ã£o multi-head
        n_layer: 4,       // 4 camadas Transformer empilhadas
        block_size: 64,   // Contexto de 64 tokens
        dropout: 0.0,     // Sem dropout durante inferÃªncia
    };
    
    // ğŸ§  **ETAPA 4: CARREGAMENTO DO MODELO**
    // 
    // Em produÃ§Ã£o, carregarÃ­amos os pesos salvos do treinamento:
    // ```rust
    // let model = MiniGPT::load_from_checkpoint("model.safetensors", device)?;
    // ```
    // 
    // Aqui criamos um modelo "virgem" apenas para demonstraÃ§Ã£o.
    // 
    // ğŸ—ï¸ **CRIAÃ‡ÃƒO DA ARQUITETURA DO MODELO**
    // 
    // Inicializa todas as camadas neurais:
    // - Embeddings de tokens e posiÃ§Ãµes
    // - Camadas Transformer com atenÃ§Ã£o multi-head
    // - Redes feed-forward
    // - Layer normalization
    // - CabeÃ§a de linguagem para prediÃ§Ã£o
    let model = MiniGPT::new(config, device).map_err(|e| anyhow::anyhow!("{}", e))?;
    
    // ğŸ¯ **ETAPA 5: CONFIGURAÃ‡ÃƒO DA GERAÃ‡ÃƒO**
    if !educational {
        println!("ğŸ¯ Prompt: '{}'", prompt);
        println!("ğŸ”„ Gerando {} tokens...", max_tokens);
        println!("ğŸŒ¡ï¸ Temperatura: 0.8 (criatividade moderada)");
    }
    
    // ğŸ² **ETAPA 6: GERAÃ‡ÃƒO AUTOREGRESSIVA COM SAMPLING**
    // 
    // ## ğŸŒ¡ï¸ **Controle de Temperatura:**
    // ```text
    // Temperatura 0.1: Muito conservador, repetitivo
    // Temperatura 0.8: EquilÃ­brio ideal (usado aqui)
    // Temperatura 1.5: Muito criativo, pode ser incoerente
    // ```
    // 
    // ## ğŸ”„ **Processo Autoregressivo:**
    // 1. Tokeniza prompt inicial
    // 2. Forward pass â†’ distribuiÃ§Ã£o de probabilidades
    // 3. Aplica temperatura para controlar aleatoriedade
    // 4. Faz sampling da distribuiÃ§Ã£o
    // 5. Adiciona token escolhido ao contexto
    // 6. Repete atÃ© atingir max_tokens ou token especial
    match model.generate(prompt, max_tokens, &tokenizer, 0.8) {
        Ok(generated_text) => {
            let processing_time = start_time.elapsed().as_secs_f32();
            
            // ğŸ“ **LOG EDUCACIONAL: RESUMO FINAL**
            if educational {
                let full_text = format!("{}{}", prompt, generated_text);
                let token_count = tokenizer.encode(&full_text)?.len();
                logger.log_process_summary(prompt, &full_text, token_count, processing_time)?;
            } else {
                println!("\nâœ¨ **RESULTADO DA GERAÃ‡ÃƒO:**");
                println!("ğŸ“ Texto completo:");
                println!("{}{}", prompt, generated_text);
                println!("\nğŸ“Š EstatÃ­sticas:");
                println!("   â€¢ Tokens gerados: ~{}", generated_text.split_whitespace().count());
                println!("   â€¢ Caracteres: {}", generated_text.len());
                println!("   â€¢ Tempo de processamento: {:.2}ms", processing_time * 1000.0);
            }
        }
        Err(e) => {
            println!("âŒ Erro na geraÃ§Ã£o: {}", e);
            println!("ğŸ’¡ Dica: Treine o modelo primeiro com 'mini-gpt train'");
        }
    }
    
    Ok(())
}

/// ğŸ’¬ **FUNÃ‡ÃƒO DE CHAT INTERATIVO**
/// 
/// Esta funÃ§Ã£o cria uma interface de conversaÃ§Ã£o em tempo real com o modelo.
/// Ã‰ como ter uma conversa com o modelo treinado, onde vocÃª pode:
/// - Fazer perguntas e receber respostas
/// - Ajustar parÃ¢metros de geraÃ§Ã£o dinamicamente
/// - Experimentar com diferentes configuraÃ§Ãµes
/// 
/// ## ğŸ›ï¸ ParÃ¢metros AjustÃ¡veis:
/// 
/// ### ğŸŒ¡ï¸ **Temperatura**
/// - Controla a "criatividade" do modelo
/// - 0.1 = Muito conservador, respostas previsÃ­veis
/// - 1.0 = Equilibrado entre criatividade e coerÃªncia
/// - 2.0 = Muito criativo, pode ser incoerente
/// 
/// ### ğŸ”¢ **Max Tokens**
/// - Define o comprimento mÃ¡ximo da resposta
/// - Mais tokens = respostas mais longas
/// - Menos tokens = respostas mais concisas
/// ğŸ’¬ **CHAT INTERATIVO: CONVERSAÃ‡ÃƒO EM TEMPO REAL**
/// 
/// Implementa um sistema de chat onde o usuÃ¡rio pode conversar
/// com o modelo de linguagem em tempo real, mantendo contexto
/// e permitindo ajustes dinÃ¢micos de parÃ¢metros.
/// 
/// ## ğŸ¯ **Funcionalidades Principais:**
/// 
/// ### 1. **ConversaÃ§Ã£o ContÃ­nua** ğŸ”„
/// - MantÃ©m histÃ³rico da conversa
/// - Contexto preservado entre mensagens
/// - Respostas baseadas no histÃ³rico completo
/// 
/// ### 2. **Comandos Especiais** ğŸ›ï¸
/// - `/temp <valor>`: Ajusta criatividade (0.1-2.0)
/// - `/tokens <num>`: Define tamanho da resposta (10-200)
/// - `/help`: Mostra ajuda dos comandos
/// - `quit`/`exit`: Sai do chat
/// 
/// ### 3. **Interface AmigÃ¡vel** ğŸ¨
/// - Prompts coloridos e informativos
/// - Feedback em tempo real
/// - Tratamento de erros gracioso
/// 
/// ## ğŸ§  **Arquitetura do Sistema:**
/// ```text
/// Input do UsuÃ¡rio â†’ TokenizaÃ§Ã£o â†’ Contexto + Nova Mensagem
///                                        â†“
/// Resposta Formatada â† DecodificaÃ§Ã£o â† GeraÃ§Ã£o Autoregressiva
/// ```
/// 
/// ## ğŸ¯ ParÃ¢metros:
/// - `device`: Dispositivo de computaÃ§Ã£o (CPU/GPU)
/// 
/// ## ğŸ“Š **ConfiguraÃ§Ãµes Otimizadas para Chat:**
/// - **Temperatura padrÃ£o**: 0.8 (equilÃ­brio criatividade/coerÃªncia)
/// - **Max tokens padrÃ£o**: 50 (respostas concisas)
/// - **Block size**: 64 (contexto suficiente para conversaÃ§Ã£o)
fn interactive_chat(device: &candle_core::Device, educational: bool, show_tensors: bool) -> Result<()> {
    use std::io::{self, Write};
    use std::time::Instant;
    
    // ğŸ“ **INICIALIZAÃ‡ÃƒO DO LOGGER EDUCACIONAL**
    let verbosity_level = if educational { if show_tensors { 3 } else { 2 } } else { 0 };
    let logger = EducationalLogger::new(verbosity_level);
    
    // ğŸ”§ **ETAPA 1: INICIALIZAÃ‡ÃƒO DO TOKENIZADOR**
    // Prepara o sistema de tokenizaÃ§Ã£o para conversaÃ§Ã£o interativa
    let mut tokenizer = tokenizer::BPETokenizer::new(1000)?;
    
    // ğŸ“– **ETAPA 2: CARREGAMENTO DO CORPUS DE TREINAMENTO**
    // 
    // Tenta carregar arquivo de corpus do disco, caso contrÃ¡rio
    // usa um corpus de exemplo em portuguÃªs para demonstraÃ§Ã£o.
    // 
    // ## ğŸ¯ **EstratÃ©gia de Fallback:**
    // 1. Tenta ler "corpus_pt_br.txt" do diretÃ³rio atual
    // 2. Se falhar, usa texto de exemplo embutido
    // 3. Garante que o sistema sempre funcione
    let sample_text = std::fs::read_to_string("corpus_pt_br.txt")
        .unwrap_or_else(|_| {
            println!("âš ï¸  Arquivo corpus_pt_br.txt nÃ£o encontrado, usando corpus de exemplo.");
            "O Brasil Ã© um paÃ­s tropical localizado na AmÃ©rica do Sul. \
             A inteligÃªncia artificial estÃ¡ transformando o mundo. \
             Rust Ã© uma linguagem de programaÃ§Ã£o segura e eficiente. \
             O aprendizado de mÃ¡quina utiliza dados para fazer previsÃµes inteligentes. \
             A conversaÃ§Ã£o Ã© uma forma natural de comunicaÃ§Ã£o humana. \
             Os chatbots modernos podem manter diÃ¡logos coerentes e Ãºteis.".to_string()
        });
    
    // ğŸ”¤ **ETAPA 3: TREINAMENTO DO TOKENIZADOR**
    // ConstrÃ³i o vocabulÃ¡rio baseado no corpus disponÃ­vel
    println!("ğŸ”¤ Treinando tokenizador com {} caracteres...", sample_text.len());
    tokenizer.train(&sample_text)?;
    println!("âœ… VocabulÃ¡rio criado com {} tokens", tokenizer.vocab_size());
    
    // ğŸ—ï¸ **ETAPA 4: CONFIGURAÃ‡ÃƒO DO MODELO PARA CHAT**
    // 
    // ConfiguraÃ§Ã£o otimizada para conversaÃ§Ã£o interativa:
    // - Modelo pequeno para respostas rÃ¡pidas
    // - Block size adequado para manter contexto
    // - Sem dropout para inferÃªncia determinÃ­stica
    let config = model::GPTConfig {
        vocab_size: tokenizer.vocab_size(),  // ğŸ“Š Baseado no vocabulÃ¡rio treinado
        n_embd: 128,                         // ğŸ§® Embeddings compactos para velocidade
        n_head: 4,                           // ğŸ¯ AtenÃ§Ã£o suficiente para coerÃªncia
        n_layer: 4,                          // ğŸ—ï¸ Profundidade balanceada
        block_size: 64,                      // ğŸ“ Contexto adequado para chat
        dropout: 0.0,                        // âš ï¸ Sem dropout para inferÃªncia!
    };
    
    // ğŸ§  **ETAPA 5: INICIALIZAÃ‡ÃƒO DO MODELO**
    // Cria o modelo com a configuraÃ§Ã£o otimizada para chat
    println!("ğŸ§  Inicializando modelo Mini-GPT...");
    let model = MiniGPT::new(config, device).map_err(|e| anyhow::anyhow!("{}", e))?;
    println!("âœ… Modelo carregado com {} parÃ¢metros", model.num_parameters());
    
    // ğŸ¨ **ETAPA 6: APRESENTAÃ‡ÃƒO DA INTERFACE**
    // Mostra informaÃ§Ãµes do sistema e comandos disponÃ­veis
    println!("\nğŸ¤– ===== MINI-GPT CHAT INTERATIVO =====");
    if educational {
        println!("ğŸ“ **MODO EDUCACIONAL ATIVADO**");
        println!("   Logs detalhados serÃ£o exibidos para as primeiras interaÃ§Ãµes.");
        println!("   Use comandos especiais para explorar o funcionamento interno.");
    }
    println!("ğŸ’¡ Digite suas mensagens e pressione Enter");
    println!("ğŸšª Digite 'quit' ou 'exit' para sair");
    println!("\nğŸ›ï¸  **COMANDOS ESPECIAIS:**");
    if educational {
        println!("   /tokens-demo <texto> : Demonstra tokenizaÃ§Ã£o de um texto");
        println!("   /explain             : Explica o processo de geraÃ§Ã£o atual");
    }
    println!("   /temp <0.1-2.0>  : Ajusta criatividade (atual: 0.8)");
    println!("   /tokens <10-200> : Define tamanho da resposta (atual: 50)");
    println!("   /help            : Mostra esta ajuda");
    println!("   /stats           : Mostra estatÃ­sticas do modelo");
    println!("\nğŸ¯ **DICAS DE USO:**");
    println!("   â€¢ Temperatura baixa (0.1-0.5): Respostas mais conservadoras");
    println!("   â€¢ Temperatura alta (1.0-2.0): Respostas mais criativas");
    println!("   â€¢ Menos tokens: Respostas mais concisas");
    println!("   â€¢ Mais tokens: Respostas mais elaboradas\n");
    
    // ğŸ“Š **ETAPA 7: CONFIGURAÃ‡ÃƒO DOS PARÃ‚METROS DE GERAÃ‡ÃƒO**
    // Valores padrÃ£o equilibrados para uma boa experiÃªncia de chat
    let mut temperature = 0.8;  // ğŸŒ¡ï¸ Criatividade moderada
    let mut max_tokens = 50;    // ğŸ“ Respostas de tamanho mÃ©dio
    let mut conversation_history = String::new();  // ğŸ“š HistÃ³rico da conversa
    let mut interaction_count = 0;  // ğŸ“Š Contador de interaÃ§Ãµes para logs educacionais
    
    // ğŸ”„ **ETAPA 8: LOOP PRINCIPAL DE CONVERSAÃ‡ÃƒO**
    // Loop infinito que processa mensagens do usuÃ¡rio
    loop {
        // ğŸ“ **CAPTURA DE INPUT DO USUÃRIO**
        print!("\nğŸ§‘ VocÃª: ");
        io::stdout().flush()?;  // ForÃ§a exibiÃ§Ã£o do prompt
        
        let mut input = String::new();
        io::stdin().read_line(&mut input)?;
        let input = input.trim();
        
        // â­ï¸ **PULA ENTRADAS VAZIAS**
        if input.is_empty() {
            continue;
        }
        
        // ğŸšª **CONDIÃ‡Ã•ES DE SAÃDA**
        if input == "sair" || input == "exit" {
            println!("ğŸ‘‹ AtÃ© logo!");
            break;
        }
        
        // ğŸŒ¡ï¸ **COMANDO: AJUSTE DE TEMPERATURA**
        // Permite modificar criatividade do modelo em tempo real
        if input.starts_with("/temp ") {
            if let Ok(new_temp) = input[6..].parse::<f32>() {
                if (0.1..=2.0).contains(&new_temp) {
                    temperature = new_temp;
                    println!("ğŸŒ¡ï¸  Temperatura ajustada para: {:.1}", temperature);
                } else {
                    println!("âŒ Temperatura deve estar entre 0.1 e 2.0");
                }
            }
            continue;
        }
        
        // ğŸ”¢ **COMANDO: AJUSTE DE TOKENS MÃXIMOS**
        // Controla comprimento das respostas geradas
        if input.starts_with("/tokens ") {
            if let Ok(new_tokens) = input[8..].parse::<usize>() {
                if (10..=200).contains(&new_tokens) {
                    max_tokens = new_tokens;
                    println!("ğŸ”¢ Max tokens ajustado para: {}", max_tokens);
                } else {
                    println!("âŒ Tokens deve estar entre 10 e 200");
                }
            }
            continue;
        }
        
        // ğŸ“ **COMANDOS EDUCACIONAIS**
        if educational && input.starts_with("/tokens-demo ") {
            let demo_text = &input[13..];
            println!("\nğŸ” **DEMONSTRAÃ‡ÃƒO DE TOKENIZAÃ‡ÃƒO:**");
            let demo_tokens = tokenizer.encode(demo_text)?;
            logger.log_tokenization(demo_text, &demo_tokens, &tokenizer)?;
            continue;
        }
        
        if educational && input == "/explain" {
            println!("\nğŸ“ **EXPLICAÃ‡ÃƒO DO PROCESSO DE GERAÃ‡ÃƒO:**");
            println!("1. ğŸ“ **TokenizaÃ§Ã£o**: Converte texto em nÃºmeros (IDs de tokens)");
            println!("2. ğŸ”¢ **Embeddings**: Transforma IDs em vetores densos de significado");
            println!("3. ğŸ§  **Transformer**: Processa sequÃªncia com atenÃ§Ã£o e feed-forward");
            println!("4. ğŸ¯ **PrediÃ§Ã£o**: Calcula probabilidades para prÃ³ximo token");
            println!("5. ğŸ² **Amostragem**: Seleciona token baseado em temperatura");
            println!("6. ğŸ”„ **RepetiÃ§Ã£o**: Processo continua atÃ© atingir limite ou EOS\n");
            continue;
        }
        
        // â“ **COMANDO: AJUDA**
        // Mostra comandos disponÃ­veis e configuraÃ§Ãµes atuais
        if input == "/help" {
            println!("\nğŸ›ï¸  **COMANDOS DISPONÃVEIS:**");
            if educational {
                println!("   /tokens-demo <texto> : Demonstra tokenizaÃ§Ã£o de um texto");
                println!("   /explain             : Explica o processo de geraÃ§Ã£o atual");
            }
            println!("   /temp <0.1-2.0>  : Ajusta criatividade (atual: {:.1})", temperature);
            println!("   /tokens <10-200> : Define tamanho da resposta (atual: {})", max_tokens);
            println!("   /stats           : Mostra estatÃ­sticas do modelo");
            println!("   /help            : Mostra esta ajuda");
            println!("   quit/exit        : Encerra o chat");
            println!("\nğŸ¯ **CONFIGURAÃ‡Ã•ES ATUAIS:**");
            println!("   ğŸŒ¡ï¸  Temperatura: {:.1} (criatividade)", temperature);
            println!("   ğŸ“ Max Tokens: {} (tamanho da resposta)", max_tokens);
            println!("   ğŸ“š HistÃ³rico: {} caracteres", conversation_history.len());
            continue;
        }
        
        // ğŸ“Š **COMANDO: ESTATÃSTICAS**
         // Mostra informaÃ§Ãµes detalhadas sobre o modelo e conversa
         if input == "/stats" {
             println!("\nğŸ“Š **ESTATÃSTICAS DO MODELO:**");
             println!("   ğŸ§  ParÃ¢metros: {} (aprox. {:.1}K)", 
                      model.num_parameters(), 
                      model.num_parameters() as f32 / 1000.0);
             println!("   ğŸ”¤ VocabulÃ¡rio: {} tokens", tokenizer.vocab_size());
             println!("   ğŸ—ï¸  Arquitetura: 4 camadas, 4 cabeÃ§as");
             println!("   ğŸ“ Embeddings: 128 dimensÃµes");
             println!("   ğŸ“ Contexto: 64 tokens");
             println!("\nğŸ’¬ **ESTATÃSTICAS DA CONVERSA:**");
             println!("   ğŸ“š HistÃ³rico: {} caracteres", conversation_history.len());
             println!("   ğŸŒ¡ï¸  Temperatura: {:.1}", temperature);
             println!("   ğŸ“ Max Tokens: {}", max_tokens);
             if educational {
                 println!("   ğŸ“ Modo educacional: Ativo");
                 println!("   ğŸ“Š InteraÃ§Ãµes com logs: {}", interaction_count);
             }
             continue;
         }
        
        // ğŸ¯ **ETAPA 9: GERAÃ‡ÃƒO DE RESPOSTA**
        // 
        // Processa a mensagem do usuÃ¡rio e gera uma resposta contextual.
        // 
        // ## ğŸ”„ **Fluxo de GeraÃ§Ã£o:**
        // 1. **PreparaÃ§Ã£o do Contexto**: Combina histÃ³rico + nova mensagem
        // 2. **TokenizaÃ§Ã£o**: Converte texto em tokens numÃ©ricos
        // 3. **Forward Pass**: Processa atravÃ©s das camadas do modelo
        // 4. **Sampling**: Aplica temperatura para controlar criatividade
        // 5. **DecodificaÃ§Ã£o**: Converte tokens de volta para texto
        // 6. **AtualizaÃ§Ã£o**: Adiciona ao histÃ³rico para prÃ³ximas interaÃ§Ãµes
        
        interaction_count += 1;
        let start_time = Instant::now();
        
        // ğŸ“ **PREPARAÃ‡ÃƒO DO PROMPT CONTEXTUAL**
        // Combina histÃ³rico da conversa com a nova mensagem do usuÃ¡rio
        let contextual_prompt = if conversation_history.is_empty() {
            input.to_string()  // ğŸ†• Primeira mensagem
        } else {
            format!("{} {}", conversation_history, input)  // ğŸ“š Com contexto
        };
        
        // ğŸ“ **LOGGING EDUCACIONAL** (apenas para as primeiras 3 interaÃ§Ãµes)
        if educational && interaction_count <= 3 {
            println!("\nğŸ“ ===== ANÃLISE EDUCACIONAL (InteraÃ§Ã£o {}) =====", interaction_count);
            let tokens = tokenizer.encode(&contextual_prompt)?;
            logger.log_tokenization(&contextual_prompt, &tokens, &tokenizer)?;
            
            println!("\nğŸ§  **PROCESSAMENTO TRANSFORMER:**");
            println!("   â€¢ SequÃªncia de entrada: {} tokens", tokens.len());
            println!("   â€¢ Processando atravÃ©s de {} camadas...", 4); // 4 camadas conforme config
        }
        
        // ğŸ¤– **INDICADOR DE PROCESSAMENTO**
        print!("ğŸ¤– Mini-GPT: ");
        io::stdout().flush()?;  // ForÃ§a exibiÃ§Ã£o imediata
        
        // ğŸ”® **PROCESSO DE INFERÃŠNCIA NEURAL**
        // 
        // Aplica o modelo treinado para gerar uma resposta coerente
        // baseada no contexto da conversa e configuraÃ§Ãµes atuais.
        // 
        // ## âš™ï¸ **ParÃ¢metros de GeraÃ§Ã£o:**
        // - **Input**: Prompt contextual (histÃ³rico + nova mensagem)
        // - **Max Tokens**: Limite de tokens para a resposta
        // - **Tokenizer**: Sistema de codificaÃ§Ã£o/decodificaÃ§Ã£o
        // - **Temperature**: Controle de criatividade/aleatoriedade
        match model.generate(&contextual_prompt, max_tokens, &tokenizer, temperature) {
            Ok(response) => {
                // âœ… **SUCESSO: EXIBE E ATUALIZA HISTÃ“RICO**
                println!("{}", response);
                
                // ğŸ“ **LOGGING DE PREDIÃ‡ÃƒO** (apenas para as primeiras 3 interaÃ§Ãµes)
                if educational && interaction_count <= 3 {
                    let generated_tokens = tokenizer.encode(&response)?;
                    
                    let duration = start_time.elapsed();
                    println!("\nâ±ï¸ **ESTATÃSTICAS DE GERAÃ‡ÃƒO:**");
                    println!("   â€¢ Tempo total: {:.2}ms", duration.as_millis());
                    println!("   â€¢ Tokens gerados: {}", generated_tokens.len());
                    println!("   â€¢ Velocidade: {:.1} tokens/s", generated_tokens.len() as f64 / duration.as_secs_f64());
                    println!("   â€¢ Temperatura usada: {:.1}", temperature);
                    println!("\n{}", "=".repeat(60));
                }
                
                // ğŸ“š **ATUALIZAÃ‡ÃƒO DO HISTÃ“RICO DA CONVERSA**
                // MantÃ©m contexto para prÃ³ximas interaÃ§Ãµes
                conversation_history.push_str(&format!(" {} {}", input, response));
                
                // ğŸ§¹ **LIMPEZA DE HISTÃ“RICO (PREVENÃ‡ÃƒO DE OVERFLOW)**
                // MantÃ©m apenas os Ãºltimos 500 caracteres para evitar
                // que o contexto cresÃ§a indefinidamente
                if conversation_history.len() > 500 {
                    let start = conversation_history.len() - 400;
                    conversation_history = conversation_history[start..].to_string();
                }
            }
            Err(e) => {
                // âŒ **ERRO: TRATAMENTO GRACIOSO**
                println!("âŒ Erro na geraÃ§Ã£o: {}", e);
                println!("ğŸ’¡ **SugestÃµes:**");
                println!("   â€¢ Tente um prompt mais simples");
                println!("   â€¢ Reduza o nÃºmero de tokens (/tokens <num>)");
                println!("   â€¢ Ajuste a temperatura (/temp <valor>)");
                println!("   â€¢ Verifique se o modelo foi treinado adequadamente");
            }
        }
        
        // ğŸ¨ **SEPARADOR VISUAL**
        // Adiciona espaÃ§o entre interaÃ§Ãµes para melhor legibilidade
        println!();
    }
    
    // ğŸ **FINALIZAÃ‡ÃƒO GRACOSA**
    // Retorna sucesso apÃ³s saÃ­da do loop principal
    Ok(())
}