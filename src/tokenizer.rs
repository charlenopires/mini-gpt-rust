//! ğŸ”¤ **TOKENIZAÃ‡ÃƒO BPE (BYTE PAIR ENCODING)**
//! 
//! Este mÃ³dulo implementa o algoritmo BPE, uma tÃ©cnica fundamental para
//! converter texto em sequÃªncias numÃ©ricas que modelos de linguagem podem processar.
//! 
//! ## ğŸ§© **O Problema da TokenizaÃ§Ã£o:**
//! 
//! ### ğŸ¤” **Por que nÃ£o usar caracteres individuais?**
//! - **SequÃªncias muito longas**: "OlÃ¡" = [O, l, Ã¡] = 3 tokens
//! - **Perda de significado**: Palavras sÃ£o quebradas arbitrariamente
//! - **IneficiÃªncia**: Modelos precisam aprender padrÃµes bÃ¡sicos
//! 
//! ### ğŸ¤” **Por que nÃ£o usar palavras completas?**
//! - **VocabulÃ¡rio gigantesco**: MilhÃµes de palavras possÃ­veis
//! - **Palavras raras**: "antidisestablishmentarianism" seria UNK
//! - **FlexÃµes**: "correr", "correndo", "correu" = tokens diferentes
//! 
//! ## ğŸ¯ **A SoluÃ§Ã£o BPE:**
//! 
//! ### ğŸ“š **Analogia da Biblioteca:**
//! Imagine organizar uma biblioteca:
//! - **NÃ­vel 1**: Letras individuais (a, b, c...)
//! - **NÃ­vel 2**: SÃ­labas comuns ("Ã§Ã£o", "mente", "pre"...)
//! - **NÃ­vel 3**: Palavras frequentes ("que", "para", "com"...)
//! - **NÃ­vel 4**: ExpressÃµes comuns ("por favor", "muito obrigado"...)
//! 
//! BPE constrÃ³i esse "sistema de organizaÃ§Ã£o" automaticamente!
//! 
//! ### ğŸ”„ **Algoritmo BPE em Etapas:**
//! 
//! #### 1ï¸âƒ£ **InicializaÃ§Ã£o:**
//! ```text
//! Texto: "hello world"
//! Tokens: ["h", "e", "l", "l", "o", " ", "w", "o", "r", "l", "d"]
//! ```
//! 
//! #### 2ï¸âƒ£ **Contagem de Pares:**
//! ```text
//! "he": 1 vez
//! "el": 1 vez  
//! "ll": 1 vez
//! "lo": 1 vez
//! "o ": 1 vez
//! " w": 1 vez
//! "wo": 1 vez
//! "or": 1 vez
//! "rl": 1 vez
//! "ld": 1 vez
//! ```
//! 
//! #### 3ï¸âƒ£ **Merge do Par Mais Frequente:**
//! Se "ll" fosse o mais frequente:
//! ```text
//! Antes: ["h", "e", "l", "l", "o", " ", "w", "o", "r", "l", "d"]
//! Depois: ["h", "e", "ll", "o", " ", "w", "o", "r", "l", "d"]
//! ```
//! 
//! #### 4ï¸âƒ£ **RepetiÃ§Ã£o:**
//! Continue atÃ© atingir o tamanho de vocabulÃ¡rio desejado!
//! 
//! ## ğŸ—ï¸ **Vantagens do BPE:**
//! 
//! ### âœ… **EficiÃªncia:**
//! - **VocabulÃ¡rio controlado**: Tamanho fixo (ex: 50k tokens)
//! - **SequÃªncias menores**: "tokenizaÃ§Ã£o" â†’ ["token", "izaÃ§Ã£o"]
//! - **Sem palavras desconhecidas**: Qualquer texto pode ser tokenizado
//! 
//! ### âœ… **Flexibilidade:**
//! - **MultilÃ­ngue**: Funciona para qualquer idioma
//! - **DomÃ­nio-especÃ­fico**: Aprende jargÃµes tÃ©cnicos automaticamente
//! - **Subpalavras**: Captura prefixos, sufixos, radicais
//! 
//! ### âœ… **Qualidade:**
//! - **Preserva significado**: "unhappy" â†’ ["un", "happy"]
//! - **EficiÃªncia computacional**: Menos tokens = menos processamento
//! - **GeneralizaÃ§Ã£o**: Modelos aprendem padrÃµes morfolÃ³gicos
//! 
//! ## ğŸ­ **Exemplo PrÃ¡tico:**
//! 
//! ### ğŸ“ **Texto Original:**
//! ```
//! "Eu amo programaÃ§Ã£o em Rust!"
//! ```
//! 
//! ### ğŸ”¤ **ApÃ³s TokenizaÃ§Ã£o BPE:**
//! ```
//! ["Eu", " amo", " program", "aÃ§Ã£o", " em", " Rust", "!"]
//! ```
//! 
//! ### ğŸ”¢ **IDs NumÃ©ricos:**
//! ```
//! [156, 892, 1247, 3891, 234, 5672, 33]
//! ```
//! 
//! ### ğŸ§  **Por que essa divisÃ£o?**
//! - **"Eu"**: Palavra comum, tem seu prÃ³prio token
//! - **" amo"**: Verbo frequente com espaÃ§o
//! - **"program"**: Radical comum em "programaÃ§Ã£o", "programar", etc.
//! - **"aÃ§Ã£o"**: Sufixo comum em portuguÃªs
//! - **" em"**: PreposiÃ§Ã£o frequente
//! - **" Rust"**: Nome prÃ³prio, aprendido como unidade
//! - **"!"**: PontuaÃ§Ã£o comum
//! 
//! ## ğŸ”„ **Processo de Encoding/Decoding:**
//! 
//! ### ğŸ“¥ **ENCODING (Texto â†’ NÃºmeros):**
//! ```text
//! 1. Dividir texto em caracteres
//! 2. Aplicar merges aprendidos (do mais frequente ao menos frequente)
//! 3. Converter tokens para IDs usando vocabulÃ¡rio
//! 4. Retornar sequÃªncia de nÃºmeros
//! ```
//! 
//! ### ğŸ“¤ **DECODING (NÃºmeros â†’ Texto):**
//! ```text
//! 1. Converter IDs para tokens usando vocabulÃ¡rio reverso
//! 2. Concatenar tokens
//! 3. Tratar espaÃ§os e caracteres especiais
//! 4. Retornar texto original
//! ```
//! 
//! ## ğŸ¯ **Tokens Especiais:**
//! 
//! ### ğŸ **EOS (End of Sequence):**
//! - **FunÃ§Ã£o**: Marca o fim de uma sequÃªncia
//! - **Uso**: Permite ao modelo saber quando parar de gerar
//! - **Exemplo**: "OlÃ¡ mundo<EOS>"
//! 
//! ### ğŸ”¤ **UNK (Unknown):**
//! - **FunÃ§Ã£o**: Representa tokens desconhecidos
//! - **Uso**: Fallback para caracteres nÃ£o vistos no treinamento
//! - **Exemplo**: Emojis raros, sÃ­mbolos especiais
//! 
//! ### ğŸ“ **PAD (Padding):**
//! - **FunÃ§Ã£o**: Preenche sequÃªncias para ter o mesmo tamanho
//! - **Uso**: Permite processamento em lotes (batches)
//! - **Exemplo**: ["Oi", "<PAD>", "<PAD>"] para igualar tamanhos
//! 
//! ## ğŸ§® **MatemÃ¡tica da TokenizaÃ§Ã£o:**
//! 
//! ### ğŸ“Š **FrequÃªncia de Pares:**
//! ```
//! freq(pair) = Î£ count(pair, word) Ã— freq(word)
//! ```
//! 
//! ### ğŸ¯ **CritÃ©rio de Merge:**
//! ```
//! best_pair = argmax(freq(pair)) for all pairs
//! ```
//! 
//! ### ğŸ“ **EficiÃªncia de CompressÃ£o:**
//! ```
//! compression_ratio = original_chars / final_tokens
//! ```
//! 
//! ## ğŸš€ **OtimizaÃ§Ãµes Implementadas:**
//! 
//! ### âš¡ **Cache de Merges:**
//! - Armazena resultados de merges jÃ¡ computados
//! - Evita recomputaÃ§Ã£o desnecessÃ¡ria
//! - Acelera encoding de textos similares
//! 
//! ### ğŸ§  **VocabulÃ¡rio Reverso:**
//! - HashMap para lookup O(1) durante decoding
//! - Evita busca linear no vocabulÃ¡rio
//! - Essencial para geraÃ§Ã£o de texto rÃ¡pida
//! 
//! ### ğŸ“ˆ **Processamento Incremental:**
//! - Aplica merges em ordem de frequÃªncia
//! - Permite interrupÃ§Ã£o e retomada do treinamento
//! - Facilita debugging e anÃ¡lise
//! ```text
//! "O gato subiu no telhado. O gato desceu do telhado."
//! ```
//! 
//! ### ğŸ”¤ **ApÃ³s Treinamento BPE:**
//! ```text
//! VocabulÃ¡rio aprendido:
//! - Caracteres: ["O", " ", "g", "a", "t", "o", "s", "u", "b", "i", ...]
//! - SÃ­labas: ["ga", "to", "su", "bi", "te", "lh", "ad", "o."]
//! - Palavras: ["gato", "telhado"]
//! ```
//! 
//! ### ğŸ¯ **TokenizaÃ§Ã£o Final:**
//! ```text
//! ["O", " ", "gato", " ", "su", "bi", "u", " ", "no", " ", "telhado", ".", ...]
//! ```

use std::collections::HashMap;
use anyhow::Result;

/// ğŸ”¤ **TOKENIZADOR BPE: CONVERSOR INTELIGENTE DE TEXTO**
/// 
/// Esta estrutura implementa o algoritmo Byte Pair Encoding (BPE),
/// que aprende automaticamente como dividir texto em unidades
/// significativas (tokens) para processamento por modelos de linguagem.
/// 
/// ## ğŸ§  **Componentes Principais:**
/// 
/// ### ğŸ“š **VocabulÃ¡rio Bidirecional:**
/// - `vocab`: String â†’ ID (para codificaÃ§Ã£o)
/// - `reverse_vocab`: ID â†’ String (para decodificaÃ§Ã£o)
/// 
/// ### ğŸ”— **Regras de Merge:**
/// - `merges`: Lista ordenada de pares que devem ser combinados
/// - Aplicadas sequencialmente durante tokenizaÃ§Ã£o
/// 
/// ### âš™ï¸ **ConfiguraÃ§Ã£o:**
/// - `vocab_size`: Tamanho mÃ¡ximo do vocabulÃ¡rio
/// 
/// ## ğŸ¯ **Tokens Especiais:**
/// - `<PAD>` (ID: 0): Preenchimento para sequÃªncias de tamanhos diferentes
/// - `<UNK>` (ID: 1): Tokens desconhecidos (raramente usado em BPE)
/// - `<BOS>` (ID: 2): InÃ­cio de sequÃªncia (Begin of Sentence)
/// - `<EOS>` (ID: 3): Fim de sequÃªncia (End of Sentence)
pub struct BPETokenizer {
    /// ğŸ“– Mapeamento de tokens (strings) para IDs numÃ©ricos
    /// Usado durante a codificaÃ§Ã£o: texto â†’ nÃºmeros
    vocab: HashMap<String, usize>,
    
    /// ğŸ”„ Mapeamento reverso de IDs para tokens
    /// Usado durante a decodificaÃ§Ã£o: nÃºmeros â†’ texto
    reverse_vocab: HashMap<usize, String>,
    
    /// ğŸ”— Lista ordenada de regras de merge aprendidas
    /// Cada elemento Ã© um par (token1, token2) que deve ser combinado
    /// A ordem importa: merges anteriores tÃªm prioridade
    merges: Vec<(String, String)>,
    
    /// ğŸ“ Tamanho mÃ¡ximo do vocabulÃ¡rio
    /// Controla quando parar o treinamento BPE
    vocab_size: usize,
}

impl BPETokenizer {
    /// ğŸ—ï¸ **CONSTRUTOR: INICIALIZANDO O TOKENIZADOR BPE**
    /// 
    /// Cria uma nova instÃ¢ncia do tokenizador BPE com vocabulÃ¡rio vazio
    /// e tokens especiais prÃ©-definidos. O tokenizador estÃ¡ pronto para
    /// treinamento, mas ainda nÃ£o pode tokenizar texto real.
    /// 
    /// ## ğŸ¯ **ParÃ¢metros:**
    /// - `vocab_size`: Tamanho mÃ¡ximo do vocabulÃ¡rio final
    ///   - TÃ­pico: 30k-50k para modelos pequenos
    ///   - GPT-2: 50,257 tokens
    ///   - BERT: 30,522 tokens
    /// 
    /// ## ğŸ”§ **InicializaÃ§Ã£o:**
    /// 
    /// ### ğŸ·ï¸ **Tokens Especiais (IDs Fixos):**
    /// - `<PAD>` (0): Preenchimento para batches de tamanhos diferentes
    /// - `<UNK>` (1): Tokens desconhecidos (backup, raramente usado)
    /// - `<BOS>` (2): Marcador de inÃ­cio de sequÃªncia
    /// - `<EOS>` (3): Marcador de fim de sequÃªncia
    /// 
    /// ### ğŸ“š **Estruturas Inicializadas:**
    /// - **VocabulÃ¡rio vazio**: Pronto para aprender tokens
    /// - **Mapeamento reverso**: Para decodificaÃ§Ã£o eficiente
    /// - **Lista de merges vazia**: SerÃ¡ preenchida durante treinamento
    /// 
    /// ## âœ… **Estado PÃ³s-ConstruÃ§Ã£o:**
    /// ```text
    /// VocabulÃ¡rio: 4 tokens especiais
    /// Merges: 0 regras
    /// Pronto para: Treinamento
    /// NÃ£o pronto para: TokenizaÃ§Ã£o de texto real
    /// ```
    pub fn new(vocab_size: usize) -> Result<Self> {
        let mut vocab = HashMap::new();
        let mut reverse_vocab = HashMap::new();
        
        // ğŸ·ï¸ **TOKENS ESPECIAIS COM IDS FIXOS**
        // Estes IDs sÃ£o padronizados e devem ser consistentes
        // entre diferentes instÃ¢ncias do tokenizador
        vocab.insert("<PAD>".to_string(), 0);  // Preenchimento
        vocab.insert("<UNK>".to_string(), 1);  // Desconhecido
        vocab.insert("<BOS>".to_string(), 2);  // InÃ­cio de sequÃªncia
        vocab.insert("<EOS>".to_string(), 3);  // Fim de sequÃªncia
        
        // ğŸ”„ **CONSTRUÃ‡ÃƒO DO MAPEAMENTO REVERSO**
        // Permite decodificaÃ§Ã£o eficiente: ID â†’ Token
        for (token, id) in &vocab {
            reverse_vocab.insert(*id, token.clone());
        }
        
        Ok(Self {
            vocab,
            reverse_vocab,
            merges: Vec::new(),
            vocab_size,
        })
    }
    
    /// ğŸ“ **TREINAMENTO BPE: APRENDENDO A TOKENIZAR**
    /// 
    /// Este Ã© o coraÃ§Ã£o do algoritmo BPE! Analisa um corpus de texto
    /// e aprende automaticamente como dividir palavras em subunidades
    /// significativas, construindo um vocabulÃ¡rio otimizado.
    /// 
    /// ## ğŸ”„ **Algoritmo em Etapas:**
    /// 
    /// ### 1ï¸âƒ£ **InicializaÃ§Ã£o com Caracteres:**
    /// ```text
    /// Texto: "hello world"
    /// Palavras: [["h","e","l","l","o"], ["w","o","r","l","d"]]
    /// VocabulÃ¡rio inicial: {"h":4, "e":5, "l":6, "o":7, ...}
    /// ```
    /// 
    /// ### 2ï¸âƒ£ **Loop Principal de Aprendizado:**
    /// ```text
    /// Enquanto vocabulÃ¡rio < tamanho_desejado:
    ///   a) Conta frequÃªncia de todos os pares adjacentes
    ///   b) Encontra o par mais frequente
    ///   c) Cria novo token combinando o par
    ///   d) Atualiza todas as palavras com o novo merge
    ///   e) Adiciona regra de merge Ã  lista
    /// ```
    /// 
    /// ### 3ï¸âƒ£ **Exemplo de EvoluÃ§Ã£o:**
    /// ```text
    /// IteraÃ§Ã£o 0: ["h","e","l","l","o"] 
    /// Par "ll" Ã© mais frequente â†’ merge
    /// IteraÃ§Ã£o 1: ["h","e","ll","o"]
    /// Par "he" Ã© mais frequente â†’ merge  
    /// IteraÃ§Ã£o 2: ["he","ll","o"]
    /// Continue atÃ© vocab_size...
    /// ```
    /// 
    /// ## ğŸ¯ **ParÃ¢metros:**
    /// - `text`: Corpus de treinamento (quanto maior, melhor)
    /// 
    /// ## ğŸ“Š **Complexidade:**
    /// - **Tempo**: O(n Ã— vocab_size) onde n = tamanho do texto
    /// - **EspaÃ§o**: O(vocab_size + unique_chars)
    /// 
    /// ## âš ï¸ **ConsideraÃ§Ãµes:**
    /// - **Qualidade do corpus**: Deve ser representativo do domÃ­nio
    /// - **Tamanho**: Corpus pequeno = vocabulÃ¡rio subÃ³timo
    /// - **Diversidade**: Textos variados = melhor generalizaÃ§Ã£o
    pub fn train(&mut self, text: &str) -> Result<()> {
        println!("ğŸ”§ Treinando tokenizador BPE...");
        
        // 1ï¸âƒ£ **FASE DE INICIALIZAÃ‡ÃƒO: CARACTERES ÃšNICOS**
        // Converte texto em palavras representadas como sequÃªncias de caracteres
        // Cada palavra mantÃ©m sua frequÃªncia no corpus
        let mut word_freqs = self.get_word_frequencies(text);
        
        // ğŸ“š **CONSTRUÃ‡ÃƒO DO VOCABULÃRIO BASE**
        // Adiciona todos os caracteres Ãºnicos encontrados no texto
        // Estes formam a "base" sobre a qual BPE construirÃ¡
        for word in word_freqs.keys() {
            for token in word {
                if !self.vocab.contains_key(token) {
                    let id = self.vocab.len();
                    self.vocab.insert(token.clone(), id);
                    self.reverse_vocab.insert(id, token.clone());
                }
            }
        }
        
        // 2ï¸âƒ£ **LOOP PRINCIPAL: APRENDIZADO ITERATIVO DE MERGES**
        // Continua atÃ© atingir o tamanho de vocabulÃ¡rio desejado
        // Cada iteraÃ§Ã£o aprende um novo "padrÃ£o" no texto
        while self.vocab.len() < self.vocab_size {
            // ğŸ” **ANÃLISE DE FREQUÃŠNCIAS**
            // Conta quantas vezes cada par de tokens adjacentes aparece
            let pair_freqs = self.get_pair_frequencies(&word_freqs);
            
            // ğŸ›‘ **CONDIÃ‡ÃƒO DE PARADA**
            // Se nÃ£o hÃ¡ mais pares para mergir, para o treinamento
            if pair_freqs.is_empty() {
                break;
            }
            
            // ğŸ† **SELEÃ‡ÃƒO DO MELHOR PAR**
            // O par mais frequente Ã© o mais "Ãºtil" para compressÃ£o
            let best_pair = pair_freqs
                .iter()
                .max_by_key(|(_, freq)| *freq)
                .map(|(pair, _)| pair.clone())
                .unwrap();
            
            // ğŸ”— **CRIAÃ‡ÃƒO DO NOVO TOKEN**
            // Combina os dois tokens do par mais frequente em um Ãºnico token
            let new_token = format!("{}{}", best_pair.0, best_pair.1);
            let id = self.vocab.len();
            
            // ğŸ“ **REGISTRO NO VOCABULÃRIO**
            // Adiciona o novo token aos mapeamentos bidirecionais
            self.vocab.insert(new_token.clone(), id);
            self.reverse_vocab.insert(id, new_token);
            
            // ğŸ“‹ **REGISTRO DA REGRA DE MERGE**
            // Salva a regra para aplicar durante tokenizaÃ§Ã£o
            self.merges.push(best_pair.clone());
            
            // ğŸ”„ **ATUALIZAÃ‡ÃƒO DO CORPUS**
            // Aplica o novo merge a todas as palavras do corpus
            // Isso reduz a fragmentaÃ§Ã£o e prepara para prÃ³xima iteraÃ§Ã£o
            word_freqs = self.apply_merge(&word_freqs, &best_pair);
            
            // ğŸ“Š **PROGRESSO DO TREINAMENTO**
            // Mostra progresso a cada 100 tokens aprendidos
            if self.vocab.len() % 100 == 0 {
                println!("  VocabulÃ¡rio: {} tokens", self.vocab.len());
            }
        }
        
        println!("âœ… Tokenizador treinado! VocabulÃ¡rio: {} tokens", self.vocab.len());
        Ok(())
    }
    
    /// ğŸ“Š **ANÃLISE DE FREQUÃŠNCIAS DE PALAVRAS**
    /// 
    /// Converte o texto bruto em uma representaÃ§Ã£o estruturada onde
    /// cada palavra Ã© uma sequÃªncia de caracteres individuais,
    /// mantendo a contagem de quantas vezes cada palavra aparece.
    /// 
    /// ## ğŸ”„ **Processo:**
    /// ```text
    /// Input: "hello world hello"
    /// Output: {
    ///   ["h","e","l","l","o"]: 2,
    ///   ["w","o","r","l","d"]: 1
    /// }
    /// ```
    /// 
    /// ## ğŸ¯ **Por que caracteres individuais?**
    /// - **Base para BPE**: ComeÃ§amos com unidades atÃ´micas
    /// - **Universalidade**: Funciona para qualquer idioma/script
    /// - **Flexibilidade**: Permite aprender qualquer padrÃ£o
    fn get_word_frequencies(&self, text: &str) -> HashMap<Vec<String>, usize> {
        let mut freqs = HashMap::new();
        
        // ğŸ”¤ **TOKENIZAÃ‡ÃƒO INICIAL EM CARACTERES**
        // Cada palavra Ã© dividida em caracteres individuais
        // EspaÃ§os em branco separam palavras
        for word in text.split_whitespace() {
            let chars: Vec<String> = word.chars().map(|c| c.to_string()).collect();
            *freqs.entry(chars).or_insert(0) += 1;
        }
        
        freqs
    }
    
    /// ğŸ” **ANÃLISE DE FREQUÃŠNCIAS DE PARES ADJACENTES**
    /// 
    /// Para cada palavra no corpus, examina todos os pares de tokens
    /// adjacentes e conta quantas vezes cada par aparece. Esta Ã© a
    /// informaÃ§Ã£o crucial que o BPE usa para decidir quais merges fazer.
    /// 
    /// ## ğŸ”„ **Processo:**
    /// ```text
    /// Palavra: ["h", "e", "l", "l", "o"] (freq: 2)
    /// Pares: ("h","e"), ("e","l"), ("l","l"), ("l","o")
    /// ContribuiÃ§Ã£o: cada par ganha +2 na contagem
    /// ```
    /// 
    /// ## ğŸ¯ **EstratÃ©gia:**
    /// - **Pares adjacentes**: SÃ³ considera tokens lado a lado
    /// - **PonderaÃ§Ã£o por frequÃªncia**: Palavras comuns tÃªm mais peso
    /// - **Busca exaustiva**: Examina todos os pares possÃ­veis
    /// 
    /// ## ğŸ“Š **Complexidade:**
    /// - **Tempo**: O(total_chars_in_corpus)
    /// - **EspaÃ§o**: O(unique_pairs)
    fn get_pair_frequencies(&self, word_freqs: &HashMap<Vec<String>, usize>) -> HashMap<(String, String), usize> {
        let mut pair_freqs = HashMap::new();
        
        // ğŸ” **VARREDURA DE TODAS AS PALAVRAS**
        for (word, freq) in word_freqs {
            // ğŸ‘¥ **ANÃLISE DE PARES ADJACENTES**
            // Para cada posiÃ§Ã£o na palavra, forma um par com o prÃ³ximo token
            for i in 0..word.len() - 1 {
                let pair = (word[i].clone(), word[i + 1].clone());
                // ğŸ“ˆ **ACUMULAÃ‡ÃƒO PONDERADA**
                // Adiciona a frequÃªncia da palavra Ã  contagem do par
                *pair_freqs.entry(pair).or_insert(0) += freq;
            }
        }
        
        pair_freqs
    }
    
    /// ğŸ”„ **APLICAÃ‡ÃƒO DE MERGE AO CORPUS**
    /// 
    /// Aplica uma regra de merge especÃ­fica a todas as palavras do corpus,
    /// substituindo todas as ocorrÃªncias do par especificado pelo novo token.
    /// Esta Ã© a operaÃ§Ã£o que "ensina" o vocabulÃ¡rio a reconhecer padrÃµes.
    /// 
    /// ## ğŸ¯ **Processo:**
    /// ```text
    /// Antes: ["h", "e", "l", "l", "o"]
    /// Merge: ("l", "l") â†’ "ll"
    /// Depois: ["h", "e", "ll", "o"]
    /// ```
    /// 
    /// ## ğŸ” **Algoritmo:**
    /// 1. **Varredura**: Examina cada palavra token por token
    /// 2. **DetecÃ§Ã£o**: Procura pelo par especÃ­fico em posiÃ§Ãµes adjacentes
    /// 3. **SubstituiÃ§Ã£o**: Substitui par encontrado pelo token merged
    /// 4. **PreservaÃ§Ã£o**: MantÃ©m outros tokens inalterados
    /// 
    /// ## âš¡ **OtimizaÃ§Ã£o:**
    /// - **Busca gulosa**: Primeira ocorrÃªncia encontrada Ã© merged
    /// - **Salto duplo**: ApÃ³s merge, pula 2 posiÃ§Ãµes (par consumido)
    /// - **PreservaÃ§Ã£o de frequÃªncia**: MantÃ©m contagens originais
    fn apply_merge(&self, word_freqs: &HashMap<Vec<String>, usize>, pair: &(String, String)) -> HashMap<Vec<String>, usize> {
        let mut new_freqs = HashMap::new();
        
        // ğŸ”— **CRIAÃ‡ÃƒO DO TOKEN MERGED**
        // Combina os dois tokens do par em um Ãºnico token
        let merged = format!("{}{}", pair.0, pair.1);
        
        // ğŸ” **PROCESSAMENTO DE CADA PALAVRA**
        for (word, freq) in word_freqs {
            let mut new_word = Vec::new();
            let mut i = 0;
            
            // ğŸ‘€ **VARREDURA TOKEN POR TOKEN**
            while i < word.len() {
                // ğŸ¯ **DETECÃ‡ÃƒO DO PAR ALVO**
                // Verifica se a posiÃ§Ã£o atual e prÃ³xima formam o par desejado
                if i < word.len() - 1 && word[i] == pair.0 && word[i + 1] == pair.1 {
                    // âœ… **MERGE EXECUTADO**
                    // Substitui o par pelo token combinado
                    new_word.push(merged.clone());
                    i += 2; // Pula ambos os tokens do par
                } else {
                    // ğŸ“‹ **PRESERVAÃ‡ÃƒO DE TOKEN**
                    // Token nÃ£o faz parte do par, mantÃ©m como estÃ¡
                    new_word.push(word[i].clone());
                    i += 1;
                }
            }
            
            // ğŸ“Š **PRESERVAÃ‡ÃƒO DE FREQUÃŠNCIA**
            // A nova palavra mantÃ©m a mesma frequÃªncia da original
            new_freqs.insert(new_word, *freq);
        }
        
        new_freqs
    }
    
    /// ğŸ”¢ **CODIFICAÃ‡ÃƒO: TEXTO â†’ NÃšMEROS**
    /// 
    /// Converte texto em uma sequÃªncia de IDs numÃ©ricos que o modelo
    /// pode processar. Aplica todas as regras de merge aprendidas durante
    /// o treinamento para produzir a tokenizaÃ§Ã£o mais eficiente.
    /// 
    /// ## ğŸ”„ **Processo Completo:**
    /// 
    /// ### 1ï¸âƒ£ **PreparaÃ§Ã£o:**
    /// ```text
    /// Input: "hello world"
    /// Adiciona: <BOS> + texto + <EOS>
    /// ```
    /// 
    /// ### 2ï¸âƒ£ **TokenizaÃ§Ã£o por Palavra:**
    /// ```text
    /// "hello" â†’ ["h", "e", "l", "l", "o"]
    /// "world" â†’ ["w", "o", "r", "l", "d"]
    /// ```
    /// 
    /// ### 3ï¸âƒ£ **AplicaÃ§Ã£o de Merges:**
    /// ```text
    /// Merge 1: ("l", "l") â†’ "ll"
    /// "hello" â†’ ["h", "e", "ll", "o"]
    /// 
    /// Merge 2: ("he", "ll") â†’ "hell"
    /// "hello" â†’ ["hell", "o"]
    /// ```
    /// 
    /// ### 4ï¸âƒ£ **ConversÃ£o para IDs:**
    /// ```text
    /// ["<BOS>", "hell", "o", " ", "world", "<EOS>"]
    /// â†’ [2, 1547, 78, 32, 2134, 3]
    /// ```
    /// 
    /// ## ğŸ¯ **CaracterÃ­sticas:**
    /// - **DeterminÃ­stica**: Mesmo texto = mesma tokenizaÃ§Ã£o
    /// - **Eficiente**: Usa tokens mais longos quando possÃ­vel
    /// - **Robusta**: Nunca falha (usa <UNK> como fallback)
    /// - **Estruturada**: Sempre inclui marcadores BOS/EOS
    /// 
    /// ## ğŸ“Š **Complexidade:**
    /// - **Tempo**: O(text_length Ã— num_merges)
    /// - **EspaÃ§o**: O(text_length)
    pub fn encode(&self, text: &str) -> Result<Vec<usize>> {
        let mut tokens = Vec::new();
        
        // ğŸ **MARCADOR DE INÃCIO**
        // Sinaliza ao modelo onde a sequÃªncia comeÃ§a
        tokens.push(self.vocab["<BOS>"]);
        
        // ğŸ”¤ **PROCESSAMENTO PALAVRA POR PALAVRA**
        for word in text.split_whitespace() {
            // ğŸ“ **INICIALIZAÃ‡ÃƒO EM CARACTERES**
            // Cada palavra comeÃ§a como sequÃªncia de caracteres individuais
            let mut chars: Vec<String> = word.chars().map(|c| c.to_string()).collect();
            
            // ğŸ”„ **APLICAÃ‡ÃƒO SEQUENCIAL DE MERGES**
            // Aplica todas as regras aprendidas na ordem correta
            // Ordem importa: merges anteriores tÃªm prioridade
            for merge in &self.merges {
                chars = self.apply_merge_to_word(chars, merge);
            }
            
            // ğŸ”¢ **CONVERSÃƒO PARA IDS NUMÃ‰RICOS**
            // Transforma tokens (strings) em IDs que o modelo entende
            for token in chars {
                let id = self.vocab.get(&token).unwrap_or(&self.vocab["<UNK>"]);
                tokens.push(*id);
            }
        }
        
        // ğŸ **MARCADOR DE FIM**
        // Sinaliza ao modelo onde a sequÃªncia termina
        tokens.push(self.vocab["<EOS>"]);
        
        Ok(tokens)
    }
    
    /// ğŸ”— **APLICAÃ‡ÃƒO DE MERGE EM PALAVRA INDIVIDUAL**
    /// 
    /// Aplica uma regra de merge especÃ­fica a uma palavra tokenizada,
    /// substituindo pares adjacentes pelo token combinado.
    /// 
    /// ## ğŸ¯ **Exemplo PrÃ¡tico:**
    /// ```text
    /// Palavra: ["h", "e", "l", "l", "o"]
    /// Merge: ("l", "l") â†’ "ll"
    /// 
    /// Processo:
    /// 1. "h" â†’ nÃ£o match, mantÃ©m "h"
    /// 2. "e" â†’ nÃ£o match, mantÃ©m "e"
    /// 3. "l", "l" â†’ MATCH! substitui por "ll"
    /// 4. "o" â†’ nÃ£o match, mantÃ©m "o"
    /// 
    /// Resultado: ["h", "e", "ll", "o"]
    /// ```
    /// 
    /// ## ğŸ” **Algoritmo de Varredura:**
    /// - **Busca Gulosa**: Primeira ocorrÃªncia encontrada Ã© substituÃ­da
    /// - **Salto Duplo**: ApÃ³s merge, pula 2 posiÃ§Ãµes (par consumido)
    /// - **PreservaÃ§Ã£o**: Tokens nÃ£o-matching sÃ£o mantidos intactos
    /// - **Ordem**: Processa da esquerda para direita
    /// 
    /// ## âš¡ **OtimizaÃ§Ãµes:**
    /// - VerificaÃ§Ã£o de bounds antes do acesso
    /// - Clone mÃ­nimo (apenas quando necessÃ¡rio)
    /// - PrÃ©-formataÃ§Ã£o do token merged
    /// 
    /// ## ğŸ“Š **Complexidade:**
    /// - **Tempo**: O(word_length)
    /// - **EspaÃ§o**: O(word_length)
    fn apply_merge_to_word(&self, word: Vec<String>, merge: &(String, String)) -> Vec<String> {
        // ğŸ¯ **PRÃ‰-FORMATAÃ‡ÃƒO DO TOKEN MERGED**
        // Evita concatenaÃ§Ã£o repetida durante o loop
        let merged = format!("{}{}", merge.0, merge.1);
        let mut result = Vec::new();
        let mut i = 0;
        
        // ğŸ” **VARREDURA SEQUENCIAL COM DETECÃ‡ÃƒO DE PARES**
        while i < word.len() {
            // ğŸ¯ **DETECÃ‡ÃƒO DE PAR ADJACENTE**
            // Verifica se posiÃ§Ã£o atual + prÃ³xima formam o par target
            if i < word.len() - 1 && word[i] == merge.0 && word[i + 1] == merge.1 {
                // âœ… **MERGE ENCONTRADO**
                // Substitui o par pelo token combinado
                result.push(merged.clone());
                i += 2; // Pula ambos os tokens do par
            } else {
                // â¡ï¸ **SEM MATCH**
                // Preserva o token atual e avanÃ§a
                result.push(word[i].clone());
                i += 1;
            }
        }
        
        result
    }
    
    /// ğŸ”¤ **DECODIFICAÃ‡ÃƒO: NÃšMEROS â†’ TEXTO**
    /// 
    /// Converte uma sequÃªncia de IDs numÃ©ricos de volta para texto
    /// legÃ­vel, removendo tokens especiais de controle.
    /// 
    /// ## ğŸ”„ **Processo Inverso:**
    /// 
    /// ### 1ï¸âƒ£ **Input (IDs):**
    /// ```text
    /// [2, 1547, 78, 32, 2134, 3]
    /// ```
    /// 
    /// ### 2ï¸âƒ£ **Mapeamento Reverso:**
    /// ```text
    /// 2    â†’ "<BOS>"     (removido)
    /// 1547 â†’ "hell"      (mantido)
    /// 78   â†’ "o"         (mantido)
    /// 32   â†’ " "         (mantido)
    /// 2134 â†’ "world"     (mantido)
    /// 3    â†’ "<EOS>"     (removido)
    /// ```
    /// 
    /// ### 3ï¸âƒ£ **ConcatenaÃ§Ã£o:**
    /// ```text
    /// "hell" + "o" + " " + "world" = "hello world"
    /// ```
    /// 
    /// ## ğŸ¯ **CaracterÃ­sticas:**
    /// - **Filtragem**: Remove automaticamente tokens especiais
    /// - **Robusta**: Ignora IDs desconhecidos silenciosamente
    /// - **Eficiente**: ConcatenaÃ§Ã£o direta sem separadores
    /// - **Limpa**: Produz texto natural sem artefatos
    /// 
    /// ## ğŸ“Š **Complexidade:**
    /// - **Tempo**: O(num_tokens)
    /// - **EspaÃ§o**: O(output_text_length)
    pub fn decode(&self, tokens: &[usize]) -> Result<String> {
        let mut text = String::new();
        
        // ğŸ” **CONVERSÃƒO ID â†’ TOKEN**
        for &id in tokens {
            if let Some(token) = self.reverse_vocab.get(&id) {
                // ğŸš« **FILTRAGEM DE TOKENS ESPECIAIS**
                // Remove marcadores de controle (<BOS>, <EOS>, <PAD>, <UNK>)
                // MantÃ©m apenas conteÃºdo textual real
                if !token.starts_with('<') || !token.ends_with('>') {
                    text.push_str(token);
                }
            }
            // ğŸ”‡ **IDs DESCONHECIDOS SÃƒO IGNORADOS SILENCIOSAMENTE**
            // Permite decodificaÃ§Ã£o robusta mesmo com vocabulÃ¡rios parciais
        }
        
        Ok(text)
    }
    
    /// ğŸ“Š **TAMANHO DO VOCABULÃRIO**
    /// 
    /// Retorna o nÃºmero total de tokens Ãºnicos no vocabulÃ¡rio,
    /// incluindo caracteres base, merges aprendidos e tokens especiais.
    /// 
    /// ## ğŸ“ˆ **ComposiÃ§Ã£o TÃ­pica:**
    /// ```text
    /// Tokens Especiais: 4     (<PAD>, <UNK>, <BOS>, <EOS>)
    /// Caracteres Base:  ~100   (letras, nÃºmeros, sÃ­mbolos)
    /// Merges Aprendidos: N-104 (resto atÃ© vocab_size)
    /// Total:            N     (vocab_size configurado)
    /// ```
    pub fn vocab_size(&self) -> usize {
        self.vocab.len()
    }
    
    /// ğŸ **DETECTOR DE FIM DE SEQUÃŠNCIA**
    /// 
    /// Verifica se um token especÃ­fico Ã© o marcador de fim de sequÃªncia,
    /// usado para determinar quando parar a geraÃ§Ã£o de texto.
    /// 
    /// ## ğŸ¯ **Uso TÃ­pico:**
    /// ```rust
    /// while let Some(next_token) = model.generate_next() {
    ///     if tokenizer.is_eos_token(next_token) {
    ///         break; // Para a geraÃ§Ã£o
    ///     }
    ///     output.push(next_token);
    /// }
    /// ```
    pub fn is_eos_token(&self, token: usize) -> bool {
        token == self.vocab["<EOS>"]
    }
}