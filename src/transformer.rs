//! # Transformer Block: A Unidade Fundamental
//! 
//! ğŸ—ï¸ Analogia: Como um prÃ©dio Ã© feito de andares, nosso modelo
//! Ã© feito de blocos Transformer empilhados. Cada bloco processa
//! e refina a informaÃ§Ã£o antes de passar para o prÃ³ximo.

use candle_core::{DType, Device, Result, Tensor};
use candle_nn::{layer_norm, linear, LayerNorm, Linear, Module, VarBuilder};
use crate::attention::MultiHeadAttention;

/// ğŸ½ï¸ **FEED-FORWARD NETWORK: PROCESSAMENTO INDIVIDUAL**
/// 
/// ApÃ³s a "conversa coletiva" da atenÃ§Ã£o, cada token passa por
/// um processamento individual para refinar sua representaÃ§Ã£o.
/// 
/// ## ğŸ­ Analogia do Restaurante:
/// Imagine um restaurante onde:
/// - **AtenÃ§Ã£o**: GarÃ§ons conversam entre si sobre pedidos
/// - **Feed-Forward**: Cada chef prepara seu prato individualmente
/// 
/// ## ğŸ§® Arquitetura MatemÃ¡tica:
/// ```text
/// Input:  [B, T, C] = [batch, seq_len, n_embd]
///         â†“ Linear 1 (expansÃ£o 4x)
/// Hidden: [B, T, 4C] = [batch, seq_len, 4*n_embd]
///         â†“ GELU (ativaÃ§Ã£o)
/// Activated: [B, T, 4C]
///         â†“ Dropout (regularizaÃ§Ã£o)
/// Regularized: [B, T, 4C]
///         â†“ Linear 2 (contraÃ§Ã£o)
/// Output: [B, T, C] = [batch, seq_len, n_embd]
/// ```
/// 
/// ## ğŸ¯ Por que ExpansÃ£o 4x?
/// - **Capacidade**: Mais neurÃ´nios = mais expressividade
/// - **PadrÃ£o**: Estabelecido pelo paper "Attention is All You Need"
/// - **Trade-off**: BalanÃ§a performance vs. eficiÃªncia computacional
/// 
/// ## âš¡ Complexidade Computacional:
/// - **ParÃ¢metros**: 8 Ã— CÂ² (duas matrizes: Câ†’4C e 4Câ†’C)
/// - **FLOPs**: O(8 Ã— B Ã— T Ã— CÂ²) por forward pass
pub struct FeedForward {
    fc1: Linear,        // ğŸš€ Primeira camada: expansÃ£o (C â†’ 4C)
    fc2: Linear,        // ğŸ¯ Segunda camada: contraÃ§Ã£o (4C â†’ C)
    dropout: f32,       // ğŸ² Taxa de dropout para regularizaÃ§Ã£o
}

impl FeedForward {
    /// ğŸ—ï¸ **CONSTRUTOR: INICIALIZANDO A REDE FEED-FORWARD**
    /// 
    /// Cria uma rede de duas camadas com expansÃ£o intermediÃ¡ria,
    /// seguindo o padrÃ£o estabelecido pelos Transformers originais.
    /// 
    /// ## ğŸ“Š DimensÃµes das Camadas:
    /// ```text
    /// fc1: [n_embd] â†’ [4 * n_embd]  (expansÃ£o)
    /// fc2: [4 * n_embd] â†’ [n_embd]  (contraÃ§Ã£o)
    /// ```
    /// 
    /// ## ğŸ¯ ParÃ¢metros:
    /// - `n_embd`: DimensÃ£o dos embeddings
    /// - `dropout`: Taxa de dropout (0.0 a 1.0)
    /// - `vb`: Variable builder para inicializaÃ§Ã£o de pesos
    pub fn new(n_embd: usize, dropout: f32, vb: VarBuilder) -> Result<Self> {
        // ğŸ“ˆ **EXPANSÃƒO 4X: PADRÃƒO DOS TRANSFORMERS**
        // Aumenta a capacidade representacional da rede
        // Permite capturar padrÃµes mais complexos
        let hidden_dim = 4 * n_embd;
        
        // ğŸ—ï¸ **CONSTRUÃ‡ÃƒO DAS CAMADAS LINEARES**
        Ok(Self {
            // ğŸš€ Primeira camada: expande o espaÃ§o de caracterÃ­sticas
            fc1: linear(n_embd, hidden_dim, vb.pp("fc1"))?,
            
            // ğŸ¯ Segunda camada: projeta de volta ao espaÃ§o original
            fc2: linear(hidden_dim, n_embd, vb.pp("fc2"))?,
            
            // ğŸ² Taxa de dropout para regularizaÃ§Ã£o
            dropout,
        })
    }
    
    /// ğŸš€ **FORWARD PASS: PROCESSAMENTO NÃƒO-LINEAR**
    /// 
    /// Implementa o fluxo completo da rede feed-forward:
    /// Linear â†’ GELU â†’ Dropout â†’ Linear
    /// 
    /// ## ğŸ§® FÃ³rmula MatemÃ¡tica:
    /// ```text
    /// FFN(x) = Linearâ‚‚(Dropout(GELU(Linearâ‚(x))))
    /// ```
    /// 
    /// ## ğŸ­ Por que GELU em vez de ReLU?
    /// 
    /// ### ğŸ“Š **GELU vs ReLU:**
    /// ```text
    /// ReLU(x) = max(0, x)           # FunÃ§Ã£o degrau
    /// GELU(x) = x * Î¦(x)            # FunÃ§Ã£o suave
    /// 
    /// Onde Î¦(x) Ã© a CDF da distribuiÃ§Ã£o normal padrÃ£o
    /// ```
    /// 
    /// ### ğŸ¯ **Vantagens do GELU:**
    /// - **Suavidade**: Gradientes mais estÃ¡veis
    /// - **NÃ£o-monotÃ´nico**: Permite valores negativos pequenos
    /// - **ProbabilÃ­stico**: Baseado em distribuiÃ§Ãµes estatÃ­sticas
    /// - **Performance**: Melhor em modelos de linguagem
    /// 
    /// ## ğŸ”„ **Fluxo de Processamento:**
    /// ```text
    /// Input: "O gato subiu"  [B, T, C]
    ///        â†“ fc1 (expansÃ£o)
    /// Hidden: representaÃ§Ãµes expandidas [B, T, 4C]
    ///        â†“ GELU (ativaÃ§Ã£o)
    /// Active: ativaÃ§Ãµes nÃ£o-lineares [B, T, 4C]
    ///        â†“ dropout (regularizaÃ§Ã£o)
    /// Regularized: [B, T, 4C]
    ///        â†“ fc2 (contraÃ§Ã£o)
    /// Output: representaÃ§Ãµes refinadas [B, T, C]
    /// ```
    /// 
    /// ## ğŸ¯ ParÃ¢metros:
    /// - `x`: Tensor de entrada [batch_size, seq_len, n_embd]
    /// 
    /// ## ğŸ“¤ Retorna:
    /// - Tensor processado [batch_size, seq_len, n_embd]
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // ğŸš€ **PRIMEIRA CAMADA LINEAR: EXPANSÃƒO 4X**
        // Transforma [B, T, C] â†’ [B, T, 4C]
        // Aumenta a capacidade representacional para capturar padrÃµes complexos
        let x = self.fc1.forward(x)?;
        
        // âš¡ **ATIVAÃ‡ÃƒO GELU: INTRODUÃ‡ÃƒO DE NÃƒO-LINEARIDADE**
        // GELU(x) = x * Î¦(x) onde Î¦ Ã© a CDF da normal padrÃ£o
        // Permite que a rede aprenda funÃ§Ãµes nÃ£o-lineares complexas
        // Mais suave que ReLU, melhor para gradientes
        let x = x.gelu()?;
        
        // ğŸ² **DROPOUT: REGULARIZAÃ‡ÃƒO ESTOCÃSTICA**
        // Durante treinamento: zera aleatoriamente alguns neurÃ´nios
        // Previne overfitting e melhora generalizaÃ§Ã£o
        // Durante inferÃªncia: mantÃ©m todos os neurÃ´nios ativos
        let x = if self.dropout > 0.0 {
            candle_nn::ops::dropout(&x, self.dropout)?
        } else {
            x  // Sem dropout durante inferÃªncia
        };
        
        // ğŸ¯ **SEGUNDA CAMADA LINEAR: CONTRAÃ‡ÃƒO DE VOLTA**
        // Transforma [B, T, 4C] â†’ [B, T, C]
        // Projeta as representaÃ§Ãµes expandidas de volta ao espaÃ§o original
        // MantÃ©m compatibilidade dimensional com conexÃµes residuais
        self.fc2.forward(&x)
    }
}

/// ğŸ—ï¸ **TRANSFORMER BLOCK: ARQUITETURA FUNDAMENTAL**
/// 
/// O bloco Transformer Ã© a unidade bÃ¡sica que combina:
/// 1. **Multi-Head Attention**: ComunicaÃ§Ã£o entre tokens
/// 2. **Feed-Forward Network**: Processamento individual
/// 3. **Layer Normalization**: EstabilizaÃ§Ã£o do treinamento
/// 4. **Residual Connections**: Fluxo de gradientes
/// 
/// ## ğŸ­ Analogia da Orquestra:
/// Imagine uma orquestra onde:
/// - **AtenÃ§Ã£o**: MÃºsicos se coordenam entre si
/// - **Feed-Forward**: Cada mÃºsico aprimora sua parte
/// - **Layer Norm**: Maestro ajusta o volume geral
/// - **Residual**: MemÃ³ria da melodia original
/// 
/// ## ğŸ§® Arquitetura MatemÃ¡tica:
/// ```text
/// Input: x [B, T, C]
///   â†“
/// xâ‚ = x + MultiHeadAttention(LayerNorm(x))
///   â†“
/// xâ‚‚ = xâ‚ + FeedForward(LayerNorm(xâ‚))
///   â†“
/// Output: xâ‚‚ [B, T, C]
/// ```
/// 
/// ## ğŸ”„ **PRE-LN vs POST-LN:**
/// 
/// ### ğŸ“Š **Pre-LN (usado aqui):**
/// ```text
/// x = x + Attention(LayerNorm(x))
/// x = x + FFN(LayerNorm(x))
/// ```
/// 
/// ### ğŸ“Š **Post-LN (original):**
/// ```text
/// x = LayerNorm(x + Attention(x))
/// x = LayerNorm(x + FFN(x))
/// ```
/// 
/// ### âœ… **Vantagens do Pre-LN:**
/// - **Treinamento mais estÃ¡vel**: Gradientes mais suaves
/// - **ConvergÃªncia mais rÃ¡pida**: Menos explosÃ£o/desaparecimento
/// - **Menos sensÃ­vel Ã  inicializaÃ§Ã£o**: Mais robusto
/// 
/// ## âš¡ Complexidade Computacional:
/// - **ParÃ¢metros**: ~12 Ã— CÂ² (atenÃ§Ã£o + FFN + layer norms)
/// - **FLOPs**: O(12 Ã— B Ã— T Ã— CÂ² + 4 Ã— B Ã— TÂ² Ã— C) por bloco
pub struct TransformerBlock {
    attention: MultiHeadAttention,    // ğŸ¯ Mecanismo de atenÃ§Ã£o multi-cabeÃ§a
    feed_forward: FeedForward,        // ğŸ½ï¸ Rede feed-forward para processamento
    ln1: LayerNorm,                   // ğŸ“ Layer norm antes da atenÃ§Ã£o
    ln2: LayerNorm,                   // ğŸ“ Layer norm antes do feed-forward
}

impl TransformerBlock {
    /// ğŸ—ï¸ **CONSTRUTOR: INICIALIZANDO O BLOCO TRANSFORMER**
    /// 
    /// Cria um bloco completo com todos os componentes necessÃ¡rios,
    /// seguindo a arquitetura Pre-LN para melhor estabilidade.
    /// 
    /// ## ğŸ¯ ParÃ¢metros:
    /// - `n_embd`: DimensÃ£o dos embeddings (tipicamente 512, 768, 1024...)
    /// - `n_head`: NÃºmero de cabeÃ§as de atenÃ§Ã£o (tipicamente 8, 12, 16...)
    /// - `dropout`: Taxa de dropout para regularizaÃ§Ã£o (0.0 a 1.0)
    /// - `vb`: Variable builder para inicializaÃ§Ã£o de pesos
    /// 
    /// ## ğŸ“Š DistribuiÃ§Ã£o de ParÃ¢metros:
    /// ```text
    /// MultiHeadAttention: ~4 Ã— CÂ² parÃ¢metros (Q, K, V, Out)
    /// FeedForward:        ~8 Ã— CÂ² parÃ¢metros (fc1, fc2)
    /// LayerNorm (2x):     ~4 Ã— C parÃ¢metros (scale, bias)
    /// Total:              ~12 Ã— CÂ² + 4 Ã— C parÃ¢metros
    /// ```
    pub fn new(n_embd: usize, n_head: usize, dropout: f32, vb: VarBuilder) -> Result<Self> {
        Ok(Self {
            // ğŸ¯ **ATENÃ‡ÃƒO MULTI-CABEÃ‡A**
            // Permite que o modelo "preste atenÃ§Ã£o" a diferentes
            // aspectos da sequÃªncia simultaneamente
            attention: MultiHeadAttention::new(n_embd, n_head, dropout, vb.pp("attention"))?,
            
            // ğŸ½ï¸ **REDE FEED-FORWARD**
            // Processamento nÃ£o-linear individual para cada posiÃ§Ã£o
            // ExpansÃ£o 4x seguida de contraÃ§Ã£o para aumentar expressividade
            feed_forward: FeedForward::new(n_embd, dropout, vb.pp("feed_forward"))?,
            
            // ğŸ“ **LAYER NORMALIZATIONS**
            // Pre-LN: normalizaÃ§Ã£o ANTES das operaÃ§Ãµes principais
            // Estabiliza gradientes e acelera convergÃªncia
            // eps=1e-5 Ã© o padrÃ£o para estabilidade numÃ©rica
            ln1: layer_norm(n_embd, 1e-5, vb.pp("ln1"))?,  // Antes da atenÃ§Ã£o
            ln2: layer_norm(n_embd, 1e-5, vb.pp("ln2"))?,  // Antes do feed-forward
        })
    }
    
    /// ğŸš€ **FORWARD PASS: PROCESSAMENTO COMPLETO DO BLOCO**
    /// 
    /// Implementa a arquitetura Pre-LN Transformer com conexÃµes residuais:
    /// 
    /// ## ğŸ”„ **Fluxo de Processamento:**
    /// ```text
    /// Input: x [B, T, C]
    ///   â†“
    /// 1ï¸âƒ£ AtenÃ§Ã£o com Residual:
    ///    norm1 = LayerNorm(x)
    ///    attn_out = MultiHeadAttention(norm1, mask)
    ///    x = x + attn_out  # ConexÃ£o residual
    ///   â†“
    /// 2ï¸âƒ£ Feed-Forward com Residual:
    ///    norm2 = LayerNorm(x)
    ///    ffn_out = FeedForward(norm2)
    ///    x = x + ffn_out   # ConexÃ£o residual
    ///   â†“
    /// Output: x [B, T, C]
    /// ```
    /// 
    /// ## ğŸ¯ **Por que ConexÃµes Residuais?**
    /// 
    /// ### ğŸ“Š **Problema dos Gradientes:**
    /// Em redes profundas, gradientes podem:
    /// - **Desaparecer**: Ficam muito pequenos (vanishing gradients)
    /// - **Explodir**: Ficam muito grandes (exploding gradients)
    /// 
    /// ### âœ… **SoluÃ§Ã£o Residual:**
    /// ```text
    /// Sem residual:  y = F(x)
    /// Com residual:  y = x + F(x)
    /// ```
    /// 
    /// **Vantagens:**
    /// - **Gradiente direto**: âˆ‚y/âˆ‚x = 1 + âˆ‚F(x)/âˆ‚x
    /// - **Identidade preservada**: Se F(x) = 0, y = x
    /// - **Aprendizado incremental**: F(x) aprende apenas o "delta"
    /// 
    /// ## ğŸ­ **Analogia da EdiÃ§Ã£o:**
    /// Imagine editar um documento:
    /// - **Sem residual**: Reescrever tudo do zero
    /// - **Com residual**: Fazer apenas correÃ§Ãµes/melhorias
    /// 
    /// ## ğŸ” **ParÃ¢metros:**
    /// - `x`: Tensor de entrada [batch_size, seq_len, n_embd]
    /// - `mask`: MÃ¡scara causal opcional para atenÃ§Ã£o
    /// 
    /// ## ğŸ“¤ **Retorno:**
    /// - Tensor processado [batch_size, seq_len, n_embd]
            ln1: layer_norm(n_embd, 1e-5, vb.pp("ln1"))?,  // Antes da atenÃ§Ã£o
            ln2: layer_norm(n_embd, 1e-5, vb.pp("ln2"))?,  // Antes do feed-forward
        })
    }
    
    /// ğŸš€ **FORWARD PASS: PROCESSAMENTO COMPLETO DO BLOCO**
    /// 
    /// Implementa a arquitetura Pre-LN com conexÃµes residuais:
    /// 1. **AtenÃ§Ã£o com Residual**: x + Attention(LayerNorm(x))
    /// 2. **Feed-Forward com Residual**: x + FFN(LayerNorm(x))
    /// 
    /// ## ğŸ”„ **Por que ConexÃµes Residuais?**
    /// 
    /// ### ğŸ¯ **Problema do Gradiente Desaparecendo:**
    /// Em redes profundas, gradientes podem "desaparecer" durante
    /// o backpropagation, tornando o treinamento impossÃ­vel.
    /// 
    /// ### âœ… **SoluÃ§Ã£o das Residuais:**
    /// ```text
    /// âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚output Ã— (1 + âˆ‚F(x)/âˆ‚x)
    /// ```
    /// O termo "1" garante que sempre hÃ¡ um caminho direto
    /// para os gradientes fluÃ­rem, mesmo se âˆ‚F(x)/âˆ‚x â‰ˆ 0.
    /// 
    /// ## ğŸ“ **Layer Normalization:**
    /// 
    /// ### ğŸ§® **FÃ³rmula:**
    /// ```text
    /// LayerNorm(x) = Î³ Ã— (x - Î¼) / Ïƒ + Î²
    /// 
    /// Onde:
    /// Î¼ = mean(x)     # MÃ©dia da camada
    /// Ïƒ = std(x)      # Desvio padrÃ£o da camada
    /// Î³, Î²            # ParÃ¢metros aprendÃ­veis
    /// ```
    /// 
    /// ### âœ… **BenefÃ­cios:**
    /// - **Estabilidade**: Normaliza ativaÃ§Ãµes para mÃ©dia 0, std 1
    /// - **Velocidade**: Acelera convergÃªncia do treinamento
    /// - **Robustez**: Menos sensÃ­vel Ã  inicializaÃ§Ã£o de pesos
    /// 
    /// ## ğŸ¯ ParÃ¢metros:
    /// - `x`: Tensor de entrada [batch_size, seq_len, n_embd]
    /// - `mask`: MÃ¡scara causal opcional para atenÃ§Ã£o
    /// 
    /// ## ğŸ“¤ Retorna:
    /// - Tensor processado [batch_size, seq_len, n_embd]
    pub fn forward(&self, x: &Tensor, mask: Option<&Tensor>) -> Result<Tensor> {
        // ğŸ¯ **ETAPA 1: ATENÃ‡ÃƒO COM CONEXÃƒO RESIDUAL**
        // 
        // Pre-LN: Normaliza ANTES da atenÃ§Ã£o (mais estÃ¡vel que Post-LN)
        // Isso estabiliza os gradientes e acelera a convergÃªncia
        let normalized_x = self.ln1.forward(x)?;
        
        // ğŸ” **MULTI-HEAD ATTENTION**
        // Permite que o modelo "olhe" para diferentes posiÃ§Ãµes simultaneamente
        // A mÃ¡scara causal garante que sÃ³ vemos tokens anteriores (autoregressive)
        let attn_out = self.attention.forward(&normalized_x, mask)?;
        
        // ğŸ”„ **PRIMEIRA CONEXÃƒO RESIDUAL**
        // x_new = x_original + attention_output
        // Preserva a informaÃ§Ã£o original e permite gradientes diretos
        let x = (x + attn_out)?;
        
        // ğŸ½ï¸ **ETAPA 2: FEED-FORWARD COM CONEXÃƒO RESIDUAL**
        // 
        // NormalizaÃ§Ã£o antes do processamento feed-forward
        let normalized_x2 = self.ln2.forward(&x)?;
        
        // ğŸ§  **PROCESSAMENTO FEED-FORWARD**
        // Cada posiÃ§Ã£o Ã© processada independentemente
        // ExpansÃ£o 4x â†’ GELU â†’ ContraÃ§Ã£o â†’ Dropout
        let ff_out = self.feed_forward.forward(&normalized_x2)?;
        
        // ğŸ”„ **SEGUNDA CONEXÃƒO RESIDUAL**
        // x_final = x_after_attention + feedforward_output
        let x = (x + ff_out)?;
        
        // ğŸ“¤ **SAÃDA FINAL**
        // Tensor com representaÃ§Ãµes refinadas de cada token
        // Cada token agora "conhece" o contexto e foi processado individualmente
        // DimensÃµes preservadas: [batch_size, seq_len, n_embd]
        Ok(x)
    }
}