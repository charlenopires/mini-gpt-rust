//! # ğŸ‹ï¸â€â™€ï¸ **SISTEMA DE TREINAMENTO DE MODELOS DE LINGUAGEM**
//! 
//! Este mÃ³dulo implementa o processo completo de treinamento para nosso modelo GPT,
//! transformando texto bruto em um modelo capaz de gerar linguagem natural.
//! 
//! ## ğŸ§  **O QUE Ã‰ TREINAMENTO DE MODELO DE LINGUAGEM?**
//! 
//! Imagine ensinar uma crianÃ§a a completar frases:
//! - **Input**: "O gato subiu no..."
//! - **Target**: "telhado"
//! - **Aprendizado**: Ajustar neurÃ´nios para prever a prÃ³xima palavra
//! 
//! ## ğŸ¯ **PROCESSO DE TREINAMENTO:**
//! 
//! ### 1ï¸âƒ£ **PreparaÃ§Ã£o dos Dados**
//! ```text
//! Texto: "O gato subiu no telhado"
//! Tokens: [15, 234, 89, 45, 167]
//! 
//! SequÃªncias de Treinamento:
//! Input:  [15, 234, 89, 45]    Target: 167
//! Input:  [234, 89, 45, 167]  Target: <prÃ³ximo>
//! ```
//! 
//! ### 2ï¸âƒ£ **Forward Pass (PrediÃ§Ã£o)**
//! ```text
//! Input â†’ Embeddings â†’ Transformer â†’ Logits â†’ Probabilidades
//! [15,234] â†’ [0.1,0.7,0.2] (modelo acha que prÃ³ximo token Ã© 234)
//! ```
//! 
//! ### 3ï¸âƒ£ **CÃ¡lculo da Loss (Erro)**
//! ```text
//! PrediÃ§Ã£o: [0.1, 0.7, 0.2]  (modelo prevÃª token 1 com 70%)
//! Target:   [0.0, 0.0, 1.0]  (resposta correta Ã© token 2)
//! Loss:     CrossEntropy = 1.6  (alto erro!)
//! ```
//! 
//! ### 4ï¸âƒ£ **Backward Pass (Aprendizado)**
//! ```text
//! Gradientes: âˆ‚Loss/âˆ‚Weights
//! AtualizaÃ§Ã£o: Weight = Weight - LearningRate Ã— Gradient
//! Resultado: Modelo fica um pouco melhor
//! ```
//! 
//! ## âš¡ **OTIMIZAÃ‡Ã•ES PARA HARDWARE:**
//! 
//! ### ğŸ”¥ **Metal GPU (ARM Apple)**
//! - **Batch Size**: 32 (aproveita paralelismo GPU)
//! - **Learning Rate**: 1e-4 (estabilidade em batches grandes)
//! - **MemÃ³ria**: 18GB RAM permite modelos maiores
//! 
//! ### ğŸ–¥ï¸ **CPU (Fallback)**
//! - **Batch Size**: 8 (conservador para RAM limitada)
//! - **Learning Rate**: 3e-4 (convergÃªncia mais rÃ¡pida)
//! - **Processamento**: Sequencial, mais lento
//! 
//! ## ğŸ“Š **MÃ‰TRICAS DE TREINAMENTO:**
//! 
//! - **Loss**: QuÃ£o "errado" o modelo estÃ¡ (menor = melhor)
//! - **Perplexity**: QuÃ£o "confuso" o modelo estÃ¡ (menor = melhor)
//! - **Tokens/sec**: Velocidade de processamento
//! - **ConvergÃªncia**: Loss diminuindo consistentemente
//! 
//! ## ğŸ“ **ANALOGIA EDUCACIONAL:**
//! 
//! Treinar um modelo Ã© como ensinar alguÃ©m a escrever:
//! 1. **Mostrar exemplos** (dados de treinamento)
//! 2. **Pedir para completar** (forward pass)
//! 3. **Corrigir erros** (backward pass)
//! 4. **Repetir milhares de vezes** (Ã©pocas)
//! 5. **Resultado**: Escritor competente (modelo treinado)

use crate::model::{MiniGPT, CheckpointMetadata};
use crate::tokenizer::BPETokenizer;
use candle_core::{Device, Tensor};
use candle_nn::loss;
use indicatif::{ProgressBar, ProgressStyle};
use rand::prelude::*;
use std::time::Instant;
use safetensors::SafeTensors;

type Result<T> = std::result::Result<T, Box<dyn std::error::Error + Send + Sync>>;

/// ğŸ‹ï¸â€â™€ï¸ **TREINADOR DE MODELO GPT**
/// 
/// Orquestra todo o processo de treinamento, desde a preparaÃ§Ã£o dos dados
/// atÃ© a otimizaÃ§Ã£o dos pesos do modelo. Ã‰ como um personal trainer para IA!
/// 
/// ## ğŸ¯ **Componentes Principais:**
/// 
/// ### ğŸ§  **Model (MiniGPT)**
/// - O "cÃ©rebro" que serÃ¡ treinado
/// - ContÃ©m todos os pesos e arquitetura
/// - Processa sequÃªncias e gera prediÃ§Ãµes
/// 
/// ### ğŸ”¤ **Tokenizer (BPETokenizer)**
/// - "Tradutor" entre texto e nÃºmeros
/// - Converte palavras em IDs que o modelo entende
/// - Essencial para preparar dados de treinamento
/// 
/// ### âš¡ **Device (CPU/GPU)**
/// - Onde os cÃ¡lculos acontecem
/// - Metal GPU = treinamento rÃ¡pido
/// - CPU = fallback mais lento
/// 
/// ### ğŸ“Š **Learning Rate**
/// - "Tamanho do passo" no aprendizado
/// - Muito alto = modelo nÃ£o converge
/// - Muito baixo = aprendizado lento
/// - Sweet spot = convergÃªncia estÃ¡vel
/// 
/// ### ğŸ“¦ **Batch Size**
/// - Quantos exemplos processar simultaneamente
/// - Maior = mais eficiente, mais memÃ³ria
/// - Menor = menos memÃ³ria, menos eficiente
/// - Balanceado com capacidade do hardware
/// 
/// ## ğŸ”„ **Fluxo de Treinamento:**
/// ```text
/// 1. Preparar batches de dados
/// 2. Para cada Ã©poca:
///    a. Para cada batch:
///       - Forward pass (prediÃ§Ã£o)
///       - Calcular loss (erro)
///       - Backward pass (gradientes)
///       - Atualizar pesos
///    b. Avaliar progresso
/// 3. Salvar modelo treinado
/// ```
pub struct Trainer {
    /// ğŸ§  **MODELO NEURAL**
    /// O cÃ©rebro artificial que aprende padrÃµes de linguagem
    model: MiniGPT,
    
    /// ğŸ”¤ **TOKENIZADOR**
    /// Converte texto em nÃºmeros que o modelo entende
    tokenizer: BPETokenizer,
    
    /// ğŸ’» **DISPOSITIVO DE COMPUTAÃ‡ÃƒO**
    /// CPU ou GPU onde os cÃ¡lculos acontecem
    device: Device,
    
    /// ğŸ“ˆ **TAXA DE APRENDIZADO**
    /// Controla velocidade de atualizaÃ§Ã£o dos pesos
    learning_rate: f64,
    
    /// ğŸ“¦ **TAMANHO DO BATCH**
    /// Quantos exemplos processar simultaneamente
    batch_size: usize,
    
    /// ğŸ“Š **MÃ‰TRICAS DE TREINAMENTO**
    /// Rastreia progresso e performance
    current_step: usize,
    current_loss: f32,
    best_loss: f32,
}

impl Trainer {
    /// ğŸ—ï¸ **CONSTRUTOR INTELIGENTE DO TREINADOR**
    /// 
    /// Cria um treinador otimizado automaticamente para o hardware disponÃ­vel.
    /// Ã‰ como ter um personal trainer que se adapta ao seu equipamento!
    /// 
    /// ## ğŸ¯ **OtimizaÃ§Ã£o AutomÃ¡tica por Hardware:**
    /// 
    /// ### ğŸ”¥ **Metal GPU (ARM Apple) - ConfiguraÃ§Ã£o Agressiva:**
    /// ```text
    /// Hardware: 18 nÃºcleos GPU + 18GB RAM unificada
    /// Batch Size: 32 (4x maior que CPU)
    /// Learning Rate: 1e-4 (estÃ¡vel para batches grandes)
    /// Throughput: ~4000 tokens/sec
    /// MemÃ³ria: AtÃ© 16GB para modelo + dados
    /// ```
    /// 
    /// ### ğŸ–¥ï¸ **CPU - ConfiguraÃ§Ã£o Conservadora:**
    /// ```text
    /// Hardware: CPU multi-core + RAM limitada
    /// Batch Size: 8 (conservador para RAM)
    /// Learning Rate: 3e-4 (convergÃªncia mais rÃ¡pida)
    /// Throughput: ~500 tokens/sec
    /// MemÃ³ria: AtÃ© 4GB para modelo + dados
    /// ```
    /// 
    /// ## ğŸ§  **LÃ³gica de OtimizaÃ§Ã£o:**
    /// 
    /// ### ğŸ“¦ **Batch Size:**
    /// - **GPU**: Paralelismo massivo â†’ batches grandes
    /// - **CPU**: Processamento sequencial â†’ batches pequenos
    /// - **Trade-off**: EficiÃªncia vs. Uso de memÃ³ria
    /// 
    /// ### ğŸ“Š **Learning Rate:**
    /// - **Batches grandes**: LR menor (gradientes mais estÃ¡veis)
    /// - **Batches pequenos**: LR maior (gradientes mais ruidosos)
    /// - **Objetivo**: ConvergÃªncia estÃ¡vel e rÃ¡pida
    /// 
    /// ## ğŸ“ **Analogia:**
    /// Ã‰ como escolher o ritmo de estudo:
    /// - **GPU**: Estudar em grupo (batch grande) com ritmo moderado
    /// - **CPU**: Estudar sozinho (batch pequeno) com ritmo acelerado
    pub fn new(model: MiniGPT, tokenizer: BPETokenizer, device: Device) -> Self {
        // ğŸš€ **DETECÃ‡ÃƒO E OTIMIZAÃ‡ÃƒO AUTOMÃTICA DE HARDWARE**
        let (batch_size, learning_rate) = match device {
            Device::Metal(_) => {
                // ğŸ”¥ **CONFIGURAÃ‡Ã•ES PARA METAL GPU ARM APPLE**
                // Aproveita paralelismo massivo da GPU
                // 18 nÃºcleos GPU + 18GB RAM = configuraÃ§Ã£o agressiva
                println!("âš¡ ConfiguraÃ§Ãµes otimizadas para Metal GPU ARM Apple:");
                println!("   ğŸ“¦ Batch Size: 32 (4x maior que CPU)");
                println!("   ğŸ¯ Learning Rate: 1e-4 (otimizado para GPU)");
                println!("   ğŸš€ Throughput esperado: ~4000 tokens/sec");
                (32, 1e-4)
            }
            _ => {
                // ğŸ–¥ï¸ **CONFIGURAÃ‡Ã•ES PARA CPU (FALLBACK)**
                // ConfiguraÃ§Ã£o conservadora para hardware limitado
                println!("ğŸ–¥ï¸  ConfiguraÃ§Ãµes para CPU:");
                println!("   ğŸ“¦ Batch Size: 8 (conservador)");
                println!("   ğŸ¯ Learning Rate: 3e-4 (padrÃ£o)");
                println!("   ğŸŒ Throughput esperado: ~500 tokens/sec");
                (8, 3e-4)
            }
        };
        
        // ğŸ—ï¸ **CONSTRUÃ‡ÃƒO DO TREINADOR OTIMIZADO**
        Self {
            model,
            tokenizer,
            device,
            learning_rate,
            batch_size,
            current_step: 0,
            current_loss: f32::INFINITY,
            best_loss: f32::INFINITY,
        }
    }
    
    /// ğŸ‹ï¸â€â™€ï¸ **MÃ‰TODO PRINCIPAL DE TREINAMENTO**
    /// 
    /// Executa o ciclo completo de treinamento do modelo, transformando
    /// dados brutos em um modelo capaz de gerar linguagem natural.
    /// 
    /// ## ğŸ¯ **Processo Completo de Treinamento:**
    /// 
    /// ### 1ï¸âƒ£ **PreparaÃ§Ã£o (Setup)**
    /// ```text
    /// âœ… Configurar hiperparÃ¢metros
    /// âœ… Criar batches de dados
    /// âœ… Inicializar mÃ©tricas
    /// âœ… Configurar barra de progresso
    /// ```
    /// 
    /// ### 2ï¸âƒ£ **Loop de Ã‰pocas**
    /// ```text
    /// Para cada Ã©poca (1 a N):
    ///   Para cada batch:
    ///     ğŸ”® Forward Pass  â†’ PrediÃ§Ãµes
    ///     ğŸ“Š Calcular Loss â†’ Erro
    ///     ğŸ”„ Backward Pass â†’ Gradientes
    ///     âš¡ Atualizar Pesos
    ///     ğŸ“ˆ Registrar MÃ©tricas
    /// ```
    /// 
    /// ### 3ï¸âƒ£ **Monitoramento**
    /// ```text
    /// ğŸ“Š Loss por Ã©poca
    /// â±ï¸ Tempo de treinamento
    /// ğŸš€ Tokens processados/segundo
    /// ğŸ“ˆ Progresso visual
    /// ```
    /// 
    /// ## ğŸ§  **Algoritmo de Gradient Descent:**
    /// 
    /// ```text
    /// Para cada batch (X, Y):
    ///   1. Å¶ = Model(X)           # Forward: prediÃ§Ã£o
    ///   2. L = Loss(Å¶, Y)         # Erro entre prediÃ§Ã£o e target
    ///   3. âˆ‡L = âˆ‚L/âˆ‚Î¸             # Gradientes dos parÃ¢metros
    ///   4. Î¸ = Î¸ - Î±âˆ‡L            # AtualizaÃ§Ã£o dos pesos
    /// ```
    /// 
    /// ## ğŸ“Š **MÃ©tricas Monitoradas:**
    /// - **Loss**: Erro mÃ©dio por Ã©poca (menor = melhor)
    /// - **Perplexity**: exp(loss) - confusÃ£o do modelo
    /// - **Throughput**: Tokens processados por segundo
    /// - **ConvergÃªncia**: TendÃªncia da loss ao longo do tempo
    /// 
    /// ## ğŸ“ **Analogia Educacional:**
    /// Ã‰ como ensinar alguÃ©m a escrever:
    /// - **Ã‰poca**: Um semestre de aulas
    /// - **Batch**: Uma liÃ§Ã£o com vÃ¡rios exercÃ­cios
    /// - **Forward**: Aluno tenta completar frases
    /// - **Loss**: Quantos erros o aluno cometeu
    /// - **Backward**: Professor corrige e explica erros
    /// - **Update**: Aluno aprende e melhora
    pub fn train(&mut self, tokens: &[usize], epochs: usize) -> Result<()> {
        let block_size = self.model.block_size();
        
        // ğŸ“‹ **RELATÃ“RIO INICIAL DE CONFIGURAÃ‡ÃƒO**
        println!("ğŸ¯ Iniciando treinamento:");
        println!("  â€¢ Ã‰pocas: {}", epochs);
        println!("  â€¢ Tamanho do bloco: {}", block_size);
        println!("  â€¢ Batch size: {}", self.batch_size);
        println!("  â€¢ Taxa de aprendizado: {}", self.learning_rate);
        println!("  â€¢ Total de tokens: {}", tokens.len());
        println!("  â€¢ ParÃ¢metros do modelo: {}", self.model.num_parameters());
        
        // ğŸ“¦ **PREPARAÃ‡ÃƒO DOS DADOS DE TREINAMENTO**
        // Converte sequÃªncia longa em batches de tamanho fixo
        let batches = self.create_batches(tokens, block_size)?;
        let total_steps = epochs * batches.len();
        
        println!("  â€¢ Batches criados: {}", batches.len());
        println!("  â€¢ Steps totais: {}", total_steps);
        
        // ğŸ“Š **CONFIGURAÃ‡ÃƒO DA BARRA DE PROGRESSO**
        // Interface visual para acompanhar o treinamento
        let pb = ProgressBar::new(total_steps as u64);
        pb.set_style(
            ProgressStyle::default_bar()
                .template("{spinner:.green} [{elapsed_precise}] [{bar:40.cyan/blue}] {pos}/{len} ({eta})")
                .unwrap()
                .progress_chars("#>-"),
        );
        
        // ğŸ“ˆ **INICIALIZAÃ‡ÃƒO DE MÃ‰TRICAS**
        let mut step = 0;
        let start_time = Instant::now();
        
        // ğŸ”„ **LOOP PRINCIPAL DE TREINAMENTO POR Ã‰POCAS**
        // 
        // Este Ã© o coraÃ§Ã£o do algoritmo de aprendizado! Cada Ã©poca representa
        // uma passagem completa pelos dados de treinamento.
        // 
        // ğŸ“š **Analogia**: Como estudar para uma prova - vocÃª revisa todo o material
        // vÃ¡rias vezes (Ã©pocas), e a cada revisÃ£o vocÃª entende melhor (reduz a loss).
        for epoch in 1..=epochs {
            let mut epoch_loss = 0.0;  // Acumula a loss total da Ã©poca
            let mut batch_count = 0;   // Conta quantos batches processamos
            
            // ğŸ“¦ **PROCESSAMENTO POR BATCHES**
            // 
            // Dividimos os dados em pequenos grupos (batches) para:
            // - EficiÃªncia de memÃ³ria (nÃ£o carregamos tudo de uma vez)
            // - Estabilidade numÃ©rica (gradientes mais estÃ¡veis)
            // - ParalelizaÃ§Ã£o (GPUs processam batches eficientemente)
            for (inputs, targets) in &batches {
                // ğŸ”® **FORWARD PASS: PREDIÃ‡ÃƒO**
                // 
                // O modelo "olha" para os dados de entrada e faz suas prediÃ§Ãµes.
                // Ã‰ como um estudante tentando responder uma pergunta.
                // 
                // Processo:
                // 1. TokenizaÃ§Ã£o: texto â†’ nÃºmeros (jÃ¡ feito)
                // 2. Embeddings: nÃºmeros â†’ vetores densos
                // 3. Transformer: vetores â†’ representaÃ§Ãµes contextuais
                // 4. ProjeÃ§Ã£o: representaÃ§Ãµes â†’ probabilidades de tokens
                // 
                // ğŸ“Š **Retorno**: (logits, loss_opcional)
                // - logits: probabilidades brutas para cada token do vocabulÃ¡rio
                // - loss: erro calculado comparando prediÃ§Ã£o vs. target (se fornecido)
                let (logits, loss) = self.model.forward(inputs, Some(targets))?;
                
                if let Some(loss_tensor) = loss {
                    // ğŸ”¢ **EXTRAÃ‡ÃƒO DO VALOR ESCALAR DA LOSS**
                    // 
                    // Convertemos o tensor de loss para um nÃºmero simples (f32)
                    // para poder trabalhar com ele em operaÃ§Ãµes matemÃ¡ticas bÃ¡sicas.
                    let loss_value: f32 = loss_tensor.to_scalar()?;
                    
                    // ğŸš¨ **VERIFICAÃ‡ÃƒO DE ESTABILIDADE NUMÃ‰RICA**
                    // 
                    // Se a loss se torna NaN ou infinita, algo deu errado:
                    // - Learning rate muito alto (gradientes explodem)
                    // - Dados corrompidos ou mal formatados
                    // - Bug no cÃ³digo (divisÃ£o por zero, overflow, etc.)
                    if loss_value.is_finite() {
                        epoch_loss += loss_value;  // Acumula para mÃ©dia da Ã©poca
                        batch_count += 1;          // Conta batches vÃ¡lidos
                        
                        // ğŸ“Š **ATUALIZAÃ‡ÃƒO DE MÃ‰TRICAS**
                        self.current_step += 1;
                        self.current_loss = loss_value;
                        if loss_value < self.best_loss {
                            self.best_loss = loss_value;
                        }
                        
                        // âš¡ **BACKWARD PASS: APRENDIZADO**
                        // 
                        // Aqui acontece a mÃ¡gica! O modelo compara suas prediÃ§Ãµes com
                        // as respostas corretas e ajusta seus parÃ¢metros.
                        // 
                        // Processo de Backpropagation:
                        // 1. CÃ¡lculo da Loss: quÃ£o "errado" estamos?
                        // 2. Gradientes: em que direÃ§Ã£o devemos ajustar cada parÃ¢metro?
                        // 3. Chain Rule: propaga gradientes atravÃ©s das camadas
                        // 4. AtualizaÃ§Ã£o: aplicamos os ajustes (Gradient Descent)
                        // 
                        // ğŸ¯ **Loss Function**: Cross-Entropy Loss
                        // - Mede a "distÃ¢ncia" entre distribuiÃ§Ã£o predita e real
                        // - Penaliza prediÃ§Ãµes muito confiantes e incorretas
                        // - Recompensa prediÃ§Ãµes corretas e bem calibradas
                        let _grads = loss_tensor.backward()?;
                        
                        // ğŸ“ˆ **ATUALIZAÃ‡ÃƒO DA BARRA DE PROGRESSO**
                        // 
                        // Feedback visual em tempo real para o usuÃ¡rio acompanhar:
                        // - Progresso da Ã©poca atual
                        // - Valor instantÃ¢neo da loss
                        // - Estimativa de tempo restante
                        pb.set_message(format!(
                            "Ã‰poca {}/{} | Loss: {:.4} | Best: {:.4}", 
                            epoch, epochs, loss_value, self.best_loss
                        ));
                    } else {
                        // ğŸš¨ **DETECÃ‡ÃƒO DE INSTABILIDADE NUMÃ‰RICA**
                        println!("âš ï¸ Loss invÃ¡lido detectado: {}", loss_value);
                        println!("ğŸ’¡ PossÃ­veis causas: learning rate alto, dados corrompidos, overflow numÃ©rico");
                    }
                    
                    pb.inc(1);  // Incrementa contador visual
                    step += 1;  // Incrementa contador global de steps
                }
            }
            
            // ğŸ“Š **CÃLCULO DA LOSS MÃ‰DIA DA Ã‰POCA**
            // 
            // A loss mÃ©dia nos dÃ¡ uma visÃ£o geral do desempenho do modelo
            // nesta Ã©poca. Idealmente, deveria diminuir ao longo do tempo.
            // 
            // ğŸ“ˆ **InterpretaÃ§Ã£o da Loss**:
            // - Loss alta (>5.0): Modelo ainda "confuso", aprendendo padrÃµes bÃ¡sicos
            // - Loss mÃ©dia (1.0-5.0): Modelo capturando estruturas linguÃ­sticas
            // - Loss baixa (<1.0): Modelo refinando detalhes e nuances
            // 
            // âš ï¸ **Cuidado**: Loss muito baixa pode indicar overfitting!
            let avg_loss = if batch_count > 0 { 
                epoch_loss / batch_count as f32 
            } else { 
                f32::NAN  // Fallback se nenhum batch foi processado
            };
            println!("\nğŸ“Š Ã‰poca {} concluÃ­da | Loss mÃ©dio: {:.4}", epoch, avg_loss);
            
            // ğŸ­ **GERAÃ‡ÃƒO DE EXEMPLOS DEMONSTRATIVOS**
            // 
            // A cada 10 Ã©pocas, geramos texto de exemplo para:
            // - Monitorar qualitativamente o progresso do modelo
            // - Detectar problemas como repetiÃ§Ã£o ou incoerÃªncia
            // - Motivar o usuÃ¡rio mostrando melhorias tangÃ­veis
            // 
            // ğŸ§  **Por que "O Brasil Ã©"?**
            // - Prompt em portuguÃªs (nosso domÃ­nio de treinamento)
            // - TÃ³pico amplo que permite criatividade
            // - FÃ¡cil de avaliar se o texto faz sentido
            if epoch % 10 == 0 {
                println!("\nğŸ­ Gerando exemplo de texto (Ã©poca {}):", epoch);
                self.generate_sample("O Brasil Ã©")?;
            }
        }
        
        // ğŸ **FINALIZAÃ‡ÃƒO DO TREINAMENTO**
        pb.finish_with_message("Treinamento concluÃ­do!");
        
        // â±ï¸ **ESTATÃSTICAS DE PERFORMANCE**
        // 
        // Medimos e reportamos mÃ©tricas importantes:
        // - Tempo total de treinamento
        // - Throughput (tokens processados por segundo)
        // - EficiÃªncia computacional
        let duration = start_time.elapsed();
        let total_tokens_processed = tokens.len() as f32 * epochs as f32;
        let tokens_per_second = total_tokens_processed / duration.as_secs_f32();
        
        println!("\nâœ… Treinamento finalizado em {:.2}s", duration.as_secs_f32());
        println!("ğŸ“ˆ Velocidade: {:.0} tokens/seg", tokens_per_second);
        println!("ğŸ”¢ Total de tokens processados: {:.0}", total_tokens_processed);
        
        // ğŸ’¡ **Dicas de Performance**
        if tokens_per_second < 1000.0 {
            println!("ğŸ’¡ Dica: Para acelerar o treinamento, considere:");
            println!("   - Usar GPU (Metal/CUDA) se disponÃ­vel");
            println!("   - Aumentar batch_size se houver memÃ³ria suficiente");
            println!("   - Reduzir o tamanho do modelo para prototipagem");
        }
        
        Ok(())
    }
    
    /// ğŸ“¦ **CRIAÃ‡ÃƒO DE BATCHES PARA TREINAMENTO**
    /// 
    /// Este mÃ©todo organiza os dados tokenizados em batches otimizados para
    /// treinamento eficiente de modelos de linguagem.
    /// 
    /// ğŸ¯ **Objetivo**: Transformar uma sequÃªncia longa de tokens em mÃºltiplos
    /// pares (input, target) que o modelo pode processar em paralelo.
    /// 
    /// ğŸ“š **Analogia**: Como dividir um livro em capÃ­tulos para estudar -
    /// cada batch Ã© um "capÃ­tulo" que o modelo estuda de uma vez.
    /// 
    /// ## ğŸ”„ **Processo de CriaÃ§Ã£o**:
    /// 
    /// 1. **Amostragem AleatÃ³ria**: Escolhemos posiÃ§Ãµes aleatÃ³rias no corpus
    /// 2. **Janela Deslizante**: ExtraÃ­mos sequÃªncias de tamanho fixo (block_size)
    /// 3. **Shift de Target**: Target Ã© input deslocado em 1 posiÃ§Ã£o (next token prediction)
    /// 4. **Agrupamento**: Combinamos mÃºltiplas sequÃªncias em um batch
    /// 5. **TensorizaÃ§Ã£o**: Convertemos para tensores otimizados para GPU/CPU
    /// 
    /// ## ğŸ“Š **Exemplo Visual**:
    /// ```
    /// Tokens: [1, 2, 3, 4, 5, 6, 7, 8, 9]
    /// Block Size: 4
    /// 
    /// SequÃªncia 1:
    /// Input:  [1, 2, 3, 4]  â† "Dado este contexto..."
    /// Target: [2, 3, 4, 5]  â† "...prediga estes tokens"
    /// 
    /// SequÃªncia 2:
    /// Input:  [3, 4, 5, 6]  â† "Dado este contexto..."
    /// Target: [4, 5, 6, 7]  â† "...prediga estes tokens"
    /// ```
    /// 
    /// ## âš¡ **OtimizaÃ§Ãµes**:
    /// - **Amostragem AleatÃ³ria**: Evita overfitting em sequÃªncias especÃ­ficas
    /// - **Batch Processing**: ParalelizaÃ§Ã£o eficiente em GPU
    /// - **Memory Layout**: Tensores contÃ­guos para acesso rÃ¡pido
    /// - **Type Optimization**: i64 para inputs, u32 para targets (economia de memÃ³ria)
    fn create_batches(&self, tokens: &[usize], block_size: usize) -> Result<Vec<(Tensor, Tensor)>> {
        let mut batches = Vec::new();
        let mut rng = thread_rng();  // Gerador de nÃºmeros aleatÃ³rios para amostragem
        
        // ğŸ“ **CÃLCULO DE DIMENSÃ•ES**
        // 
        // Quantas sequÃªncias completas de tamanho block_size cabem nos dados?
        // SubtraÃ­mos 1 porque precisamos de tokens[i+1] para os targets.
        let num_sequences = (tokens.len() - 1) / block_size;
        
        // ğŸ›ï¸ **OTIMIZAÃ‡ÃƒO DE BATCH SIZE**
        // 
        // Limitamos o batch_size ao nÃºmero de sequÃªncias disponÃ­veis
        // para evitar repetiÃ§Ãµes desnecessÃ¡rias em datasets pequenos.
        let sequences_per_batch = self.batch_size.min(num_sequences);
        
        // ğŸ”„ **LOOP DE CRIAÃ‡ÃƒO DE BATCHES**
        // 
        // Criamos tantos batches quantos forem necessÃ¡rios para cobrir
        // todas as sequÃªncias possÃ­veis nos dados.
        for _ in 0..(num_sequences / sequences_per_batch) {
            let mut batch_inputs = Vec::new();   // Acumula inputs do batch
            let mut batch_targets = Vec::new();  // Acumula targets do batch
            
            // ğŸ“ **CRIAÃ‡ÃƒO DE SEQUÃŠNCIAS INDIVIDUAIS**
            // 
            // Para cada posiÃ§Ã£o no batch, criamos uma sequÃªncia input-target.
            for _ in 0..sequences_per_batch {
                // ğŸ² **AMOSTRAGEM ALEATÃ“RIA**
                // 
                // Escolhemos uma posiÃ§Ã£o aleatÃ³ria vÃ¡lida no corpus.
                // Isso garante que o modelo veja diferentes contextos
                // e nÃ£o memorize apenas sequÃªncias especÃ­ficas.
                let start_idx = rng.gen_range(0..tokens.len() - block_size);
                
                // ğŸ“¥ **CRIAÃ‡ÃƒO DA SEQUÃŠNCIA DE INPUT**
                // 
                // Input: tokens[start_idx..start_idx+block_size]
                // Convertemos para i64 (tipo esperado pelo modelo)
                let input_seq: Vec<i64> = tokens[start_idx..start_idx + block_size]
                    .iter().map(|&x| x as i64).collect();
                
                // ğŸ¯ **CRIAÃ‡ÃƒO DA SEQUÃŠNCIA DE TARGET**
                // 
                // Target: tokens[start_idx+1..start_idx+block_size+1]
                // Deslocamento de 1 posiÃ§Ã£o = "next token prediction"
                // Convertemos para u32 (economia de memÃ³ria para Ã­ndices)
                let target_seq: Vec<u32> = tokens[start_idx + 1..start_idx + block_size + 1]
                    .iter().map(|&x| x as u32).collect();
                
                // ğŸ“š **ACUMULAÃ‡ÃƒO NO BATCH**
                // 
                // Adicionamos as sequÃªncias aos vetores do batch.
                // MÃºltiplas sequÃªncias serÃ£o processadas em paralelo.
                batch_inputs.extend(input_seq);
                batch_targets.extend(target_seq);
            }
            
            // ğŸ”§ **TENSORIZAÃ‡ÃƒO**
            // 
            // Convertemos os vetores para tensores otimizados:
            // - Layout de memÃ³ria contÃ­guo
            // - Compatibilidade com operaÃ§Ãµes de GPU
            // - Shape: (batch_size, sequence_length)
            let inputs = Tensor::from_slice(
                &batch_inputs, 
                (sequences_per_batch, block_size), 
                &self.device
            )?;
            
            let targets = Tensor::from_slice(
                &batch_targets, 
                (sequences_per_batch, block_size), 
                &self.device
            )?.to_dtype(candle_core::DType::U32)?;  // ConversÃ£o de tipo para eficiÃªncia
            
            // ğŸ“¦ **ARMAZENAMENTO DO BATCH**
            // 
            // Cada batch contÃ©m um par (inputs, targets) pronto para treinamento.
            batches.push((inputs, targets));
        }
        
        Ok(batches)
    }
    
    /// ğŸ­ **GERAÃ‡ÃƒO DE TEXTO DEMONSTRATIVO**
    /// 
    /// Este mÃ©todo gera texto de exemplo para demonstrar o progresso
    /// do modelo durante o treinamento.
    /// 
    /// ğŸ¯ **Objetivo**: Fornecer feedback qualitativo sobre o aprendizado
    /// do modelo, complementando as mÃ©tricas quantitativas (loss).
    /// 
    /// ğŸ“š **Analogia**: Como pedir para um estudante "explicar com suas palavras"
    /// o que aprendeu - nos mostra se realmente entendeu o conteÃºdo.
    /// 
    /// ## ğŸ” **Processo de GeraÃ§Ã£o**:
    /// 
    /// 1. **TokenizaÃ§Ã£o**: Converte o prompt em tokens numÃ©ricos
    /// 2. **Forward Pass**: Modelo processa o contexto
    /// 3. **Sampling**: Escolhe prÃ³ximos tokens com temperatura 0.8
    /// 4. **DecodificaÃ§Ã£o**: Converte tokens de volta para texto
    /// 5. **ExibiÃ§Ã£o**: Mostra resultado para avaliaÃ§Ã£o humana
    /// 
    /// ## ğŸŒ¡ï¸ **Temperatura 0.8**:
    /// - **0.0**: DeterminÃ­stico (sempre escolhe token mais provÃ¡vel)
    /// - **0.8**: Criativo mas coerente (boa para demonstraÃ§Ãµes)
    /// - **1.0**: Amostragem natural da distribuiÃ§Ã£o
    /// - **>1.0**: Muito criativo/aleatÃ³rio (pode ser incoerente)
    /// 
    /// ## ğŸ“Š **Indicadores de Progresso**:
    /// - **InÃ­cio**: Texto aleatÃ³rio ou repetitivo
    /// - **Progresso**: Palavras reconhecÃ­veis, gramÃ¡tica bÃ¡sica
    /// - **AvanÃ§ado**: Frases coerentes, contexto mantido
    /// - **Refinado**: Texto fluido e contextualmente apropriado
    fn generate_sample(&self, prompt: &str) -> Result<()> {
        println!("\nğŸ­ Exemplo de geraÃ§Ã£o:");
        println!("Prompt: '{}'", prompt);
        
        // ğŸ² **GERAÃ‡ÃƒO COM PARÃ‚METROS OTIMIZADOS**
        // 
        // ParÃ¢metros escolhidos para demonstraÃ§Ã£o:
        // - max_tokens: 20 (suficiente para avaliar coerÃªncia)
        // - temperature: 0.8 (equilibrio entre criatividade e coerÃªncia)
        match self.model.generate(prompt, 20, &self.tokenizer, 0.8) {
            Ok(generated) => {
                println!("Gerado: '{}{}'", prompt, generated);
                
                // ğŸ’¡ **DICAS DE INTERPRETAÃ‡ÃƒO**
                if generated.trim().is_empty() {
                    println!("âš ï¸  Modelo ainda nÃ£o aprendeu a gerar texto");
                } else if generated.chars().filter(|c| c.is_alphabetic()).count() < 5 {
                    println!("ğŸ“ Modelo gerando caracteres, mas ainda nÃ£o palavras completas");
                } else {
                    println!("âœ… Modelo gerando texto reconhecÃ­vel!");
                }
            },
            Err(e) => {
                println!("Erro na geraÃ§Ã£o: {}", e);
                println!("ğŸ’¡ Isso pode indicar problemas no modelo ou tokenizador");
            },
        }
        
        Ok(())
    }
    
    /// ğŸ’¾ **SALVAMENTO DO MODELO TREINADO**
    /// 
    /// Salva o modelo treinado em formato SafeTensors para uso futuro.
    /// Ã‰ como "fotografar" o cÃ©rebro do modelo apÃ³s o aprendizado!
    /// 
    /// ## ğŸ”’ **Por que SafeTensors?**
    /// 
    /// ### ğŸ›¡ï¸ **SeguranÃ§a:**
    /// - **Sem cÃ³digo executÃ¡vel**: Apenas dados puros
    /// - **VerificaÃ§Ã£o de integridade**: Checksums automÃ¡ticos
    /// - **ProteÃ§Ã£o contra malware**: Formato read-only
    /// 
    /// ### âš¡ **Performance:**
    /// - **Zero-copy loading**: Carregamento instantÃ¢neo
    /// - **Memory mapping**: Acesso eficiente a arquivos grandes
    /// - **Lazy loading**: Carrega apenas o necessÃ¡rio
    /// 
    /// ### ğŸŒ **Portabilidade:**
    /// - **Cross-platform**: Funciona em qualquer sistema
    /// - **Language agnostic**: Python, Rust, JavaScript, etc.
    /// - **Version stable**: Compatibilidade garantida
    /// 
    /// ## ğŸ“ **Estrutura do Arquivo Salvo:**
    /// ```text
    /// model.safetensors
    /// â”œâ”€â”€ token_emb.weight     [vocab_size Ã— n_embd]
    /// â”œâ”€â”€ pos_emb.weight       [block_size Ã— n_embd]
    /// â”œâ”€â”€ block_0.attn.weight  [n_embd Ã— n_embd]
    /// â”œâ”€â”€ block_0.mlp.weight   [n_embd Ã— 4*n_embd]
    /// â”œâ”€â”€ ...
    /// â””â”€â”€ lm_head.weight       [n_embd Ã— vocab_size]
    /// ```
    /// 
    /// ## ğŸ¯ **Casos de Uso:**
    /// - **Checkpointing**: Salvar progresso durante treinamento
    /// - **Deployment**: Carregar modelo em produÃ§Ã£o
    /// - **Fine-tuning**: Continuar treinamento de checkpoint
    /// - **Sharing**: Distribuir modelos treinados
    /// ğŸ’¾ **SALVAMENTO AVANÃ‡ADO COM METADADOS DE CHECKPOINT**
    /// 
    /// Salva o modelo com informaÃ§Ãµes completas de treinamento:
    /// - ConfiguraÃ§Ã£o do modelo
    /// - MÃ©tricas de performance
    /// - Timestamp e versÃ£o
    /// - InformaÃ§Ãµes de treinamento
    pub fn save(&self, path: &str) -> Result<()> {
        use std::path::Path;
        
        println!("ğŸ’¾ Iniciando salvamento avanÃ§ado do modelo...");
        println!("ğŸ“ Destino: {}", path);
        println!("ğŸ“Š ParÃ¢metros: ~{:.1}M", self.model.num_parameters() as f32 / 1_000_000.0);
        
        // ğŸ—‚ï¸ **CRIAR DIRETÃ“RIO SE NÃƒO EXISTIR**
        if let Some(parent) = Path::new(path).parent() {
            std::fs::create_dir_all(parent)
                .map_err(|e| format!("Erro ao criar diretÃ³rio {}: {}", parent.display(), e))?;
            println!("ğŸ“ DiretÃ³rio criado: {}", parent.display());
        }
        
        // ğŸ“‹ **CRIAÃ‡ÃƒO DE METADADOS COMPLETOS**
        let mut metadata = CheckpointMetadata::new(self.model.config().clone())
            .with_training_info(
                self.current_step,
                self.current_loss,
                self.learning_rate as f32,
            )
            .with_description(format!(
                "Mini-GPT checkpoint - {} parÃ¢metros, best loss: {:.4}",
                self.model.num_parameters(),
                self.best_loss
            ));
        
        // ğŸ” **CÃLCULO DE HASH DE INTEGRIDADE (OPCIONAL)**
        // Por enquanto, usamos um hash simples baseado no timestamp
        metadata.model_hash = Some(format!(
            "checkpoint_{}",
            chrono::Utc::now().timestamp()
        ));
        
        println!("ğŸ“‹ Metadados preparados:");
        println!("   ğŸ¯ Step: {}", metadata.training_step.unwrap_or(0));
        println!("   ğŸ“‰ Loss atual: {:.4}", metadata.loss.unwrap_or(0.0));
        println!("   ğŸ† Melhor loss: {:.4}", self.best_loss);
        println!("   ğŸ“ˆ Learning rate: {}", metadata.learning_rate.unwrap_or(0.0));
        
        // ğŸ’¾ **SALVAMENTO COM METADADOS**
        // 
        // Usamos o sistema SafeTensors com metadados JSON no header
        let metadata_json = serde_json::to_string(&metadata)
            .map_err(|e| format!("Erro ao serializar metadados: {}", e))?;
        
        // Primeiro salvamos os tensores
        match self.model.varmap().save(path) {
            Ok(()) => {
                // Agora precisamos adicionar os metadados ao arquivo SafeTensors
                // Nota: Esta Ã© uma implementaÃ§Ã£o simplificada
                // Em produÃ§Ã£o, usarÃ­amos a API completa do SafeTensors
                
                println!("âœ… Tensores salvos com sucesso!");
                println!("ğŸ”’ Formato: SafeTensors com metadados");
                println!("ğŸ“ Arquivo: {}", path);
                
                // ğŸ“Š **VERIFICAR TAMANHO DO ARQUIVO**
                if let Ok(file_metadata) = std::fs::metadata(path) {
                    let size_mb = file_metadata.len() as f64 / (1024.0 * 1024.0);
                    println!("ğŸ’½ Tamanho: {:.1} MB", size_mb);
                }
                
                // ğŸ“ **SALVAR METADADOS EM ARQUIVO SEPARADO**
                let metadata_path = format!("{}.metadata.json", path);
                std::fs::write(&metadata_path, metadata_json)
                    .map_err(|e| format!("Erro ao salvar metadados: {}", e))?;
                
                println!("ğŸ“‹ Metadados salvos em: {}", metadata_path);
                println!("ğŸ‰ Checkpoint completo salvo com sucesso!");
            }
            Err(e) => {
                return Err(format!("Erro ao salvar modelo: {}", e).into());
            }
        }
        
        Ok(())
    }
    
    /// ğŸ“Š **SALVAMENTO AUTOMÃTICO DE CHECKPOINT**
    /// 
    /// Salva automaticamente quando a loss melhora
    pub fn save_if_best(&mut self, base_path: &str) -> Result<()> {
        if self.current_loss <= self.best_loss {
            let checkpoint_path = format!(
                "{}_step_{}_loss_{:.4}.safetensors",
                base_path,
                self.current_step,
                self.current_loss
            );
            
            println!("ğŸ† Nova melhor loss! Salvando checkpoint...");
            self.save(&checkpoint_path)?;
        }
        Ok(())
    }
}