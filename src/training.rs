//! # üèãÔ∏è‚Äç‚ôÄÔ∏è **SISTEMA DE TREINAMENTO DE MODELOS DE LINGUAGEM**
//! 
//! Este m√≥dulo implementa o processo completo de treinamento para nosso modelo GPT,
//! transformando texto bruto em um modelo capaz de gerar linguagem natural.
//! 
//! ## üß† **O QUE √â TREINAMENTO DE MODELO DE LINGUAGEM?**
//! 
//! Imagine ensinar uma crian√ßa a completar frases:
//! - **Input**: "O gato subiu no..."
//! - **Target**: "telhado"
//! - **Aprendizado**: Ajustar neur√¥nios para prever a pr√≥xima palavra
//! 
//! ## üéØ **PROCESSO DE TREINAMENTO:**
//! 
//! ### 1Ô∏è‚É£ **Prepara√ß√£o dos Dados**
//! ```text
//! Texto: "O gato subiu no telhado"
//! Tokens: [15, 234, 89, 45, 167]
//! 
//! Sequ√™ncias de Treinamento:
//! Input:  [15, 234, 89, 45]    Target: 167
//! Input:  [234, 89, 45, 167]  Target: <pr√≥ximo>
//! ```
//! 
//! ### 2Ô∏è‚É£ **Forward Pass (Predi√ß√£o)**
//! ```text
//! Input ‚Üí Embeddings ‚Üí Transformer ‚Üí Logits ‚Üí Probabilidades
//! [15,234] ‚Üí [0.1,0.7,0.2] (modelo acha que pr√≥ximo token √© 234)
//! ```
//! 
//! ### 3Ô∏è‚É£ **C√°lculo da Loss (Erro)**
//! ```text
//! Predi√ß√£o: [0.1, 0.7, 0.2]  (modelo prev√™ token 1 com 70%)
//! Target:   [0.0, 0.0, 1.0]  (resposta correta √© token 2)
//! Loss:     CrossEntropy = 1.6  (alto erro!)
//! ```
//! 
//! ### 4Ô∏è‚É£ **Backward Pass (Aprendizado)**
//! ```text
//! Gradientes: ‚àÇLoss/‚àÇWeights
//! Atualiza√ß√£o: Weight = Weight - LearningRate √ó Gradient
//! Resultado: Modelo fica um pouco melhor
//! ```
//! 
//! ## ‚ö° **OTIMIZA√á√ïES PARA HARDWARE:**
//! 
//! ### üî• **Metal GPU (ARM Apple)**
//! - **Batch Size**: 32 (aproveita paralelismo GPU)
//! - **Learning Rate**: 1e-4 (estabilidade em batches grandes)
//! - **Mem√≥ria**: 18GB RAM permite modelos maiores
//! 
//! ### üñ•Ô∏è **CPU (Fallback)**
//! - **Batch Size**: 8 (conservador para RAM limitada)
//! - **Learning Rate**: 3e-4 (converg√™ncia mais r√°pida)
//! - **Processamento**: Sequencial, mais lento
//! 
//! ## üìä **M√âTRICAS DE TREINAMENTO:**
//! 
//! - **Loss**: Qu√£o "errado" o modelo est√° (menor = melhor)
//! - **Perplexity**: Qu√£o "confuso" o modelo est√° (menor = melhor)
//! - **Tokens/sec**: Velocidade de processamento
//! - **Converg√™ncia**: Loss diminuindo consistentemente
//! 
//! ## üéì **ANALOGIA EDUCACIONAL:**
//! 
//! Treinar um modelo √© como ensinar algu√©m a escrever:
//! 1. **Mostrar exemplos** (dados de treinamento)
//! 2. **Pedir para completar** (forward pass)
//! 3. **Corrigir erros** (backward pass)
//! 4. **Repetir milhares de vezes** (√©pocas)
//! 5. **Resultado**: Escritor competente (modelo treinado)

use crate::model::{MiniGPT, CheckpointMetadata};
use crate::tokenizer::BPETokenizer;
use candle_core::{Device, Tensor};
// use candle_nn::loss; // Removido - n√£o utilizado
use indicatif::{ProgressBar, ProgressStyle};
use rand::prelude::*;
use std::time::Instant;
// use safetensors::SafeTensors; // Removido - n√£o utilizado

type Result<T> = std::result::Result<T, Box<dyn std::error::Error + Send + Sync>>;

/// üèãÔ∏è‚Äç‚ôÄÔ∏è **TREINADOR DE MODELO GPT**
/// 
/// Orquestra todo o processo de treinamento, desde a prepara√ß√£o dos dados
/// at√© a otimiza√ß√£o dos pesos do modelo. √â como um personal trainer para IA!
/// 
/// ## üéØ **Componentes Principais:**
/// 
/// ### üß† **Model (MiniGPT)**
/// - O "c√©rebro" que ser√° treinado
/// - Cont√©m todos os pesos e arquitetura
/// - Processa sequ√™ncias e gera predi√ß√µes
/// 
/// ### üî§ **Tokenizer (BPETokenizer)**
/// - "Tradutor" entre texto e n√∫meros
/// - Converte palavras em IDs que o modelo entende
/// - Essencial para preparar dados de treinamento
/// 
/// ### ‚ö° **Device (CPU/GPU)**
/// - Onde os c√°lculos acontecem
/// - Metal GPU = treinamento r√°pido
/// - CPU = fallback mais lento
/// 
/// ### üìä **Learning Rate**
/// - "Tamanho do passo" no aprendizado
/// - Muito alto = modelo n√£o converge
/// - Muito baixo = aprendizado lento
/// - Sweet spot = converg√™ncia est√°vel
/// 
/// ### üì¶ **Batch Size**
/// - Quantos exemplos processar simultaneamente
/// - Maior = mais eficiente, mais mem√≥ria
/// - Menor = menos mem√≥ria, menos eficiente
/// - Balanceado com capacidade do hardware
/// 
/// ## üîÑ **Fluxo de Treinamento:**
/// ```text
/// 1. Preparar batches de dados
/// 2. Para cada √©poca:
///    a. Para cada batch:
///       - Forward pass (predi√ß√£o)
///       - Calcular loss (erro)
///       - Backward pass (gradientes)
///       - Atualizar pesos
///    b. Avaliar progresso
/// 3. Salvar modelo treinado
/// ```
pub struct Trainer {
    /// üß† **MODELO NEURAL**
    /// O c√©rebro artificial que aprende padr√µes de linguagem
    model: MiniGPT,
    
    /// üî§ **TOKENIZADOR**
    /// Converte texto em n√∫meros que o modelo entende
    tokenizer: BPETokenizer,
    
    /// üíª **DISPOSITIVO DE COMPUTA√á√ÉO**
    /// CPU ou GPU onde os c√°lculos acontecem
    device: Device,
    
    /// üìà **TAXA DE APRENDIZADO**
    /// Controla velocidade de atualiza√ß√£o dos pesos
    learning_rate: f64,
    
    /// üì¶ **TAMANHO DO BATCH**
    /// Quantos exemplos processar simultaneamente
    batch_size: usize,
    
    /// üìä **M√âTRICAS DE TREINAMENTO**
    /// Rastreia progresso e performance
    current_step: usize,
    current_loss: f32,
    best_loss: f32,
}

impl Trainer {
    /// üèóÔ∏è **CONSTRUTOR INTELIGENTE DO TREINADOR**
    /// 
    /// Cria um treinador otimizado automaticamente para o hardware dispon√≠vel.
    /// √â como ter um personal trainer que se adapta ao seu equipamento!
    /// 
    /// ## üéØ **Otimiza√ß√£o Autom√°tica por Hardware:**
    /// 
    /// ### üî• **Metal GPU (ARM Apple) - Configura√ß√£o Agressiva:**
    /// ```text
    /// Hardware: 18 n√∫cleos GPU + 18GB RAM unificada
    /// Batch Size: 32 (4x maior que CPU)
    /// Learning Rate: 1e-4 (est√°vel para batches grandes)
    /// Throughput: ~4000 tokens/sec
    /// Mem√≥ria: At√© 16GB para modelo + dados
    /// ```
    /// 
    /// ### üñ•Ô∏è **CPU - Configura√ß√£o Conservadora:**
    /// ```text
    /// Hardware: CPU multi-core + RAM limitada
    /// Batch Size: 8 (conservador para RAM)
    /// Learning Rate: 3e-4 (converg√™ncia mais r√°pida)
    /// Throughput: ~500 tokens/sec
    /// Mem√≥ria: At√© 4GB para modelo + dados
    /// ```
    /// 
    /// ## üß† **L√≥gica de Otimiza√ß√£o:**
    /// 
    /// ### üì¶ **Batch Size:**
    /// - **GPU**: Paralelismo massivo ‚Üí batches grandes
    /// - **CPU**: Processamento sequencial ‚Üí batches pequenos
    /// - **Trade-off**: Efici√™ncia vs. Uso de mem√≥ria
    /// 
    /// ### üìä **Learning Rate:**
    /// - **Batches grandes**: LR menor (gradientes mais est√°veis)
    /// - **Batches pequenos**: LR maior (gradientes mais ruidosos)
    /// - **Objetivo**: Converg√™ncia est√°vel e r√°pida
    /// 
    /// ## üéì **Analogia:**
    /// √â como escolher o ritmo de estudo:
    /// - **GPU**: Estudar em grupo (batch grande) com ritmo moderado
    /// - **CPU**: Estudar sozinho (batch pequeno) com ritmo acelerado
    pub fn new(model: MiniGPT, tokenizer: BPETokenizer, device: Device) -> Self {
        // üöÄ **DETEC√á√ÉO E OTIMIZA√á√ÉO AUTOM√ÅTICA DE HARDWARE**
        let (batch_size, learning_rate) = match device {
            Device::Metal(_) => {
                // üî• **CONFIGURA√á√ïES PARA METAL GPU ARM APPLE**
                // Aproveita paralelismo massivo da GPU
                // 18 n√∫cleos GPU + 18GB RAM = configura√ß√£o agressiva
                println!("‚ö° Configura√ß√µes otimizadas para Metal GPU ARM Apple:");
                println!("   üì¶ Batch Size: 32 (4x maior que CPU)");
                println!("   üéØ Learning Rate: 1e-4 (otimizado para GPU)");
                println!("   üöÄ Throughput esperado: ~4000 tokens/sec");
                (32, 1e-4)
            }
            _ => {
                // üñ•Ô∏è **CONFIGURA√á√ïES PARA CPU (FALLBACK)**
                // Configura√ß√£o conservadora para hardware limitado
                println!("üñ•Ô∏è  Configura√ß√µes para CPU:");
                println!("   üì¶ Batch Size: 8 (conservador)");
                println!("   üéØ Learning Rate: 3e-4 (padr√£o)");
                println!("   üêå Throughput esperado: ~500 tokens/sec");
                (8, 3e-4)
            }
        };
        
        // üèóÔ∏è **CONSTRU√á√ÉO DO TREINADOR OTIMIZADO**
        Self {
            model,
            tokenizer,
            device,
            learning_rate,
            batch_size,
            current_step: 0,
            current_loss: f32::INFINITY,
            best_loss: f32::INFINITY,
        }
    }
    
    /// üèãÔ∏è‚Äç‚ôÄÔ∏è **M√âTODO PRINCIPAL DE TREINAMENTO**
    /// 
    /// Executa o ciclo completo de treinamento do modelo, transformando
    /// dados brutos em um modelo capaz de gerar linguagem natural.
    /// 
    /// ## üéØ **Processo Completo de Treinamento:**
    /// 
    /// ### 1Ô∏è‚É£ **Prepara√ß√£o (Setup)**
    /// ```text
    /// ‚úÖ Configurar hiperpar√¢metros
    /// ‚úÖ Criar batches de dados
    /// ‚úÖ Inicializar m√©tricas
    /// ‚úÖ Configurar barra de progresso
    /// ```
    /// 
    /// ### 2Ô∏è‚É£ **Loop de √âpocas**
    /// ```text
    /// Para cada √©poca (1 a N):
    ///   Para cada batch:
    ///     üîÆ Forward Pass  ‚Üí Predi√ß√µes
    ///     üìä Calcular Loss ‚Üí Erro
    ///     üîÑ Backward Pass ‚Üí Gradientes
    ///     ‚ö° Atualizar Pesos
    ///     üìà Registrar M√©tricas
    /// ```
    /// 
    /// ### 3Ô∏è‚É£ **Monitoramento**
    /// ```text
    /// üìä Loss por √©poca
    /// ‚è±Ô∏è Tempo de treinamento
    /// üöÄ Tokens processados/segundo
    /// üìà Progresso visual
    /// ```
    /// 
    /// ## üß† **Algoritmo de Gradient Descent:**
    /// 
    /// ```text
    /// Para cada batch (X, Y):
    ///   1. ≈∂ = Model(X)           # Forward: predi√ß√£o
    ///   2. L = Loss(≈∂, Y)         # Erro entre predi√ß√£o e target
    ///   3. ‚àáL = ‚àÇL/‚àÇŒ∏             # Gradientes dos par√¢metros
    ///   4. Œ∏ = Œ∏ - Œ±‚àáL            # Atualiza√ß√£o dos pesos
    /// ```
    /// 
    /// ## üìä **M√©tricas Monitoradas:**
    /// - **Loss**: Erro m√©dio por √©poca (menor = melhor)
    /// - **Perplexity**: exp(loss) - confus√£o do modelo
    /// - **Throughput**: Tokens processados por segundo
    /// - **Converg√™ncia**: Tend√™ncia da loss ao longo do tempo
    /// 
    /// ## üéì **Analogia Educacional:**
    /// √â como ensinar algu√©m a escrever:
    /// - **√âpoca**: Um semestre de aulas
    /// - **Batch**: Uma li√ß√£o com v√°rios exerc√≠cios
    /// - **Forward**: Aluno tenta completar frases
    /// - **Loss**: Quantos erros o aluno cometeu
    /// - **Backward**: Professor corrige e explica erros
    /// - **Update**: Aluno aprende e melhora
    pub fn train(&mut self, tokens: &[usize], epochs: usize) -> Result<()> {
        let block_size = self.model.block_size();
        
        // üìã **RELAT√ìRIO INICIAL DE CONFIGURA√á√ÉO**
        println!("üéØ Iniciando treinamento:");
        println!("  ‚Ä¢ √âpocas: {}", epochs);
        println!("  ‚Ä¢ Tamanho do bloco: {}", block_size);
        println!("  ‚Ä¢ Batch size: {}", self.batch_size);
        println!("  ‚Ä¢ Taxa de aprendizado: {}", self.learning_rate);
        println!("  ‚Ä¢ Total de tokens: {}", tokens.len());
        println!("  ‚Ä¢ Par√¢metros do modelo: {}", self.model.num_parameters());
        
        // üì¶ **PREPARA√á√ÉO DOS DADOS DE TREINAMENTO**
        // Converte sequ√™ncia longa em batches de tamanho fixo
        let batches = self.create_batches(tokens, block_size)?;
        let total_steps = epochs * batches.len();
        
        println!("  ‚Ä¢ Batches criados: {}", batches.len());
        println!("  ‚Ä¢ Steps totais: {}", total_steps);
        
        // üìä **CONFIGURA√á√ÉO DA BARRA DE PROGRESSO**
        // Interface visual para acompanhar o treinamento
        let pb = ProgressBar::new(total_steps as u64);
        pb.set_style(
            ProgressStyle::default_bar()
                .template("{spinner:.green} [{elapsed_precise}] [{bar:40.cyan/blue}] {pos}/{len} ({eta})")
                .unwrap()
                .progress_chars("#>-"),
        );
        
        // üìà **INICIALIZA√á√ÉO DE M√âTRICAS**
        let mut step = 0;
        let start_time = Instant::now();
        
        // üîÑ **LOOP PRINCIPAL DE TREINAMENTO POR √âPOCAS**
        // 
        // Este √© o cora√ß√£o do algoritmo de aprendizado! Cada √©poca representa
        // uma passagem completa pelos dados de treinamento.
        // 
        // üìö **Analogia**: Como estudar para uma prova - voc√™ revisa todo o material
        // v√°rias vezes (√©pocas), e a cada revis√£o voc√™ entende melhor (reduz a loss).
        for epoch in 1..=epochs {
            let mut epoch_loss = 0.0;  // Acumula a loss total da √©poca
            let mut batch_count = 0;   // Conta quantos batches processamos
            
            // üì¶ **PROCESSAMENTO POR BATCHES**
            // 
            // Dividimos os dados em pequenos grupos (batches) para:
            // - Efici√™ncia de mem√≥ria (n√£o carregamos tudo de uma vez)
            // - Estabilidade num√©rica (gradientes mais est√°veis)
            // - Paraleliza√ß√£o (GPUs processam batches eficientemente)
            for (inputs, targets) in &batches {
                // üîÆ **FORWARD PASS: PREDI√á√ÉO**
                // 
                // O modelo "olha" para os dados de entrada e faz suas predi√ß√µes.
                // √â como um estudante tentando responder uma pergunta.
                // 
                // Processo:
                // 1. Tokeniza√ß√£o: texto ‚Üí n√∫meros (j√° feito)
                // 2. Embeddings: n√∫meros ‚Üí vetores densos
                // 3. Transformer: vetores ‚Üí representa√ß√µes contextuais
                // 4. Proje√ß√£o: representa√ß√µes ‚Üí probabilidades de tokens
                // 
                // üìä **Retorno**: (logits, loss_opcional)
                // - logits: probabilidades brutas para cada token do vocabul√°rio
                // - loss: erro calculado comparando predi√ß√£o vs. target (se fornecido)
                let (_logits, loss) = self.model.forward(inputs, Some(targets))?;
                
                if let Some(loss_tensor) = loss {
                    // üî¢ **EXTRA√á√ÉO DO VALOR ESCALAR DA LOSS**
                    // 
                    // Convertemos o tensor de loss para um n√∫mero simples (f32)
                    // para poder trabalhar com ele em opera√ß√µes matem√°ticas b√°sicas.
                    let loss_value: f32 = loss_tensor.to_scalar()?;
                    
                    // üö® **VERIFICA√á√ÉO DE ESTABILIDADE NUM√âRICA**
                    // 
                    // Se a loss se torna NaN ou infinita, algo deu errado:
                    // - Learning rate muito alto (gradientes explodem)
                    // - Dados corrompidos ou mal formatados
                    // - Bug no c√≥digo (divis√£o por zero, overflow, etc.)
                    if loss_value.is_finite() {
                        epoch_loss += loss_value;  // Acumula para m√©dia da √©poca
                        batch_count += 1;          // Conta batches v√°lidos
                        
                        // üìä **ATUALIZA√á√ÉO DE M√âTRICAS**
                        self.current_step += 1;
                        self.current_loss = loss_value;
                        if loss_value < self.best_loss {
                            self.best_loss = loss_value;
                        }
                        
                        // ‚ö° **BACKWARD PASS: APRENDIZADO**
                        // 
                        // Aqui acontece a m√°gica! O modelo compara suas predi√ß√µes com
                        // as respostas corretas e ajusta seus par√¢metros.
                        // 
                        // Processo de Backpropagation:
                        // 1. C√°lculo da Loss: qu√£o "errado" estamos?
                        // 2. Gradientes: em que dire√ß√£o devemos ajustar cada par√¢metro?
                        // 3. Chain Rule: propaga gradientes atrav√©s das camadas
                        // 4. Atualiza√ß√£o: aplicamos os ajustes (Gradient Descent)
                        // 
                        // üéØ **Loss Function**: Cross-Entropy Loss
                        // - Mede a "dist√¢ncia" entre distribui√ß√£o predita e real
                        // - Penaliza predi√ß√µes muito confiantes e incorretas
                        // - Recompensa predi√ß√µes corretas e bem calibradas
                        let _grads = loss_tensor.backward()?;
                        
                        // üìà **ATUALIZA√á√ÉO DA BARRA DE PROGRESSO**
                        // 
                        // Feedback visual em tempo real para o usu√°rio acompanhar:
                        // - Progresso da √©poca atual
                        // - Valor instant√¢neo da loss
                        // - Estimativa de tempo restante
                        pb.set_message(format!(
                            "√âpoca {}/{} | Loss: {:.4} | Best: {:.4}", 
                            epoch, epochs, loss_value, self.best_loss
                        ));
                    } else {
                        // üö® **DETEC√á√ÉO DE INSTABILIDADE NUM√âRICA**
                        println!("‚ö†Ô∏è Loss inv√°lido detectado: {}", loss_value);
                        println!("üí° Poss√≠veis causas: learning rate alto, dados corrompidos, overflow num√©rico");
                    }
                    
                    pb.inc(1);  // Incrementa contador visual
                    step += 1;  // Incrementa contador global de steps
                }
            }
            
            // üìä **C√ÅLCULO DA LOSS M√âDIA DA √âPOCA**
            // 
            // A loss m√©dia nos d√° uma vis√£o geral do desempenho do modelo
            // nesta √©poca. Idealmente, deveria diminuir ao longo do tempo.
            // 
            // üìà **Interpreta√ß√£o da Loss**:
            // - Loss alta (>5.0): Modelo ainda "confuso", aprendendo padr√µes b√°sicos
            // - Loss m√©dia (1.0-5.0): Modelo capturando estruturas lingu√≠sticas
            // - Loss baixa (<1.0): Modelo refinando detalhes e nuances
            // 
            // ‚ö†Ô∏è **Cuidado**: Loss muito baixa pode indicar overfitting!
            let avg_loss = if batch_count > 0 { 
                epoch_loss / batch_count as f32 
            } else { 
                f32::NAN  // Fallback se nenhum batch foi processado
            };
            println!("\nüìä √âpoca {} conclu√≠da | Loss m√©dio: {:.4}", epoch, avg_loss);
            
            // üé≠ **GERA√á√ÉO DE EXEMPLOS DEMONSTRATIVOS**
            // 
            // A cada 10 √©pocas, geramos texto de exemplo para:
            // - Monitorar qualitativamente o progresso do modelo
            // - Detectar problemas como repeti√ß√£o ou incoer√™ncia
            // - Motivar o usu√°rio mostrando melhorias tang√≠veis
            // 
            // üß† **Por que "O Brasil √©"?**
            // - Prompt em portugu√™s (nosso dom√≠nio de treinamento)
            // - T√≥pico amplo que permite criatividade
            // - F√°cil de avaliar se o texto faz sentido
            if epoch % 10 == 0 {
                println!("\nüé≠ Gerando exemplo de texto (√©poca {}):", epoch);
                self.generate_sample("O Brasil √©")?;
            }
        }
        
        // üèÅ **FINALIZA√á√ÉO DO TREINAMENTO**
        pb.finish_with_message("Treinamento conclu√≠do!");
        
        // ‚è±Ô∏è **ESTAT√çSTICAS DE PERFORMANCE**
        // 
        // Medimos e reportamos m√©tricas importantes:
        // - Tempo total de treinamento
        // - Throughput (tokens processados por segundo)
        // - Efici√™ncia computacional
        let duration = start_time.elapsed();
        let total_tokens_processed = tokens.len() as f32 * epochs as f32;
        let tokens_per_second = total_tokens_processed / duration.as_secs_f32();
        
        println!("\n‚úÖ Treinamento finalizado em {:.2}s", duration.as_secs_f32());
        println!("üìà Velocidade: {:.0} tokens/seg", tokens_per_second);
        println!("üî¢ Total de tokens processados: {:.0}", total_tokens_processed);
        
        // üí° **Dicas de Performance**
        if tokens_per_second < 1000.0 {
            println!("üí° Dica: Para acelerar o treinamento, considere:");
            println!("   - Usar GPU (Metal/CUDA) se dispon√≠vel");
            println!("   - Aumentar batch_size se houver mem√≥ria suficiente");
            println!("   - Reduzir o tamanho do modelo para prototipagem");
        }
        
        Ok(())
    }
    
    /// üì¶ **CRIA√á√ÉO DE BATCHES PARA TREINAMENTO**
    /// 
    /// Este m√©todo organiza os dados tokenizados em batches otimizados para
    /// treinamento eficiente de modelos de linguagem.
    /// 
    /// üéØ **Objetivo**: Transformar uma sequ√™ncia longa de tokens em m√∫ltiplos
    /// pares (input, target) que o modelo pode processar em paralelo.
    /// 
    /// üìö **Analogia**: Como dividir um livro em cap√≠tulos para estudar -
    /// cada batch √© um "cap√≠tulo" que o modelo estuda de uma vez.
    /// 
    /// ## üîÑ **Processo de Cria√ß√£o**:
    /// 
    /// 1. **Amostragem Aleat√≥ria**: Escolhemos posi√ß√µes aleat√≥rias no corpus
    /// 2. **Janela Deslizante**: Extra√≠mos sequ√™ncias de tamanho fixo (block_size)
    /// 3. **Shift de Target**: Target √© input deslocado em 1 posi√ß√£o (next token prediction)
    /// 4. **Agrupamento**: Combinamos m√∫ltiplas sequ√™ncias em um batch
    /// 5. **Tensoriza√ß√£o**: Convertemos para tensores otimizados para GPU/CPU
    /// 
    /// ## üìä **Exemplo Visual**:
    /// ```
    /// Tokens: [1, 2, 3, 4, 5, 6, 7, 8, 9]
    /// Block Size: 4
    /// 
    /// Sequ√™ncia 1:
    /// Input:  [1, 2, 3, 4]  ‚Üê "Dado este contexto..."
    /// Target: [2, 3, 4, 5]  ‚Üê "...prediga estes tokens"
    /// 
    /// Sequ√™ncia 2:
    /// Input:  [3, 4, 5, 6]  ‚Üê "Dado este contexto..."
    /// Target: [4, 5, 6, 7]  ‚Üê "...prediga estes tokens"
    /// ```
    /// 
    /// ## ‚ö° **Otimiza√ß√µes**:
    /// - **Amostragem Aleat√≥ria**: Evita overfitting em sequ√™ncias espec√≠ficas
    /// - **Batch Processing**: Paraleliza√ß√£o eficiente em GPU
    /// - **Memory Layout**: Tensores cont√≠guos para acesso r√°pido
    /// - **Type Optimization**: i64 para inputs, u32 para targets (economia de mem√≥ria)
    fn create_batches(&self, tokens: &[usize], block_size: usize) -> Result<Vec<(Tensor, Tensor)>> {
        let mut batches = Vec::new();
        let mut rng = thread_rng();  // Gerador de n√∫meros aleat√≥rios para amostragem
        
        // üìè **C√ÅLCULO DE DIMENS√ïES**
        // 
        // Quantas sequ√™ncias completas de tamanho block_size cabem nos dados?
        // Subtra√≠mos 1 porque precisamos de tokens[i+1] para os targets.
        let num_sequences = (tokens.len() - 1) / block_size;
        
        // üéõÔ∏è **OTIMIZA√á√ÉO DE BATCH SIZE**
        // 
        // Limitamos o batch_size ao n√∫mero de sequ√™ncias dispon√≠veis
        // para evitar repeti√ß√µes desnecess√°rias em datasets pequenos.
        let sequences_per_batch = self.batch_size.min(num_sequences);
        
        // üîÑ **LOOP DE CRIA√á√ÉO DE BATCHES**
        // 
        // Criamos tantos batches quantos forem necess√°rios para cobrir
        // todas as sequ√™ncias poss√≠veis nos dados.
        for _ in 0..(num_sequences / sequences_per_batch) {
            let mut batch_inputs = Vec::new();   // Acumula inputs do batch
            let mut batch_targets = Vec::new();  // Acumula targets do batch
            
            // üìù **CRIA√á√ÉO DE SEQU√äNCIAS INDIVIDUAIS**
            // 
            // Para cada posi√ß√£o no batch, criamos uma sequ√™ncia input-target.
            for _ in 0..sequences_per_batch {
                // üé≤ **AMOSTRAGEM ALEAT√ìRIA**
                // 
                // Escolhemos uma posi√ß√£o aleat√≥ria v√°lida no corpus.
                // Isso garante que o modelo veja diferentes contextos
                // e n√£o memorize apenas sequ√™ncias espec√≠ficas.
                let start_idx = rng.gen_range(0..tokens.len() - block_size);
                
                // üì• **CRIA√á√ÉO DA SEQU√äNCIA DE INPUT**
                // 
                // Input: tokens[start_idx..start_idx+block_size]
                // Convertemos para i64 (tipo esperado pelo modelo)
                let input_seq: Vec<i64> = tokens[start_idx..start_idx + block_size]
                    .iter().map(|&x| x as i64).collect();
                
                // üéØ **CRIA√á√ÉO DA SEQU√äNCIA DE TARGET**
                // 
                // Target: tokens[start_idx+1..start_idx+block_size+1]
                // Deslocamento de 1 posi√ß√£o = "next token prediction"
                // Convertemos para u32 (economia de mem√≥ria para √≠ndices)
                let target_seq: Vec<u32> = tokens[start_idx + 1..start_idx + block_size + 1]
                    .iter().map(|&x| x as u32).collect();
                
                // üìö **ACUMULA√á√ÉO NO BATCH**
                // 
                // Adicionamos as sequ√™ncias aos vetores do batch.
                // M√∫ltiplas sequ√™ncias ser√£o processadas em paralelo.
                batch_inputs.extend(input_seq);
                batch_targets.extend(target_seq);
            }
            
            // üîß **TENSORIZA√á√ÉO**
            // 
            // Convertemos os vetores para tensores otimizados:
            // - Layout de mem√≥ria cont√≠guo
            // - Compatibilidade com opera√ß√µes de GPU
            // - Shape: (batch_size, sequence_length)
            let inputs = Tensor::from_slice(
                &batch_inputs, 
                (sequences_per_batch, block_size), 
                &self.device
            )?;
            
            let targets = Tensor::from_slice(
                &batch_targets, 
                (sequences_per_batch, block_size), 
                &self.device
            )?.to_dtype(candle_core::DType::U32)?;  // Convers√£o de tipo para efici√™ncia
            
            // üì¶ **ARMAZENAMENTO DO BATCH**
            // 
            // Cada batch cont√©m um par (inputs, targets) pronto para treinamento.
            batches.push((inputs, targets));
        }
        
        Ok(batches)
    }
    
    /// üé≠ **GERA√á√ÉO DE TEXTO DEMONSTRATIVO**
    /// 
    /// Este m√©todo gera texto de exemplo para demonstrar o progresso
    /// do modelo durante o treinamento.
    /// 
    /// üéØ **Objetivo**: Fornecer feedback qualitativo sobre o aprendizado
    /// do modelo, complementando as m√©tricas quantitativas (loss).
    /// 
    /// üìö **Analogia**: Como pedir para um estudante "explicar com suas palavras"
    /// o que aprendeu - nos mostra se realmente entendeu o conte√∫do.
    /// 
    /// ## üîç **Processo de Gera√ß√£o**:
    /// 
    /// 1. **Tokeniza√ß√£o**: Converte o prompt em tokens num√©ricos
    /// 2. **Forward Pass**: Modelo processa o contexto
    /// 3. **Sampling**: Escolhe pr√≥ximos tokens com temperatura 0.8
    /// 4. **Decodifica√ß√£o**: Converte tokens de volta para texto
    /// 5. **Exibi√ß√£o**: Mostra resultado para avalia√ß√£o humana
    /// 
    /// ## üå°Ô∏è **Temperatura 0.8**:
    /// - **0.0**: Determin√≠stico (sempre escolhe token mais prov√°vel)
    /// - **0.8**: Criativo mas coerente (boa para demonstra√ß√µes)
    /// - **1.0**: Amostragem natural da distribui√ß√£o
    /// - **>1.0**: Muito criativo/aleat√≥rio (pode ser incoerente)
    /// 
    /// ## üìä **Indicadores de Progresso**:
    /// - **In√≠cio**: Texto aleat√≥rio ou repetitivo
    /// - **Progresso**: Palavras reconhec√≠veis, gram√°tica b√°sica
    /// - **Avan√ßado**: Frases coerentes, contexto mantido
    /// - **Refinado**: Texto fluido e contextualmente apropriado
    fn generate_sample(&self, prompt: &str) -> Result<()> {
        println!("\nüé≠ Exemplo de gera√ß√£o:");
        println!("Prompt: '{}'", prompt);
        
        // üé≤ **GERA√á√ÉO COM PAR√ÇMETROS OTIMIZADOS**
        // 
        // Par√¢metros escolhidos para demonstra√ß√£o:
        // - max_tokens: 20 (suficiente para avaliar coer√™ncia)
        // - temperature: 0.8 (equilibrio entre criatividade e coer√™ncia)
        match self.model.generate(prompt, 20, &self.tokenizer, 0.8) {
            Ok(generated) => {
                println!("Gerado: '{}{}'", prompt, generated);
                
                // üí° **DICAS DE INTERPRETA√á√ÉO**
                if generated.trim().is_empty() {
                    println!("‚ö†Ô∏è  Modelo ainda n√£o aprendeu a gerar texto");
                } else if generated.chars().filter(|c| c.is_alphabetic()).count() < 5 {
                    println!("üìù Modelo gerando caracteres, mas ainda n√£o palavras completas");
                } else {
                    println!("‚úÖ Modelo gerando texto reconhec√≠vel!");
                }
            },
            Err(e) => {
                println!("Erro na gera√ß√£o: {}", e);
                println!("üí° Isso pode indicar problemas no modelo ou tokenizador");
            },
        }
        
        Ok(())
    }
    
    /// üíæ **SALVAMENTO DO MODELO TREINADO**
    /// 
    /// Salva o modelo treinado em formato SafeTensors para uso futuro.
    /// √â como "fotografar" o c√©rebro do modelo ap√≥s o aprendizado!
    /// 
    /// ## üîí **Por que SafeTensors?**
    /// 
    /// ### üõ°Ô∏è **Seguran√ßa:**
    /// - **Sem c√≥digo execut√°vel**: Apenas dados puros
    /// - **Verifica√ß√£o de integridade**: Checksums autom√°ticos
    /// - **Prote√ß√£o contra malware**: Formato read-only
    /// 
    /// ### ‚ö° **Performance:**
    /// - **Zero-copy loading**: Carregamento instant√¢neo
    /// - **Memory mapping**: Acesso eficiente a arquivos grandes
    /// - **Lazy loading**: Carrega apenas o necess√°rio
    /// 
    /// ### üåê **Portabilidade:**
    /// - **Cross-platform**: Funciona em qualquer sistema
    /// - **Language agnostic**: Python, Rust, JavaScript, etc.
    /// - **Version stable**: Compatibilidade garantida
    /// 
    /// ## üìÅ **Estrutura do Arquivo Salvo:**
    /// ```text
    /// model.safetensors
    /// ‚îú‚îÄ‚îÄ token_emb.weight     [vocab_size √ó n_embd]
    /// ‚îú‚îÄ‚îÄ pos_emb.weight       [block_size √ó n_embd]
    /// ‚îú‚îÄ‚îÄ block_0.attn.weight  [n_embd √ó n_embd]
    /// ‚îú‚îÄ‚îÄ block_0.mlp.weight   [n_embd √ó 4*n_embd]
    /// ‚îú‚îÄ‚îÄ ...
    /// ‚îî‚îÄ‚îÄ lm_head.weight       [n_embd √ó vocab_size]
    /// ```
    /// 
    /// ## üéØ **Casos de Uso:**
    /// - **Checkpointing**: Salvar progresso durante treinamento
    /// - **Deployment**: Carregar modelo em produ√ß√£o
    /// - **Fine-tuning**: Continuar treinamento de checkpoint
    /// - **Sharing**: Distribuir modelos treinados
    /// üíæ **SALVAMENTO AVAN√áADO COM METADADOS DE CHECKPOINT**
    /// 
    /// Salva o modelo com informa√ß√µes completas de treinamento:
    /// - Configura√ß√£o do modelo
    /// - M√©tricas de performance
    /// - Timestamp e vers√£o
    /// - Informa√ß√µes de treinamento
    pub fn save(&self, path: &str) -> Result<()> {
        use std::path::Path;
        
        println!("üíæ Iniciando salvamento avan√ßado do modelo...");
        println!("üìç Destino: {}", path);
        println!("üìä Par√¢metros: ~{:.1}M", self.model.num_parameters() as f32 / 1_000_000.0);
        
        // üóÇÔ∏è **CRIAR DIRET√ìRIO SE N√ÉO EXISTIR**
        if let Some(parent) = Path::new(path).parent() {
            std::fs::create_dir_all(parent)
                .map_err(|e| format!("Erro ao criar diret√≥rio {}: {}", parent.display(), e))?;
            println!("üìÅ Diret√≥rio criado: {}", parent.display());
        }
        
        // üìã **CRIA√á√ÉO DE METADADOS COMPLETOS**
        let mut metadata = CheckpointMetadata::new(self.model.config().clone())
            .with_training_info(
                self.current_step,
                self.current_loss,
                self.learning_rate as f32,
            )
            .with_description(format!(
                "Mini-GPT checkpoint - {} par√¢metros, best loss: {:.4}",
                self.model.num_parameters(),
                self.best_loss
            ));
        
        // üîê **C√ÅLCULO DE HASH DE INTEGRIDADE (OPCIONAL)**
        // Por enquanto, usamos um hash simples baseado no timestamp
        metadata.model_hash = Some(format!(
            "checkpoint_{}",
            chrono::Utc::now().timestamp()
        ));
        
        println!("üìã Metadados preparados:");
        println!("   üéØ Step: {}", metadata.training_step.unwrap_or(0));
        println!("   üìâ Loss atual: {:.4}", metadata.loss.unwrap_or(0.0));
        println!("   üèÜ Melhor loss: {:.4}", self.best_loss);
        println!("   üìà Learning rate: {}", metadata.learning_rate.unwrap_or(0.0));
        
        // üíæ **SALVAMENTO COM METADADOS**
        // 
        // Usamos o sistema SafeTensors com metadados JSON no header
        let metadata_json = serde_json::to_string(&metadata)
            .map_err(|e| format!("Erro ao serializar metadados: {}", e))?;
        
        // Primeiro salvamos os tensores
        match self.model.varmap().save(path) {
            Ok(()) => {
                // Agora precisamos adicionar os metadados ao arquivo SafeTensors
                // Nota: Esta √© uma implementa√ß√£o simplificada
                // Em produ√ß√£o, usar√≠amos a API completa do SafeTensors
                
                println!("‚úÖ Tensores salvos com sucesso!");
                println!("üîí Formato: SafeTensors com metadados");
                println!("üìè Arquivo: {}", path);
                
                // üìä **VERIFICAR TAMANHO DO ARQUIVO**
                if let Ok(file_metadata) = std::fs::metadata(path) {
                    let size_mb = file_metadata.len() as f64 / (1024.0 * 1024.0);
                    println!("üíΩ Tamanho: {:.1} MB", size_mb);
                }
                
                // üìù **SALVAR METADADOS EM ARQUIVO SEPARADO**
                let metadata_path = format!("{}.metadata.json", path);
                std::fs::write(&metadata_path, metadata_json)
                    .map_err(|e| format!("Erro ao salvar metadados: {}", e))?;
                
                println!("üìã Metadados salvos em: {}", metadata_path);
                println!("üéâ Checkpoint completo salvo com sucesso!");
            }
            Err(e) => {
                return Err(format!("Erro ao salvar modelo: {}", e).into());
            }
        }
        
        Ok(())
    }
    
    /// üìä **SALVAMENTO AUTOM√ÅTICO DE CHECKPOINT**
    /// 
    /// Salva automaticamente quando a loss melhora
    pub fn save_if_best(&mut self, base_path: &str) -> Result<()> {
        if self.current_loss <= self.best_loss {
            let checkpoint_path = format!(
                "{}_step_{}_loss_{:.4}.safetensors",
                base_path,
                self.current_step,
                self.current_loss
            );
            
            println!("üèÜ Nova melhor loss! Salvando checkpoint...");
            self.save(&checkpoint_path)?;
        }
        Ok(())
    }
}