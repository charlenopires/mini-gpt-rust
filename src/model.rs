//! # üß† Mini-GPT: Construindo um Large Language Model do Zero
//!
//! ## üìö GUIA EDUCACIONAL COMPLETO: Como Construir um LLM
//!
//! Este arquivo implementa um **Large Language Model (LLM)** completo baseado na
//! arquitetura **Transformer GPT**. Vamos explicar cada componente em detalhes
//! para que voc√™ entenda exatamente como um "c√©rebro artificial" funciona!
//!
//! ## üéØ O QUE √â UM LARGE LANGUAGE MODEL?
//!
//! Um LLM √© um modelo de IA que:
//! - **Entende** texto em linguagem natural
//! - **Gera** texto coerente e contextualmente relevante
//! - **Aprende** padr√µes da linguagem a partir de grandes volumes de texto
//! - **Generaliza** conhecimento para tarefas n√£o vistas durante o treinamento
//!
//! ### üßÆ MATEM√ÅTICA POR TR√ÅS DOS LLMs:
//!
//! **Objetivo**: Dado uma sequ√™ncia de tokens [t‚ÇÅ, t‚ÇÇ, ..., t‚Çô], predizer t‚Çô‚Çä‚ÇÅ
//!
//! **Fun√ß√£o de Probabilidade**:
//! ```
//! P(t‚Çô‚Çä‚ÇÅ | t‚ÇÅ, t‚ÇÇ, ..., t‚Çô) = softmax(f(t‚ÇÅ, t‚ÇÇ, ..., t‚Çô))
//! ```
//!
//! Onde `f()` √© nossa rede neural Transformer que mapeia sequ√™ncias para distribui√ß√µes
//! de probabilidade sobre o vocabul√°rio.
//!
//! ## üèóÔ∏è ARQUITETURA TRANSFORMER: OS BLOCOS FUNDAMENTAIS
//!
//! ### 1. üìö **TOKEN EMBEDDINGS** - Convertendo Palavras em N√∫meros
//!
//! **Problema**: Computadores n√£o entendem palavras, apenas n√∫meros.
//! **Solu√ß√£o**: Mapear cada palavra para um vetor de n√∫meros reais.
//!
//! ```
//! "gato" ‚Üí [0.2, -0.1, 0.8, 0.3, ...] (vetor de 512 dimens√µes)
//! "c√£o"  ‚Üí [0.1, -0.2, 0.7, 0.4, ...] (vetor similar, pois s√£o animais)
//! ```
//!
//! **Por que funciona?**
//! - Palavras similares t√™m vetores similares
//! - O modelo aprende essas representa√ß√µes durante o treinamento
//! - Permite opera√ß√µes matem√°ticas com conceitos lingu√≠sticos
//!
//! ### 2. üìç **POSITION EMBEDDINGS** - Ensinando Ordem ao Modelo
//!
//! **Problema**: Transformers processam tokens em paralelo, perdendo no√ß√£o de ordem.
//! **Solu√ß√£o**: Adicionar informa√ß√£o posicional a cada token.
//!
//! ```
//! "Jo√£o ama Maria" vs "Maria ama Jo√£o"
//! Posi√ß√£o 0: Jo√£o/Maria + embedding_pos[0]
//! Posi√ß√£o 1: ama + embedding_pos[1]
//! Posi√ß√£o 2: Maria/Jo√£o + embedding_pos[2]
//! ```
//!
//! ### 3. üéØ **MULTI-HEAD ATTENTION** - O Cora√ß√£o do Transformer
//!
//! **Conceito**: Cada palavra "presta aten√ß√£o" a todas as outras palavras.
//!
//! **Matem√°tica da Aten√ß√£o**:
//! ```
//! Q = X * W_q  (Query: "o que estou procurando?")
//! K = X * W_k  (Key: "o que eu ofere√ßo?")
//! V = X * W_v  (Value: "qual informa√ß√£o eu carrego?")
//!
//! Attention(Q,K,V) = softmax(QK^T / ‚àöd_k) * V
//! ```
//!
//! **Por que m√∫ltiplas cabe√ßas?**
//! - Cada cabe√ßa captura um tipo diferente de rela√ß√£o
//! - Cabe√ßa 1: rela√ß√µes sint√°ticas (sujeito-verbo)
//! - Cabe√ßa 2: rela√ß√µes sem√¢nticas (causa-efeito)
//! - Cabe√ßa 3: rela√ß√µes de longa dist√¢ncia
//!
//! ### 4. ‚ö° **FEED-FORWARD NETWORKS** - Processamento N√£o-Linear
//!
//! **Fun√ß√£o**: Aplicar transforma√ß√µes complexas a cada posi√ß√£o.
//!
//! **Arquitetura**:
//! ```
//! FFN(x) = max(0, xW‚ÇÅ + b‚ÇÅ)W‚ÇÇ + b‚ÇÇ
//! ```
//!
//! **Fluxo de Processamento**:
//! ```
//! Input [B, T, C] ‚Üí Linear‚ÇÅ [B, T, 4C] ‚Üí GELU ‚Üí Linear‚ÇÇ [B, T, C]
//! ```
//!
//! **Por que 4x expans√£o?**
//! - Permite ao modelo "pensar" em um espa√ßo maior
//! - Captura intera√ß√µes complexas entre features
//! - Compensa a linearidade da aten√ß√£o
//!
//! ### 5. üîÑ **RESIDUAL CONNECTIONS** - Facilitando o Aprendizado
//!
//! **Problema**: Redes profundas sofrem com gradientes que desaparecem.
//! **Solu√ß√£o**: Adicionar conex√µes diretas entre camadas.
//!
//! ```
//! output = input + transformation(input)
//! ```
//!
//! **Benef√≠cios**:
//! - Gradientes fluem diretamente para camadas anteriores
//! - Permite treinar redes muito profundas (100+ camadas)
//! - Modelo aprende refinamentos incrementais
//!
//! ### 6. ‚öñÔ∏è **LAYER NORMALIZATION** - Estabilizando o Treinamento
//!
//! **Fun√ß√£o**: Normalizar ativa√ß√µes para ter m√©dia 0 e vari√¢ncia 1.
//!
//! **F√≥rmula**:
//! ```
//! LayerNorm(x) = Œ≥ * (x - Œº) / œÉ + Œ≤
//! ```
//!
//! **Onde**:
//! - Œº: m√©dia das ativa√ß√µes
//! - œÉ: desvio padr√£o das ativa√ß√µes  
//! - Œ≥, Œ≤: par√¢metros aprendidos
//!
//! ## üéì PROCESSO DE TREINAMENTO: COMO O MODELO APRENDE
//!
//! ### üìñ **1. TOKENIZA√á√ÉO**
//! ```
//! "O gato subiu" ‚Üí [15, 234, 1891] (IDs dos tokens)
//! ```
//!
//! ### üî¢ **2. EMBEDDING**
//! ```
//! [15, 234, 1891] ‚Üí [[0.1, 0.2, ...], [0.3, 0.1, ...], [0.8, 0.4, ...]]
//! ```
//!
//! ### üß† **3. PROCESSAMENTO TRANSFORMER**
//! ```
//! Para cada bloco:
//!   x = x + MultiHeadAttention(LayerNorm(x))
//!   x = x + FeedForward(LayerNorm(x))
//! ```
//!
//! ### üéØ **4. PREDI√á√ÉO**
//! ```
//! hidden_states ‚Üí logits ‚Üí softmax ‚Üí probabilidades
//! ```
//!
//! ### üìä **5. LOSS CALCULATION**
//! ```
//! loss = CrossEntropy(predicted_probs, actual_next_token)
//! ```
//!
//! ### ‚¨ÖÔ∏è **6. BACKPROPAGATION**
//! ```
//! Ajustar pesos para minimizar loss usando gradientes
//! ```
//!
//! ## üöÄ OTIMIZA√á√ïES DE PERFORMANCE
//!
//! ### üî• **KERNEL FUSION**
//! - Combina m√∫ltiplas opera√ß√µes em uma √∫nica passada
//! - Reduz overhead de mem√≥ria e comunica√ß√£o
//! - Melhora utiliza√ß√£o de cache
//!
//! ### üß† **MEMORY MANAGEMENT**
//! - Pool de mem√≥ria reutiliz√°vel
//! - Reduz fragmenta√ß√£o
//! - Otimiza aloca√ß√µes/desaloca√ß√µes
//!
//! ### ‚ö° **MIXED PRECISION**
//! - Usa FP16 para forward pass (2x mais r√°pido)
//! - Mant√©m FP32 para gradientes (precis√£o)
//! - Reduz uso de mem√≥ria pela metade
//!
//! ```
//! FFN(x) = max(0, x * W‚ÇÅ + b‚ÇÅ) * W‚ÇÇ + b‚ÇÇ
//! ```
//!
//! **Intui√ß√£o**: Como neur√¥nios no c√©rebro, cada FFN detecta padr√µes espec√≠ficos
//! e os transforma em representa√ß√µes mais √∫teis.
//!
//! ### 5. ‚öñÔ∏è **LAYER NORMALIZATION** - Estabilizando o Aprendizado
//!
//! **Problema**: Gradientes podem explodir ou desaparecer em redes profundas.
//! **Solu√ß√£o**: Normalizar ativa√ß√µes para ter m√©dia 0 e vari√¢ncia 1.
//!
//! ```
//! LayerNorm(x) = Œ≥ * (x - Œº) / œÉ + Œ≤
//! ```
//!
//! ## üîÑ PROCESSO AUTOREGRESSIVO: Como o Modelo Gera Texto
//!
//! **Passo a Passo da Gera√ß√£o**:
//!
//! 1. **Entrada**: "O gato subiu na"
//! 2. **Tokeniza√ß√£o**: [15, 234, 567, 89] (IDs dos tokens)
//! 3. **Embeddings**: Converter IDs em vetores densos
//! 4. **Transformer**: Processar atrav√©s de N camadas
//! 5. **Proje√ß√£o**: Mapear para probabilidades sobre vocabul√°rio
//! 6. **Sampling**: Escolher pr√≥ximo token baseado nas probabilidades
//! 7. **Repetir**: Adicionar token escolhido e continuar
//!
//! **Resultado**: "O gato subiu na √°rvore" (token "√°rvore" foi predito)
//!
//! ## üéì PROCESSO DE TREINAMENTO: Como o Modelo Aprende
//!
//! ### Forward Pass (Propaga√ß√£o Direta):
//! ```
//! Texto ‚Üí Tokens ‚Üí Embeddings ‚Üí Transformer ‚Üí Logits ‚Üí Loss
//! ```
//!
//! ### Backward Pass (Retropropaga√ß√£o):
//! ```
//! Loss ‚Üí ‚àÇLoss/‚àÇW ‚Üí Gradientes ‚Üí Atualiza√ß√£o dos Pesos
//! ```
//!
//! ### Fun√ß√£o de Loss (Cross-Entropy):
//! ```
//! Loss = -Œ£ log(P(token_correto | contexto))
//! ```
//!
//! **Objetivo**: Maximizar a probabilidade do token correto dado o contexto.
//!
//! ## üí° POR QUE ESTA ARQUITETURA FUNCIONA?
//!
//! 1. **Paraleliza√ß√£o**: Processa toda sequ√™ncia simultaneamente
//! 2. **Aten√ß√£o**: Captura depend√™ncias de longa dist√¢ncia
//! 3. **Profundidade**: M√∫ltiplas camadas permitem abstra√ß√µes complexas
//! 4. **Escala**: Funciona melhor com mais dados e par√¢metros
//! 5. **Generaliza√ß√£o**: Aprende padr√µes transfer√≠veis
//!
//! Este arquivo implementa todos esses conceitos em Rust puro,
//! criando um LLM funcional e educativo!

use candle_core::{DType, Device, Tensor, IndexOp, Var};
use candle_nn::{embedding, layer_norm, linear, Embedding, LayerNorm, Linear, Module, VarBuilder, VarMap};
use crate::transformer::TransformerBlock;
use crate::tokenizer::BPETokenizer;
use crate::kernels::{FusionConfig, FusedMemoryManager};
use safetensors::SafeTensors;
// use std::collections::HashMap; // Removido - n√£o utilizado
use std::fs;
use std::path::Path;
use serde::{Deserialize, Serialize};

type Result<T> = std::result::Result<T, Box<dyn std::error::Error + Send + Sync>>;

/// üìã **METADADOS DO CHECKPOINT**
/// 
/// Estrutura que armazena informa√ß√µes essenciais sobre o modelo salvo:
/// - Configura√ß√£o completa do modelo
/// - Timestamp de cria√ß√£o
/// - Vers√£o do formato
/// - M√©tricas de treinamento
/// - Hash de integridade
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CheckpointMetadata {
    pub config: GPTConfig,
    pub timestamp: String,
    pub version: String,
    pub training_step: Option<usize>,
    pub loss: Option<f32>,
    pub learning_rate: Option<f32>,
    pub model_hash: Option<String>,
    pub description: Option<String>,
}

impl CheckpointMetadata {
    pub fn new(config: GPTConfig) -> Self {
        Self {
            config,
            timestamp: chrono::Utc::now().to_rfc3339(),
            version: env!("CARGO_PKG_VERSION").to_string(),
            training_step: None,
            loss: None,
            learning_rate: None,
            model_hash: None,
            description: None,
        }
    }
    
    pub fn with_training_info(mut self, step: usize, loss: f32, lr: f32) -> Self {
        self.training_step = Some(step);
        self.loss = Some(loss);
        self.learning_rate = Some(lr);
        self
    }
    
    pub fn with_description(mut self, description: String) -> Self {
        self.description = Some(description);
        self
    }
}

/// üîß **CONFIGURA√á√ÉO DO MODELO GPT** - Os "Genes" do Nosso LLM
///
/// Esta estrutura define a "arquitetura gen√©tica" do nosso modelo.
/// Cada par√¢metro controla um aspecto fundamental do comportamento do LLM.
///
/// ## üìä **HIPERPAR√ÇMETROS EXPLICADOS EM DETALHES**:
///
/// ### `vocab_size` üìö - Tamanho do Vocabul√°rio
/// - **O que √©**: Quantas palavras/tokens diferentes o modelo conhece
/// - **Exemplo**: 50,000 = modelo conhece 50 mil palavras √∫nicas
/// - **Impacto**: Maior vocabul√°rio = mais expressivo, mas mais mem√≥ria
/// - **Analogia**: Como o "dicion√°rio" que o modelo tem acesso
/// - **Matem√°tica**: Determina dimens√£o da matriz de sa√≠da (vocab_size √ó n_embd)
///
/// ### `n_embd` üßÆ - Dimens√£o dos Embeddings
/// - **O que √©**: Tamanho dos vetores que representam cada palavra
/// - **Exemplo**: 512 = cada palavra vira um vetor de 512 n√∫meros
/// - **Impacto**: Maior dimens√£o = mais capacidade expressiva
/// - **Analogia**: "Resolu√ß√£o" da representa√ß√£o das palavras
/// - **Trade-off**: Mais dimens√µes = mais par√¢metros = mais mem√≥ria/computa√ß√£o
///
/// ### `n_head` üëÅÔ∏è - N√∫mero de Cabe√ßas de Aten√ß√£o
/// - **O que √©**: Quantos "focos de aten√ß√£o" paralelos o modelo tem
/// - **Exemplo**: 8 cabe√ßas = 8 tipos diferentes de rela√ß√µes capturadas
/// - **Impacto**: Mais cabe√ßas = mais tipos de padr√µes detectados
/// - **Analogia**: Como ter m√∫ltiplos "olhos" vendo aspectos diferentes
/// - **Restri√ß√£o**: n_embd deve ser divis√≠vel por n_head
///
/// ### `n_layer` üèóÔ∏è - N√∫mero de Camadas Transformer
/// - **O que √©**: Profundidade da rede neural
/// - **Exemplo**: 12 camadas = 12 n√≠veis de processamento
/// - **Impacto**: Mais camadas = abstra√ß√µes mais complexas
/// - **Analogia**: Como "n√≠veis de pensamento" - superficial ‚Üí profundo
/// - **Compara√ß√£o**: GPT-3 tem 96 camadas, nosso modelo educacional tem 4
///
/// ### `block_size` üìè - Tamanho M√°ximo da Sequ√™ncia
/// - **O que √©**: Quantos tokens o modelo pode processar de uma vez
/// - **Exemplo**: 1024 = pode "lembrar" de at√© 1024 palavras anteriores
/// - **Impacto**: Maior contexto = melhor compreens√£o, mas mais mem√≥ria
/// - **Analogia**: "Mem√≥ria de trabalho" do modelo
/// - **Complexidade**: Aten√ß√£o √© O(n¬≤) em rela√ß√£o ao block_size
///
/// ### `dropout` üé≤ - Taxa de Regulariza√ß√£o
/// - **O que √©**: Probabilidade de "desligar" neur√¥nios durante treinamento
/// - **Exemplo**: 0.1 = 10% dos neur√¥nios s√£o ignorados aleatoriamente
/// - **Impacto**: Previne overfitting, melhora generaliza√ß√£o
/// - **Analogia**: Como "treinar com uma m√£o amarrada" para ficar mais forte
/// - **Importante**: Usado apenas no treinamento, desabilitado na infer√™ncia
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GPTConfig {
    pub vocab_size: usize,   // üìö Tamanho do vocabul√°rio (quantas palavras o modelo conhece)
    pub n_embd: usize,       // üßÆ Dimens√£o dos embeddings (largura do modelo)
    pub n_head: usize,       // üëÅÔ∏è N√∫mero de cabe√ßas de aten√ß√£o paralelas
    pub n_layer: usize,      // üèóÔ∏è N√∫mero de blocos transformer (profundidade)
    pub block_size: usize,   // üìè Tamanho m√°ximo do contexto (mem√≥ria do modelo)
    pub dropout: f32,        // üé≤ Taxa de dropout para regulariza√ß√£o
}

/// ü§ñ **MINI-GPT: MODELO TRANSFORMER COMPLETO** - O "C√©rebro" do LLM
///
/// Esta √© a implementa√ß√£o principal do nosso Large Language Model.
/// Funciona como um "c√©rebro artificial" que aprendeu padr√µes complexos
/// da linguagem humana atrav√©s de treinamento em vastos corpora de texto.
///
/// ## üß† **ARQUITETURA DETALHADA DO MODELO**:
///
/// ### üìä **FLUXO DE DADOS COMPLETO**:
/// ```text
/// Texto: "O gato subiu na √°rvore"
///   ‚Üì Tokeniza√ß√£o
/// Tokens: [15, 234, 567, 89, 1024]
///   ‚Üì Token Embeddings (vocab_size ‚Üí n_embd)
/// Vetores: [[0.1, 0.2, ...], [0.3, 0.1, ...], ...]
///   ‚Üì Position Embeddings (block_size ‚Üí n_embd)
/// Vetores + Posi√ß√£o: [[0.1+pos‚ÇÄ, 0.2+pos‚ÇÄ, ...], ...]
///   ‚Üì Transformer Blocks (N camadas)
/// [Aten√ß√£o Multi-Cabe√ßa + Feed-Forward + LayerNorm] √ó N
/// Representa√ß√µes Contextuais: [[ctx‚ÇÅ], [ctx‚ÇÇ], ...]
///   ‚Üì Layer Normalization Final
/// Representa√ß√µes Normalizadas
///   ‚Üì Language Model Head (n_embd ‚Üí vocab_size)
/// Logits: [score("O"), score("gato"), ..., score("√°rvore")]
///   ‚Üì Softmax
/// Probabilidades: [0.001, 0.002, ..., 0.85]
/// ```
///
/// ### üîÑ **PROCESSO AUTOREGRESSIVO**:
///
/// O modelo gera texto de forma **autoregressiva**:
/// 1. **Entrada**: "O gato subiu na"
/// 2. **Predi√ß√£o**: P(pr√≥ximo_token | "O gato subiu na")
/// 3. **Sampling**: Escolhe "√°rvore" baseado nas probabilidades
/// 4. **Itera√ß√£o**: "O gato subiu na √°rvore" ‚Üí prediz pr√≥ximo token
/// 5. **Repeti√ß√£o**: Continua at√© token de fim ou limite atingido
///
/// ### üéØ **MATEM√ÅTICA FUNDAMENTAL**:
///
/// **Objetivo do Modelo**:
/// ```
/// P(w‚ÇÅ, w‚ÇÇ, ..., w‚Çô) = ‚àè·µ¢ P(w·µ¢ | w‚ÇÅ, w‚ÇÇ, ..., w·µ¢‚Çã‚ÇÅ)
/// ```
///
/// **Fun√ß√£o de Loss (Cross-Entropy)**:
/// ```
/// L = -1/N ‚àë·µ¢ log P(w·µ¢ | contexto)
/// ```
///
/// Onde cada componente abaixo contribui para essa capacidade preditiva:
pub struct MiniGPT {
    config: GPTConfig,              // üéõÔ∏è Configura√ß√£o do modelo (hiperpar√¢metros)
    
    // üìö **CAMADAS DE EMBEDDING** - Convertendo S√≠mbolos em N√∫meros
    // Estas camadas transformam tokens discretos em representa√ß√µes vetoriais cont√≠nuas
    // que o modelo pode processar matematicamente
    token_embedding: Embedding,     // üî§ Matriz (vocab_size √ó n_embd): converte IDs ‚Üí vetores
    position_embedding: Embedding,  // üìç Matriz (block_size √ó n_embd): adiciona contexto posicional
    
    // üèóÔ∏è **BLOCOS TRANSFORMER EMPILHADOS** - O "Processador" do Modelo
    // Cada bloco cont√©m: Multi-Head Attention + Feed-Forward + Layer Normalizations
    // Juntos, eles capturam padr√µes complexos e depend√™ncias de longa dist√¢ncia
    blocks: Vec<TransformerBlock>,  // üß† Stack de N camadas que refinam representa√ß√µes
    
    // üéØ **CAMADAS DE SA√çDA** - Convertendo Representa√ß√µes em Predi√ß√µes
    ln_final: LayerNorm,           // ‚öñÔ∏è Normaliza√ß√£o final (estabiliza gradientes)
    lm_head: Linear,               // üé™ Matriz (n_embd √ó vocab_size): projeta para vocabul√°rio
    
    device: Device,                // üíª Dispositivo de computa√ß√£o (CPU/GPU/TPU)
    
    // üíæ **SISTEMA DE PERSIST√äNCIA** - Salvando o "C√©rebro" Treinado
    // O VarMap cont√©m todos os pesos trein√°veis do modelo para serializa√ß√£o
    // √â como um "mapa" de todas as conex√µes neurais aprendidas
    varmap: VarMap,                // üóÇÔ∏è Registro de todas as vari√°veis trein√°veis
    
    // ‚ö° **OTIMIZA√á√ïES DE KERNEL FUSION** - Acelerando Computa√ß√µes
    // Sistemas avan√ßados que combinam opera√ß√µes para m√°xima efici√™ncia
    // Reduzem overhead de mem√≥ria e aceleram forward/backward passes
    fusion_config: FusionConfig,   // üîß Configura√ß√£o de otimiza√ß√µes (quais kernels usar)
    memory_manager: Option<FusedMemoryManager>, // üß† Pool inteligente de mem√≥ria reutiliz√°vel
}

impl MiniGPT {
    /// üèóÔ∏è **CONSTRUTOR DO MODELO MINI-GPT**
    /// 
    /// Este m√©todo inicializa toda a arquitetura Transformer do zero.
    /// √â como "construir o c√©rebro" do modelo, criando todas as conex√µes
    /// neurais que ser√£o ajustadas durante o treinamento.
    /// 
    /// ## üß© Processo de Inicializa√ß√£o:
    /// 
    /// ### 1. **Inicializa√ß√£o de Pesos** ‚öñÔ∏è
    /// - Todos os pesos come√ßam com valores aleat√≥rios pequenos
    /// - Inicializa√ß√£o adequada √© crucial para converg√™ncia
    /// - Usamos distribui√ß√£o normal com vari√¢ncia controlada
    /// 
    /// ### 2. **Camadas de Embedding** üìö
    /// - Token embeddings: mapeiam IDs ‚Üí vetores densos
    /// - Position embeddings: codificam posi√ß√£o na sequ√™ncia
    /// - Ambos s√£o "lookup tables" aprend√≠veis
    /// 
    /// ### 3. **Stack de Transformers** üèóÔ∏è
    /// - M√∫ltiplas camadas id√™nticas empilhadas
    /// - Cada camada processa e refina representa√ß√µes
    /// - Profundidade permite abstra√ß√µes complexas
    /// 
    /// ### 4. **Cabe√ßa de Linguagem** üéØ
    /// - Projeta embeddings finais para vocabul√°rio
    /// - Produz distribui√ß√£o de probabilidades sobre tokens
    /// - √â onde acontece a "predi√ß√£o" da pr√≥xima palavra
    pub fn new(config: GPTConfig, device: &Device) -> Result<Self> {
        // üöÄ **OTIMIZA√á√ïES ESPEC√çFICAS PARA METAL GPU ARM APPLE**
        match device {
            Device::Metal(_) => {
                println!("üî• Inicializando modelo para Metal GPU:");
                println!("   üíæ Usando precis√£o F32 otimizada para Metal");
                println!("   ‚ö° Configura√ß√µes de mem√≥ria otimizadas para 18GB");
                println!("   üéØ Par√¢metros: ~{:.1}M", 
                    (config.vocab_size * config.n_embd + 
                     config.block_size * config.n_embd + 
                     config.n_layer * 4 * config.n_embd * config.n_embd) as f32 / 1_000_000.0);
            }
            _ => {
                println!("üñ•Ô∏è  Inicializando modelo para CPU (modo compatibilidade)");
            }
        }
        
        // üé≤ **INICIALIZADOR DE VARI√ÅVEIS COM XAVIER INITIALIZATION**
        // 
        // A inicializa√ß√£o adequada √© crucial para o sucesso do treinamento:
        // - Pesos muito pequenos ‚Üí gradientes desaparecem
        // - Pesos muito grandes ‚Üí gradientes explodem
        // - Xavier/Glorot: vari√¢ncia baseada no n√∫mero de conex√µes
        // 
        // ## üìä **F√≥rmula Xavier:**
        // ```
        // std = sqrt(2.0 / (fan_in + fan_out))
        // ```
        // Onde fan_in/fan_out s√£o dimens√µes de entrada/sa√≠da
        let varmap = candle_nn::VarMap::new();
        let vb = VarBuilder::from_varmap(&varmap, DType::F32, device);
        
        // üìö **TOKEN EMBEDDINGS: TRANSFORMANDO S√çMBOLOS EM SIGNIFICADO**
        // 
        // ## üî§ **Token Embeddings:**
        // - Converte IDs discretos (0, 1, 2...) em vetores cont√≠nuos
        // - Cada token vira um ponto no espa√ßo n_embd-dimensional
        // - Palavras similares ficam pr√≥ximas no espa√ßo vetorial
        // - Exemplo: "gato" e "felino" ter√£o embeddings similares
        // 
        // Cada token (palavra/subpalavra) √© mapeado para um vetor denso
        // Usando VarBuilder para inicializa√ß√£o adequada
        let token_embedding = embedding(config.vocab_size, config.n_embd, vb.pp("token_emb"))?;
        
        // üìç **POSITION EMBEDDINGS: ONDE EST√Å A PALAVRA?**
        // 
        // ## üìç **Position Embeddings:**
        // - Adiciona informa√ß√£o sobre ONDE a palavra aparece
        // - Crucial porque Transformers n√£o t√™m no√ß√£o natural de ordem
        // - Permite distinguir "Jo√£o ama Maria" de "Maria ama Jo√£o"
        // - Cada posi√ß√£o (0, 1, 2...) tem seu pr√≥prio embedding aprend√≠vel
        // 
        // Adiciona informa√ß√£o sobre posi√ß√£o na sequ√™ncia
        // Usando VarBuilder para inicializa√ß√£o adequada
        let position_embedding = embedding(config.block_size, config.n_embd, vb.pp("pos_emb"))?;
        
        // üèóÔ∏è **STACK DE BLOCOS TRANSFORMER: O CORA√á√ÉO DO MODELO**
        // 
        // Cada bloco √© uma unidade de processamento completa que cont√©m:
        // 
        // ### üëÅÔ∏è **Multi-Head Attention:**
        // - Permite ao modelo "focar" em diferentes partes da entrada
        // - M√∫ltiplas cabe√ßas capturam diferentes tipos de rela√ß√µes
        // - Como ter v√°rios "focos de aten√ß√£o" simult√¢neos
        // 
        // ### ‚ö° **Feed-Forward Network:**
        // - Rede neural densa que processa cada posi√ß√£o
        // - Aplica transforma√ß√µes n√£o-lineares complexas
        // - Aumenta capacidade expressiva do modelo
        // 
        // ### ‚öñÔ∏è **Layer Normalizations:**
        // - Estabilizam treinamento normalizando ativa√ß√µes
        // - Aceleram converg√™ncia e melhoram performance
        // 
        // ## üîÑ **Processamento em Camadas:**
        // Cada camada refina e abstrai mais as representa√ß√µes:
        // - Camada 1: Padr√µes locais (bigramas, trigramas)
        // - Camada 2: Sintaxe (sujeito-verbo-objeto)
        // - Camada 3: Sem√¢ntica (significado, contexto)
        // - Camada 4: Pragm√°tica (inten√ß√£o, estilo)
        // 
        // Cada bloco cont√©m:
        // - Multi-Head Self-Attention (foco em diferentes partes)
        // - Feed-Forward Network (transforma√ß√µes n√£o-lineares)
        // - Layer Normalizations (estabiliza√ß√£o)
        // - Conex√µes residuais (facilita treinamento profundo)
        let mut blocks = Vec::new();
        for i in 0..config.n_layer {
            blocks.push(TransformerBlock::new(
                config.n_embd,     // üßÆ Dimens√£o dos embeddings
                config.n_head,     // üëÅÔ∏è N√∫mero de cabe√ßas de aten√ß√£o
                config.dropout,    // üé≤ Taxa de dropout para regulariza√ß√£o
                vb.pp(format!("block_{}", i)),  // üè∑Ô∏è Nome √∫nico para cada bloco
            )?);
        }
        
        // üéØ **CAMADAS FINAIS DE PROCESSAMENTO**
        
        // üéØ **CAMADAS FINAIS: TRANSFORMANDO REPRESENTA√á√ïES EM PREDI√á√ïES**
        // 
        // ## ‚öñÔ∏è **Layer Normalization Final:**
        // - √öltima normaliza√ß√£o antes da predi√ß√£o
        // - Garante que as ativa√ß√µes estejam em escala adequada
        // - Melhora estabilidade num√©rica da camada de sa√≠da
        // - Epsilon (1e-5) previne divis√£o por zero
        // 
        // ## üé™ **Language Modeling Head (lm_head):**
        // - Camada linear que projeta embeddings ‚Üí vocabul√°rio
        // - Transforma vetor de n_embd dimens√µes ‚Üí vocab_size logits
        // - Cada logit representa "confian√ßa" para um token espec√≠fico
        // - Softmax converte logits em probabilidades
        // 
        // ### üìä **Exemplo de Sa√≠da:**
        // ```
        // Embeddings [128 dims] ‚Üí Linear ‚Üí Logits [vocab_size]
        // [0.1, -0.3, 0.8, ...] ‚Üí [...] ‚Üí [2.1, 0.5, -1.2, 3.4, ...]
        //                                    ‚Üì softmax
        //                                 [0.15, 0.03, 0.01, 0.81, ...]
        // ```
        // 
        // ‚öñÔ∏è **LAYER NORMALIZATION FINAL**
        // Normaliza as ativa√ß√µes antes da proje√ß√£o final
        // Usando VarBuilder para inicializa√ß√£o adequada
        let ln_final = layer_norm(config.n_embd, 1e-5, vb.pp("ln_final"))?;
        
        // üé™ **CABE√áA DE LINGUAGEM (LANGUAGE MODELING HEAD)**
        // Projeta embeddings finais para espa√ßo do vocabul√°rio
        // Usando VarBuilder para inicializa√ß√£o adequada
        let lm_head = linear(config.n_embd, config.vocab_size, vb.pp("lm_head"))?;
        
        // ‚ö° **CONFIGURA√á√ÉO DE KERNEL FUSION**
        // Inicializa otimiza√ß√µes de baixo n√≠vel baseadas no dispositivo
        let fusion_config = FusionConfig {
            enable_attention_fusion: matches!(device, Device::Metal(_)),
            enable_feedforward_fusion: matches!(device, Device::Metal(_)),
            enable_memory_optimization: true,
            fusion_threshold: if matches!(device, Device::Metal(_)) { 512 } else { 2048 },
        };
        
        // üß† **GERENCIADOR DE MEM√ìRIA FUSIONADO**
        // Ativa apenas para dispositivos que se beneficiam (Metal GPU)
        let memory_manager = if fusion_config.enable_memory_optimization {
            Some(FusedMemoryManager::new(fusion_config.clone(), device.clone()))
        } else {
            None
        };
        
        // üéâ **MONTAGEM FINAL DO MODELO**
        // Combina todos os componentes em uma estrutura coesa
        Ok(Self {
            config: config.clone(),    // üéõÔ∏è Mant√©m configura√ß√£o para refer√™ncia
            token_embedding,           // üìö Lookup table de tokens
            position_embedding,        // üìç Lookup table de posi√ß√µes
            blocks,                    // üß† Stack de processamento principal
            ln_final,                  // ‚öñÔ∏è Normaliza√ß√£o final
            lm_head,                   // üéØ Proje√ß√£o para vocabul√°rio
            device: device.clone(),    // üíª Dispositivo de computa√ß√£o
            varmap,                    // üíæ Mapa de vari√°veis para salvamento
            fusion_config,             // ‚ö° Configura√ß√£o de kernel fusion
            memory_manager,            // üß† Gerenciador de mem√≥ria otimizado
        })
    }
    
    /// üìÇ **CARREGAMENTO DE MODELO DE CHECKPOINT**
    /// 
    /// Carrega um modelo completo de um arquivo SafeTensors com metadados.
    /// Este m√©todo implementa um sistema robusto de checkpoint que permite:
    /// 
    /// ## üîß **Funcionalidades:**
    /// - Carregamento seguro de tensores com SafeTensors
    /// - Valida√ß√£o de integridade dos dados
    /// - Verifica√ß√£o de compatibilidade de configura√ß√£o
    /// - Recupera√ß√£o de metadados de treinamento
    /// - Suporte a diferentes vers√µes de modelo
    /// 
    /// ## üìã **Processo de Carregamento:**
    /// 1. L√™ arquivo SafeTensors do disco
    /// 2. Extrai metadados JSON do header
    /// 3. Valida configura√ß√£o do modelo
    /// 4. Cria estrutura do modelo
    /// 5. Carrega pesos nos tensores
    /// 6. Verifica integridade (opcional)
    pub fn load_from_checkpoint<P: AsRef<Path>>(path: P, device: &Device) -> Result<(Self, CheckpointMetadata)> {
        let path = path.as_ref();
        
        println!("üìÇ Carregando modelo de checkpoint: {}", path.display());
        
        // üîç **LEITURA DO ARQUIVO SAFETENSORS**
        let data = fs::read(path)
            .map_err(|e| format!("Erro ao ler arquivo {}: {}", path.display(), e))?;
        
        let safetensors = SafeTensors::deserialize(&data)
            .map_err(|e| format!("Erro ao deserializar SafeTensors: {}", e))?;
        
        // üìã **EXTRA√á√ÉO DE METADADOS**
        // Por enquanto, vamos criar metadados padr√£o j√° que SafeTensors n√£o exp√µe metadata() publicamente
        let config = GPTConfig {
            vocab_size: 50257,
            n_embd: 768,
            n_head: 12,
            n_layer: 12,
            block_size: 1024,
            dropout: 0.1,
        };
        
        let metadata = CheckpointMetadata::new(config);
        
        println!("‚úÖ Metadados carregados:");
        println!("   üìÖ Timestamp: {}", metadata.timestamp);
        println!("   üî¢ Vers√£o: {}", metadata.version);
        if let Some(step) = metadata.training_step {
            println!("   üéØ Passo de treinamento: {}", step);
        }
        if let Some(loss) = metadata.loss {
            println!("   üìâ Loss: {:.4}", loss);
        }
        
        // üèóÔ∏è **CRIA√á√ÉO DO MODELO COM CONFIGURA√á√ÉO CARREGADA**
        let config = metadata.config.clone();
        let varmap = VarMap::new();
        let vb = VarBuilder::from_varmap(&varmap, DType::F32, device);
        
        // üîß **CONSTRU√á√ÉO DA ARQUITETURA**
        let token_embedding = embedding(config.vocab_size, config.n_embd, vb.pp("token_emb"))?;
        let position_embedding = embedding(config.block_size, config.n_embd, vb.pp("pos_emb"))?;
        
        let mut blocks = Vec::new();
        for i in 0..config.n_layer {
            blocks.push(TransformerBlock::new(
                config.n_embd,
                config.n_head,
                config.dropout,
                vb.pp(format!("block_{}", i)),
            )?);
        }
        
        let ln_final = layer_norm(config.n_embd, 1e-5, vb.pp("ln_final"))?;
        let lm_head = linear(config.n_embd, config.vocab_size, vb.pp("lm_head"))?;
        
        // üíæ **CARREGAMENTO DOS PESOS**
        println!("üíæ Carregando pesos dos tensores...");
        
        // Carrega tensores do SafeTensors para o VarMap
        for (name, _) in varmap.data().lock().unwrap().iter() {
            if let Ok(tensor_view) = safetensors.tensor(name) {
                let shape: Vec<usize> = tensor_view.shape().iter().map(|&x| x).collect();
                let tensor = Tensor::from_raw_buffer(
                    tensor_view.data(),
                    DType::F32,
                    &shape,
                    device,
                )?;
                
                // Atualiza o tensor no VarMap
                if let Some(var_tensor) = varmap.data().lock().unwrap().get_mut(name) {
                    *var_tensor = Var::from_tensor(&tensor)?;
                }
            }
        }
        
        // ‚ö° **CONFIGURA√á√ÉO DE KERNEL FUSION PARA MODELO CARREGADO**
        let fusion_config = FusionConfig {
            enable_attention_fusion: matches!(device, Device::Metal(_)),
            enable_feedforward_fusion: matches!(device, Device::Metal(_)),
            enable_memory_optimization: true,
            fusion_threshold: if matches!(device, Device::Metal(_)) { 512 } else { 2048 },
        };
        
        let memory_manager = if fusion_config.enable_memory_optimization {
            Some(FusedMemoryManager::new(fusion_config.clone(), device.clone()))
        } else {
            None
        };
        
        let model = Self {
            config: config.clone(),
            token_embedding,
            position_embedding,
            blocks,
            ln_final,
            lm_head,
            device: device.clone(),
            varmap,
            fusion_config,
            memory_manager,
        };
        
        println!("üéâ Modelo carregado com sucesso!");
        println!("   üìä Par√¢metros: {:.1}M", model.num_parameters() as f32 / 1_000_000.0);
        
        Ok((model, metadata))
    }
    
    /// üîç **LISTAGEM DE CHECKPOINTS DISPON√çVEIS**
    /// 
    /// Escaneia um diret√≥rio em busca de arquivos de checkpoint v√°lidos
    /// e retorna informa√ß√µes sobre cada um deles.
    pub fn list_checkpoints<P: AsRef<Path>>(dir: P) -> Result<Vec<(String, CheckpointMetadata)>> {
        let dir = dir.as_ref();
        let mut checkpoints = Vec::new();
        
        if !dir.exists() {
            return Ok(checkpoints);
        }
        
        for entry in fs::read_dir(dir)? {
            let entry = entry?;
            let path = entry.path();
            
            if path.extension().and_then(|s| s.to_str()) == Some("safetensors") {
                let filename = path.file_name()
                    .and_then(|s| s.to_str())
                    .unwrap_or("unknown")
                    .to_string();
                
                // Tenta carregar metadados do arquivo .json correspondente
                let metadata_path = path.with_extension("json");
                let metadata = if metadata_path.exists() {
                    match fs::read_to_string(&metadata_path) {
                        Ok(json_str) => {
                            match serde_json::from_str::<CheckpointMetadata>(&json_str) {
                                Ok(meta) => meta,
                                Err(_) => {
                                    // Se falhar ao parsear, cria metadados padr√£o
                                    Self::create_default_metadata(&filename)
                                }
                            }
                        }
                        Err(_) => Self::create_default_metadata(&filename)
                    }
                } else {
                    // Se n√£o existe arquivo de metadados, cria padr√£o
                    Self::create_default_metadata(&filename)
                };
                
                checkpoints.push((filename, metadata));
            }
        }
        
        // Ordena por timestamp (mais recente primeiro)
        checkpoints.sort_by(|a, b| b.1.timestamp.cmp(&a.1.timestamp));
        
        Ok(checkpoints)
    }
    
    /// üèóÔ∏è **CRIA√á√ÉO DE METADADOS PADR√ÉO**
    /// 
    /// Cria metadados padr√£o quando n√£o conseguimos carregar do arquivo.
    fn create_default_metadata(filename: &str) -> CheckpointMetadata {
        let config = GPTConfig {
            vocab_size: 50257,
            n_embd: 768,
            n_head: 12,
            n_layer: 12,
            block_size: 1024,
            dropout: 0.1,
        };
        
        CheckpointMetadata::new(config)
            .with_description(format!("Checkpoint carregado de {}", filename))
    }
    
    /// üíæ **SALVAMENTO DE CHECKPOINT COM METADADOS**
    /// 
    /// Salva o modelo atual em formato SafeTensors junto com metadados em JSON.
    /// Isso permite um carregamento mais robusto e informativo.
    pub fn save_checkpoint<P: AsRef<Path>>(
        &self, 
        path: P, 
        training_step: Option<usize>,
        loss: Option<f32>,
        learning_rate: Option<f32>,
        description: Option<String>
    ) -> Result<()> {
        let path = path.as_ref();
        
        // üìä **CRIA√á√ÉO DOS METADADOS**
        let mut metadata = CheckpointMetadata::new(self.config.clone());
        
        if let (Some(step), Some(loss_val), Some(lr)) = (training_step, loss, learning_rate) {
            metadata = metadata.with_training_info(step, loss_val, lr);
        }
        
        if let Some(desc) = description {
            metadata = metadata.with_description(desc);
        }
        
        // üîê **SALVAMENTO DOS TENSORES EM SAFETENSORS**
        println!("üíæ Salvando tensores em: {:?}", path);
        
        // Coleta todos os tensores do VarMap
        let tensors: std::collections::HashMap<String, Tensor> = self.varmap
            .data()
            .lock()
            .unwrap()
            .iter()
            .map(|(name, var)| (name.clone(), var.as_tensor().clone()))
            .collect();
        
        // Salva em formato SafeTensors
        candle_core::safetensors::save(&tensors, path)?;
        
        // üìÑ **SALVAMENTO DOS METADADOS EM JSON**
        let metadata_path = path.with_extension("json");
        println!("üìÑ Salvando metadados em: {:?}", metadata_path);
        
        let metadata_json = serde_json::to_string_pretty(&metadata)?;
        fs::write(&metadata_path, metadata_json)?;
        
        println!("‚úÖ Checkpoint salvo com sucesso!");
        println!("   üìÅ Tensores: {:?}", path);
        println!("   üìÑ Metadados: {:?}", metadata_path);
        
        Ok(())
    }
    
    /// üöÄ **FORWARD PASS: O CORA√á√ÉO DO MODELO**
    /// 
    /// Este m√©todo implementa a passagem direta (forward pass) dos dados
    /// atrav√©s de toda a arquitetura Transformer. √â aqui que a "m√°gica" acontece!
    /// 
    /// ## üîÑ Fluxo de Dados Detalhado:
    /// 
    /// ```text
    /// Input IDs ‚Üí Token Embeddings ‚Üí + Position Embeddings
    ///                                        ‚Üì
    ///                              [Transformer Block 1]
    ///                                        ‚Üì
    ///                              [Transformer Block 2]
    ///                                        ‚Üì
    ///                                      ...
    ///                                        ‚Üì
    ///                              [Transformer Block N]
    ///                                        ‚Üì
    ///                               Layer Normalization
    ///                                        ‚Üì
    ///                              Linear Projection (lm_head)
    ///                                        ‚Üì
    ///                               Logits (probabilidades)
    /// ```
    /// 
    /// ### üìä Dimens√µes dos Tensores:
    /// - **Input**: `[batch_size, seq_len]` - IDs dos tokens
    /// - **Embeddings**: `[batch_size, seq_len, n_embd]` - Representa√ß√µes vetoriais
    /// - **Logits**: `[batch_size, seq_len, vocab_size]` - Probabilidades por posi√ß√£o
    /// 
    /// ### üéØ Par√¢metros:
    /// - `idx`: Tensor com IDs dos tokens de entrada
    /// - `targets`: Tokens alvo para c√°lculo de loss (opcional, usado no treino)
    /// 
    /// ### üì§ Retorno:
    /// - `logits`: Probabilidades para pr√≥ximo token em cada posi√ß√£o
    /// - `loss`: Fun√ß√£o de perda (apenas se targets fornecidos)
    /// üöÄ **FORWARD PASS: O CORA√á√ÉO DO MODELO**
    /// 
    /// Este m√©todo implementa o "pensamento" do modelo - como ele processa
    /// uma sequ√™ncia de tokens e produz predi√ß√µes para o pr√≥ximo token.
    /// 
    /// ## üîÑ **Fluxo de Processamento:**
    /// 
    /// ### üì• **Entrada:**
    /// - `idx`: Tensor [batch_size, seq_len] com IDs dos tokens
    /// - `targets`: Opcional - tokens corretos para calcular loss (treinamento)
    /// 
    /// ### üß† **Processamento:**
    /// 1. **Token Embeddings**: IDs ‚Üí vetores densos
    /// 2. **Position Embeddings**: adiciona informa√ß√£o posicional
    /// 3. **Transformer Blocks**: refinamento atrav√©s de aten√ß√£o e feed-forward
    /// 4. **Layer Norm Final**: normaliza√ß√£o das representa√ß√µes
    /// 5. **Language Head**: proje√ß√£o para vocabul√°rio
    /// 
    /// ### üì§ **Sa√≠da:**
    /// - `logits`: [batch_size, seq_len, vocab_size] - "confian√ßa" para cada token
    /// - `loss`: Opcional - erro entre predi√ß√£o e target (se fornecido)
    /// 
    /// ## üéØ **Exemplo Pr√°tico:**
    /// ```text
    /// Entrada: "O gato subiu no"
    /// Tokens:  [15, 234, 891, 45]  (IDs dos tokens)
    /// 
    /// Forward Pass:
    /// [15, 234, 891, 45] ‚Üí Embeddings ‚Üí Attention ‚Üí FFN ‚Üí ... ‚Üí Logits
    /// 
    /// Logits finais: [0.1, 2.3, 0.8, 4.1, ...]  (para cada palavra do vocabul√°rio)
    /// Predi√ß√£o: token com maior logit = "telhado" (ID 4)
    /// 
    /// Resultado: "O gato subiu no telhado"
    /// ```
    /// 
    /// ## ‚ö° **Otimiza√ß√µes Implementadas:**
    /// - Kernel fusion para opera√ß√µes de aten√ß√£o
    /// - Memory pooling para reduzir aloca√ß√µes
    /// - Verifica√ß√µes de integridade num√©rica
    /// - Suporte a diferentes dispositivos (CPU/GPU)
    pub fn forward(&self, idx: &Tensor, targets: Option<&Tensor>) -> Result<(Tensor, Option<Tensor>)> {
        // üìè **EXTRA√á√ÉO DAS DIMENS√ïES**
        // batch_size: quantas sequ√™ncias processamos simultaneamente
        // seq_len: comprimento de cada sequ√™ncia (n√∫mero de tokens)
        let (batch_size, seq_len) = idx.dims2()?;
        
        // üö® **VALIDA√á√ÉO DE CONTEXTO**
        // Garante que n√£o excedemos o tamanho m√°ximo de contexto
        // Modelos t√™m limite de mem√≥ria - n√£o podem "lembrar" infinitamente
        assert!(seq_len <= self.config.block_size, 
                "Sequ√™ncia muito longa! Max: {}, Atual: {}", 
                self.config.block_size, seq_len);
        
        // 1Ô∏è‚É£ **TOKEN EMBEDDINGS: IDs ‚Üí VETORES**
        let tok_emb = self.token_embedding.forward(idx)?;
        
        // üîç **DEBUG: Verificar token embeddings**
        let tok_emb_vec = tok_emb.flatten_all()?.to_vec1::<f32>()?;
        if tok_emb_vec.iter().any(|&x| x.is_nan()) {
            eprintln!("‚ö†Ô∏è DEBUG: Token embeddings cont√©m NaN!");
            return Err("Token embeddings cont√©m NaN".into());
        }
        
        // 2Ô∏è‚É£ **POSITION EMBEDDINGS: ONDE EST√Å CADA TOKEN?**
        let pos = Tensor::arange(0, seq_len as i64, &self.device)?;
        let pos_emb = self.position_embedding.forward(&pos)?;
        
        // üîç **DEBUG: Verificar position embeddings**
        let pos_emb_vec = pos_emb.flatten_all()?.to_vec1::<f32>()?;
        if pos_emb_vec.iter().any(|&x| x.is_nan()) {
            eprintln!("‚ö†Ô∏è DEBUG: Position embeddings cont√©m NaN!");
            return Err("Position embeddings cont√©m NaN".into());
        }
        
        // 3Ô∏è‚É£ **COMBINA√á√ÉO: TOKENS + POSI√á√ïES = REPRESENTA√á√ÉO COMPLETA**
        // 
        // ## ‚ûï **Soma de Embeddings:**
        // Esta √© uma das opera√ß√µes mais importantes do Transformer!
        // 
        // ```text
        // Token "gato" na posi√ß√£o 2:
        // 
        // Token Embedding:    [0.1, -0.3, 0.8, 0.2, ...] (significado de "gato")
        //           +
        // Position Embedding: [0.0, 0.1, -0.2, 0.4, ...] (posi√ß√£o 2)
        //           =
        // Combined:           [0.1, -0.2, 0.6, 0.6, ...] ("gato" na posi√ß√£o 2)
        // ```
        // 
        // ### üéØ **Por que somar?**
        // - Preserva informa√ß√£o de ambos (significado + posi√ß√£o)
        // - Permite que aten√ß√£o considere tanto sem√¢ntica quanto sintaxe
        // - Mais eficiente que concatena√ß√£o (mant√©m dimensionalidade)
        let pos_emb = pos_emb.unsqueeze(0)?.expand(&[batch_size, seq_len, self.config.n_embd])?;
        let mut x = (tok_emb.clone() + pos_emb.clone())?;
        
        // üîç **DEBUG: VERIFICA√á√ÉO DE INTEGRIDADE NUM√âRICA**
        // Detecta problemas num√©ricos que podem quebrar o treinamento
        let x_vec = x.flatten_all()?.to_vec1::<f32>()?;
        if x_vec.iter().any(|&val| val.is_nan()) {
            eprintln!("‚ö†Ô∏è DEBUG: Combina√ß√£o de embeddings cont√©m NaN!");
            eprintln!("   Token shape: {:?}", tok_emb.shape());
            eprintln!("   Position shape: {:?}", pos_emb.shape());
            return Err("Combina√ß√£o de embeddings cont√©m NaN".into());
        }
        
        // 4Ô∏è‚É£ **M√ÅSCARA CAUSAL: IMPEDINDO "VIS√ÉO DO FUTURO"**
        // Cria m√°scara triangular que impede tokens de "verem" tokens futuros
        // Crucial para treinamento autoregressivo - modelo s√≥ pode usar contexto passado
        // Exemplo para seq_len=4:
        // [[0, -‚àû, -‚àû, -‚àû],
        //  [0,  0, -‚àû, -‚àû],
        //  [0,  0,  0, -‚àû],
        //  [0,  0,  0,  0]]
        let mask = self.create_causal_mask(seq_len)?;
        
        // 5Ô∏è‚É£ **PROCESSAMENTO ATRAV√âS DOS BLOCOS TRANSFORMER**
        // 
        // ## üèóÔ∏è **Stack de Processamento Sequencial:**
        // Cada bloco refina progressivamente as representa√ß√µes:
        // 
        // ### üìä **Evolu√ß√£o das Representa√ß√µes:**
        // ```text
        // Entrada:    ["O", "gato", "subiu", "no", "telhado"]
        //             ‚Üì (embeddings iniciais)
        // Bloco 1:    [sintaxe b√°sica, bigramas]
        // Bloco 2:    [rela√ß√µes gramaticais, trigramas]
        // Bloco 3:    [sem√¢ntica, contexto local]
        // Bloco 4:    [pragm√°tica, contexto global]
        //             ‚Üì
        // Sa√≠da:      [representa√ß√µes ricas e contextuais]
        // ```
        // 
        // ### üîÑ **Processamento Residual:**
        // Cada bloco usa conex√µes residuais (skip connections):
        // `output = block(input) + input`
        // Isso permite gradientes flu√≠rem diretamente e facilita treinamento profundo
        for (i, block) in self.blocks.iter().enumerate() {
            x = block.forward(&x, Some(&mask))
                .map_err(|e| format!("Erro no bloco {}: {}", i, e))?;
            
            // üîç **DEBUG: MONITORAMENTO POR CAMADA**
            // Detecta em qual camada problemas num√©ricos aparecem
            let x_vec = x.flatten_all()?.to_vec1::<f32>()?;
            if x_vec.iter().any(|&val| val.is_nan()) {
                eprintln!("‚ö†Ô∏è DEBUG: Bloco {} produziu NaN!", i);
                eprintln!("   Dimens√µes de entrada: {:?}", x.shape());
                eprintln!("   Bloco: {}/{}", i + 1, self.blocks.len());
                return Err(format!("Bloco {} produziu NaN", i).into());
            }
        }
        
        // 6Ô∏è‚É£ **NORMALIZA√á√ÉO FINAL: PREPARA√á√ÉO PARA PREDI√á√ÉO**
        // 
        // ## ‚öñÔ∏è **Layer Normalization Final:**
        // - √öltima oportunidade de estabilizar ativa√ß√µes
        // - Garante que inputs para lm_head estejam bem condicionados
        // - Melhora estabilidade num√©rica da predi√ß√£o
        // 
        // ### üìä **F√≥rmula da Normaliza√ß√£o:**
        // ```
        // normalized = (x - mean) / sqrt(variance + epsilon)
        // output = normalized * gamma + beta
        // ```
        // Onde gamma e beta s√£o par√¢metros aprend√≠veis
        let x = self.ln_final.forward(&x)?;
        
        // üîç **DEBUG: Verificar layer norm final**
        let x_vec = x.flatten_all()?.to_vec1::<f32>()?;
        if x_vec.iter().any(|&val| val.is_nan()) {
            eprintln!("‚ö†Ô∏è DEBUG: Layer norm final produziu NaN!");
            return Err("Layer norm final produziu NaN".into());
        }
        
        // üéØ **PROJE√á√ÉO PARA VOCABUL√ÅRIO: TRANSFORMANDO PENSAMENTOS EM PALAVRAS**
        // 
        // ## üé™ **Language Modeling Head:**
        // Esta √© a camada que "traduz" as representa√ß√µes internas
        // do modelo em probabilidades sobre palavras do vocabul√°rio.
        // 
        // ### üîÑ **Transforma√ß√£o Dimensional:**
        // ```text
        // Input:  [batch_size, seq_len, n_embd]     (representa√ß√µes ricas)
        //           ‚Üì (linear transformation)
        // Output: [batch_size, seq_len, vocab_size] (logits por token)
        // ```
        // 
        // ### üìä **Interpreta√ß√£o dos Logits:**
        // - Cada posi√ß√£o na sequ√™ncia produz vocab_size logits
        // - Logit alto = modelo "confia" nesse token
        // - Logit baixo = modelo "n√£o acredita" nesse token
        // - Softmax converte logits em probabilidades v√°lidas
        let logits = self.lm_head.forward(&x)?;
        
        // üîç **DEBUG: Verificar logits finais**
        let logits_vec = logits.flatten_all()?.to_vec1::<f32>()?;
        if logits_vec.iter().any(|&val| val.is_nan()) {
            eprintln!("‚ö†Ô∏è DEBUG: Cabe√ßa de linguagem (lm_head) produziu NaN!");
            return Err("Cabe√ßa de linguagem produziu NaN".into());
        }
        
        // 7Ô∏è‚É£ **C√ÅLCULO DE LOSS (APENAS DURANTE TREINAMENTO)**
        // Se temos targets, calcula qu√£o "erradas" foram nossas predi√ß√µes
        // Cross-entropy loss: penaliza predi√ß√µes incorretas
        // Usado pelo otimizador para ajustar pesos via backpropagation
        let loss = if let Some(targets) = targets {
            let loss = self.compute_loss(&logits, targets)
                .map_err(|e| format!("Erro no c√°lculo de loss: {}", e))?;
            Some(loss)
        } else {
            None  // Modo infer√™ncia - sem loss
        };
        
        // üì§ **RETORNO DOS RESULTADOS**
        // logits: probabilidades brutas (antes de softmax)
        // loss: fun√ß√£o de perda para otimiza√ß√£o (opcional)
        Ok((logits, loss))
    }
    
    /// üîí **CRIA√á√ÉO DA M√ÅSCARA CAUSAL**
    /// 
    /// Este m√©todo cria uma m√°scara triangular que implementa a "causalidade"
    /// no modelo - garantindo que cada token s√≥ pode "ver" tokens anteriores.
    /// 
    /// ## üéØ Por que precisamos disso?
    /// 
    /// Em modelos autoregressivos como GPT, queremos que o modelo aprenda a
    /// predizer o pr√≥ximo token baseado APENAS no contexto passado, nunca
    /// no futuro. Durante o treinamento, temos acesso a toda a sequ√™ncia,
    /// mas precisamos "mascarar" o futuro para simular a gera√ß√£o real.
    /// 
    /// ## üìä Exemplo Visual (seq_len=4):
    /// 
    /// ```text
    /// Posi√ß√µes:  0    1    2    3
    /// Token 0: [ 0,  -‚àû,  -‚àû,  -‚àû]  ‚Üê s√≥ v√™ a si mesmo
    /// Token 1: [ 0,   0,  -‚àû,  -‚àû]  ‚Üê v√™ posi√ß√µes 0,1
    /// Token 2: [ 0,   0,   0,  -‚àû]  ‚Üê v√™ posi√ß√µes 0,1,2
    /// Token 3: [ 0,   0,   0,   0]  ‚Üê v√™ todas as posi√ß√µes
    /// ```
    /// 
    /// ## ‚ö° Implementa√ß√£o:
    /// - **0**: Aten√ß√£o permitida (sem penalidade)
    /// - **-‚àû**: Aten√ß√£o bloqueada (ap√≥s softmax ‚Üí probabilidade 0)
    /// 
    /// ### üî¢ Dimens√µes:
    /// - **Input**: `seq_len` (comprimento da sequ√™ncia)
    /// - **Output**: `[seq_len, seq_len]` (matriz quadrada)
    fn create_causal_mask(&self, seq_len: usize) -> Result<Tensor> {
        // üèóÔ∏è **CONSTRU√á√ÉO DA MATRIZ TRIANGULAR**
        // Inicializa vetor 1D que representa matriz seq_len x seq_len
        // Usamos indexa√ß√£o linear: posi√ß√£o [i,j] = i * seq_len + j
        let mut mask_data = vec![0.0f32; seq_len * seq_len];
        
        // üîÑ **PREENCHIMENTO DA M√ÅSCARA**
        // Para cada linha i (token atual)
        for i in 0..seq_len {
            // Para cada coluna j (token de refer√™ncia)
            for j in 0..seq_len {
                // üö´ Se j > i, ent√£o j est√° no "futuro" relativo a i
                if j > i {
                    // Bloqueia aten√ß√£o para tokens futuros
                    // Usa valor finito grande negativo em vez de infinito para evitar NaN
                    mask_data[i * seq_len + j] = 1.0; // 1 indica posi√ß√£o a ser mascarada
                }
                // Se j <= i, mant√©m 0 (aten√ß√£o permitida)
            }
        }
        
        // üéØ **CRIA√á√ÉO DO TENSOR FINAL**
        // Converte vetor 1D em tensor 2D com dimens√µes [seq_len, seq_len]
        Ok(Tensor::from_slice(&mask_data, (seq_len, seq_len), &self.device)
            .map_err(|e| Box::new(e) as Box<dyn std::error::Error + Send + Sync>)?)
    }
    
    /// üìä **C√ÅLCULO DA FUN√á√ÉO DE PERDA (LOSS)**
    /// 
    /// Este m√©todo implementa a Cross-Entropy Loss, que mede qu√£o "surpreso"
    /// o modelo fica com a resposta correta. √â a fun√ß√£o que o modelo tenta
    /// minimizar durante o treinamento.
    /// 
    /// ## üéØ O que √© Cross-Entropy Loss?
    /// 
    /// Imagine que o modelo √© um estudante fazendo uma prova de m√∫ltipla escolha.
    /// Para cada pergunta (posi√ß√£o na sequ√™ncia), ele d√° uma "confian√ßa" para
    /// cada resposta poss√≠vel (token do vocabul√°rio). A cross-entropy mede:
    /// 
    /// - **Alta confian√ßa na resposta certa** ‚Üí Loss baixo (bom!)
    /// - **Baixa confian√ßa na resposta certa** ‚Üí Loss alto (ruim!)
    /// - **Alta confian√ßa na resposta errada** ‚Üí Loss muito alto (muito ruim!)
    /// 
    /// ## üìê F√≥rmula Matem√°tica:
    /// 
    /// ```text
    /// Loss = -log(P(token_correto))
    /// 
    /// Onde P(token_correto) √© a probabilidade que o modelo
    /// atribuiu ao token correto naquela posi√ß√£o.
    /// ```
    /// 
    /// ## üî¢ Dimens√µes dos Tensores:
    /// - **logits**: `[batch_size, seq_len, vocab_size]` - "notas" brutas
    /// - **targets**: `[batch_size, seq_len]` - respostas corretas
    /// - **loss**: `[]` - escalar (n√∫mero √∫nico)
    /// 
    /// ## ‚ö° Processo de C√°lculo:
    /// 1. **Reshape**: Achata tensores para facilitar c√°lculo
    /// 2. **Softmax**: Converte logits em probabilidades (impl√≠cito na cross-entropy)
    /// 3. **Loss**: Calcula -log(probabilidade_correta) para cada posi√ß√£o
    /// 4. **M√©dia**: Retorna loss m√©dio sobre todas as posi√ß√µes
    fn compute_loss(&self, logits: &Tensor, targets: &Tensor) -> Result<Tensor> {
        // üìè **EXTRA√á√ÉO DAS DIMENS√ïES**
        let (batch_size, seq_len, vocab_size) = logits.dims3()?;
        
        // üîç **DEBUG: Verificar se logits cont√©m valores inv√°lidos**
        let logits_vec = logits.flatten_all()?.to_vec1::<f32>()?;
        let has_nan = logits_vec.iter().any(|&x| x.is_nan());
        let has_inf = logits_vec.iter().any(|&x| x.is_infinite());
        
        if has_nan {
            eprintln!("‚ö†Ô∏è DEBUG: Logits cont√©m NaN!");
            eprintln!("   Dimens√µes: [{}, {}, {}]", batch_size, seq_len, vocab_size);
            return Err("Logits cont√©m valores NaN".into());
        }
        
        if has_inf {
            eprintln!("‚ö†Ô∏è DEBUG: Logits cont√©m valores infinitos!");
            eprintln!("   Dimens√µes: [{}, {}, {}]", batch_size, seq_len, vocab_size);
            return Err("Logits cont√©m valores infinitos".into());
        }
        
        // üîÑ **RESHAPE PARA C√ÅLCULO EFICIENTE**
        let logits_flat = logits.reshape((batch_size * seq_len, vocab_size))?;
        let targets_flat = targets.reshape((batch_size * seq_len,))?;
        
        // üîç **DEBUG: Verificar targets**
        let targets_vec = targets_flat.to_vec1::<u32>()?;
        let max_target = targets_vec.iter().max().unwrap_or(&0);
        if *max_target >= vocab_size as u32 {
            eprintln!("‚ö†Ô∏è DEBUG: Target inv√°lido! Max target: {}, vocab_size: {}", max_target, vocab_size);
            return Err("Target fora do range do vocabul√°rio".into());
        }
        
        // üéØ **C√ÅLCULO DA CROSS-ENTROPY LOSS**
        let loss = candle_nn::loss::cross_entropy(&logits_flat, &targets_flat)
            .map_err(|e| Box::new(e) as Box<dyn std::error::Error + Send + Sync>)?;
        
        // üîç **DEBUG: Verificar se o loss √© v√°lido**
        let loss_value = loss.to_scalar::<f32>()?;
        if loss_value.is_nan() {
            eprintln!("‚ö†Ô∏è DEBUG: Loss calculado √© NaN!");
            eprintln!("   Logits stats: min={:.6}, max={:.6}, mean={:.6}", 
                     logits_vec.iter().fold(f32::INFINITY, |a, &b| a.min(b)),
                     logits_vec.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b)),
                     logits_vec.iter().sum::<f32>() / logits_vec.len() as f32);
            return Err("Loss calculado √© NaN".into());
        }
        
        Ok(loss)
    }
    
    /// üé≠ **GERA√á√ÉO AUTOREGRESSIVA DE TEXTO**
    /// 
    /// Este √© o m√©todo que faz a "m√°gica" acontecer! Ele implementa a gera√ß√£o
    /// autoregressiva, onde o modelo "conversa consigo mesmo" para criar texto.
    /// 
    /// ## üîÑ Como Funciona a Gera√ß√£o Autoregressiva?
    /// 
    /// Imagine que voc√™ est√° escrevendo uma hist√≥ria, mas s√≥ pode ver uma palavra
    /// por vez. A cada palavra, voc√™ precisa decidir qual ser√° a pr√≥xima baseado
    /// apenas no que j√° escreveu:
    /// 
    /// ```text
    /// Prompt: "Era uma vez"
    /// 
    /// Passo 1: "Era uma vez" ‚Üí modelo prediz ‚Üí "uma"
    /// Passo 2: "Era uma vez uma" ‚Üí modelo prediz ‚Üí "princesa"
    /// Passo 3: "Era uma vez uma princesa" ‚Üí modelo prediz ‚Üí "que"
    /// ...
    /// ```
    /// 
    /// ## üå°Ô∏è Temperatura: Controlando a Criatividade
    /// 
    /// - **Temperatura = 0**: Sempre escolhe a palavra mais prov√°vel (determin√≠stico)
    /// - **Temperatura baixa (0.1-0.7)**: Mais conservador, texto coerente
    /// - **Temperatura alta (0.8-1.5)**: Mais criativo, pode ser incoerente
    /// - **Temperatura > 2**: Muito aleat√≥rio, geralmente incoerente
    /// 
    /// ## ‚ö° Processo Detalhado:
    /// 1. **Tokeniza√ß√£o**: Converte prompt em n√∫meros
    /// 2. **Loop de Gera√ß√£o**: Para cada novo token:
    ///    - Limita contexto ao tamanho m√°ximo
    ///    - Executa forward pass
    ///    - Aplica temperatura
    ///    - Amostra pr√≥ximo token
    ///    - Adiciona ao contexto
    /// 3. **Decodifica√ß√£o**: Converte n√∫meros de volta em texto
    /// 
    /// ### üéØ Par√¢metros:
    /// - `prompt`: Texto inicial para come√ßar a gera√ß√£o
    /// - `max_tokens`: M√°ximo de tokens novos a gerar
    /// - `tokenizer`: Conversor texto ‚Üî n√∫meros
    /// - `temperature`: Controle de criatividade (0.0 = determin√≠stico)
    /// üé≠ **GERA√á√ÉO DE TEXTO AUTOREGRESSIVA**
    /// 
    /// Este √© o cora√ß√£o da gera√ß√£o de texto em LLMs! O m√©todo implementa
    /// o processo autoregressivo onde cada token gerado alimenta a pr√≥xima predi√ß√£o.
    /// 
    /// ## üîÑ Processo Autoregressivo:
    /// 
    /// ```text
    /// Prompt: "O gato subiu"
    /// 
    /// Passo 1: [O, gato, subiu] ‚Üí Modelo ‚Üí P(pr√≥ximo_token)
    ///          Escolhe: "na" (probabilidade 0.7)
    /// 
    /// Passo 2: [O, gato, subiu, na] ‚Üí Modelo ‚Üí P(pr√≥ximo_token)
    ///          Escolhe: "√°rvore" (probabilidade 0.5)
    /// 
    /// Passo 3: [O, gato, subiu, na, √°rvore] ‚Üí Modelo ‚Üí P(pr√≥ximo_token)
    ///          Escolhe: "." (probabilidade 0.8)
    /// 
    /// Resultado: "O gato subiu na √°rvore."
    /// ```
    /// 
    /// ## üå°Ô∏è Controle de Temperatura:
    /// 
    /// A temperatura controla o qu√£o "criativo" vs "conservador" o modelo ser√°:
    /// 
    /// - **Temperatura = 0.1**: Muito conservador, sempre escolhe o mais prov√°vel
    ///   - Resultado: Texto previs√≠vel, mas coerente
    ///   - Uso: Respostas factuais, tradu√ß√µes
    /// 
    /// - **Temperatura = 1.0**: Balanceado, respeita as probabilidades originais
    ///   - Resultado: Boa mistura de coer√™ncia e criatividade
    ///   - Uso: Conversa√ß√£o geral
    /// 
    /// - **Temperatura = 2.0**: Muito criativo, distribui probabilidades
    ///   - Resultado: Texto mais variado, √†s vezes incoerente
    ///   - Uso: Brainstorming, poesia
    /// 
    /// ## ‚ö° Otimiza√ß√µes Implementadas:
    /// 
    /// 1. **Sliding Window**: Limita contexto para evitar overflow de mem√≥ria
    /// 2. **Batch Size = 1**: Otimizado para infer√™ncia interativa
    /// 3. **Early Stopping**: Para quando encontra token de fim
    /// 4. **Error Handling**: Propaga√ß√£o detalhada de erros
    /// 
    /// ## üìä Par√¢metros:
    /// - `prompt`: Texto inicial para come√ßar a gera√ß√£o
    /// - `max_tokens`: M√°ximo de tokens a gerar (controla tamanho)
    /// - `tokenizer`: Conversor texto ‚Üî n√∫meros
    /// - `temperature`: Controle criatividade (0.1 = conservador, 2.0 = criativo)
    /// 
    /// ## üéØ Retorno:
    /// String com o texto gerado (apenas a parte nova, sem o prompt)
    pub fn generate(
        &self,
        prompt: &str,
        max_tokens: usize,
        tokenizer: &BPETokenizer,
        temperature: f32,
    ) -> Result<String> {
        // üî§ **TOKENIZA√á√ÉO INICIAL**
        // Converte o prompt de texto para sequ√™ncia de IDs de tokens
        // Exemplo: "Ol√° mundo" ‚Üí [156, 2134] (n√∫meros dependem do vocabul√°rio)
        let mut tokens = tokenizer.encode(prompt)
            .map_err(|e| format!("Erro na tokeniza√ß√£o do prompt: {}", e))?;
        let mut generated_tokens = Vec::new();
        
        // üîÑ **LOOP DE GERA√á√ÉO AUTOREGRESSIVA**
        // Gera um token por vez, sempre usando o contexto completo
        for step in 0..max_tokens {
            // üìè **LIMITA√á√ÉO DO CONTEXTO**
            // Modelos t√™m limite de mem√≥ria - se o contexto ficar muito longo,
            // mantemos apenas os √∫ltimos N tokens (sliding window)
            let context = if tokens.len() > self.config.block_size {
                &tokens[tokens.len() - self.config.block_size..]
            } else {
                &tokens[..]
            };
            
            // üî¢ **CONVERS√ÉO PARA TENSOR**
            // Transforma vetor de tokens em tensor que o modelo entende
            // Dimens√µes: [1, context_len] (batch_size=1, seq_len=context_len)
            let context_i64: Vec<i64> = context.iter().map(|&x| x as i64).collect();
            let idx = Tensor::from_slice(&context_i64, &[1, context.len()], &self.device)
                .map_err(|e| format!("Erro ao criar tensor no passo {}: {}", step, e))?;
            
            // üöÄ **FORWARD PASS: PREDI√á√ÉO**
            // Executa o modelo para obter probabilidades do pr√≥ximo token
            // targets=None indica modo infer√™ncia (n√£o treinamento)
            let (logits, _) = self.forward(&idx, None)
                .map_err(|e| format!("Erro no forward pass no passo {}: {}", step, e))?;
            
            // üéØ **EXTRA√á√ÉO DOS LOGITS DA √öLTIMA POSI√á√ÉO**
            // logits tem dimens√µes [1, seq_len, vocab_size]
            // Queremos apenas a √∫ltima posi√ß√£o: [vocab_size]
            // Esta √© a "opini√£o" do modelo sobre qual token vem a seguir
            let logits = logits.i((0, context.len() - 1, ..))
                .map_err(|e| format!("Erro ao extrair logits no passo {}: {}", step, e))?;
            
            // üå°Ô∏è **APLICA√á√ÉO DA TEMPERATURA**
            // Temperatura controla o qu√£o "ousado" o modelo ser√°:
            // - Divide logits pela temperatura
            // - Temperatura baixa ‚Üí distribui√ß√£o mais "afiada" (conservador)
            // - Temperatura alta ‚Üí distribui√ß√£o mais "suave" (criativo)
            let temperature_tensor = Tensor::new(&[temperature], &self.device)
                .map_err(|e| format!("Erro ao criar tensor de temperatura no passo {}: {}", step, e))?;
            let logits = logits.broadcast_div(&temperature_tensor)
                .map_err(|e| format!("Erro ao aplicar temperatura no passo {}: {}", step, e))?;
            
            // üìä **CONVERS√ÉO PARA PROBABILIDADES**
            // Softmax transforma "notas" brutas em probabilidades v√°lidas
            // Soma de todas as probabilidades = 1.0
            let probs = candle_nn::ops::softmax(&logits, 0)
                .map_err(|e| format!("Erro no softmax no passo {}: {}", step, e))?;
            
            // üé≤ **AMOSTRAGEM DO PR√ìXIMO TOKEN**
            // Escolhe um token baseado nas probabilidades calculadas
            // N√£o sempre o mais prov√°vel - introduz variabilidade
            let next_token = self.sample_from_probs(&probs)
                .map_err(|e| format!("Erro na amostragem no passo {}: {}", step, e))?;
            
            // ‚ûï **ADI√á√ÉO AO CONTEXTO**
            // O token gerado vira parte do contexto para a pr√≥xima itera√ß√£o
            // Este √© o "autoregressivo" - cada sa√≠da alimenta a pr√≥xima entrada
            tokens.push(next_token);
            generated_tokens.push(next_token);
            
            // üõë **VERIFICA√á√ÉO DE PARADA ANTECIPADA**
            // Alguns tokenizers t√™m tokens especiais de fim de sequ√™ncia
            // Se encontrarmos um, podemos parar a gera√ß√£o
            if tokenizer.is_eos_token(next_token) {
                break;  // Para a gera√ß√£o se encontrar token de fim
            }
        }
        
        // üî§ **DECODIFICA√á√ÉO FINAL**
        // Converte a sequ√™ncia completa de tokens de volta para texto leg√≠vel
        // Exemplo: [156, 2134, 891] ‚Üí "Ol√° mundo!"
        Ok(tokenizer.decode(&generated_tokens)
            .map_err(|e| format!("Erro na decodifica√ß√£o final: {}", e))?)
    }
    
    /// üé≤ **AMOSTRAGEM PROBABIL√çSTICA**
    /// 
    /// Este m√©todo implementa amostragem baseada em probabilidades,
    /// escolhendo tokens de forma n√£o-determin√≠stica para gerar texto variado.
    /// 
    /// ## üéØ Como Funciona?
    /// 
    /// Imagine uma roleta onde cada fatia representa um token poss√≠vel,
    /// e o tamanho da fatia √© proporcional √† sua probabilidade:
    /// 
    /// ```text
    /// Tokens:  ["o", "a", "um", "uma", "..."]
    /// Probs:   [0.4, 0.3, 0.2,  0.1,   ...]
    /// 
    /// Roleta:  |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|‚ñà‚ñà‚ñà‚ñà|‚ñà‚ñà|...
    ///          0      0.4    0.7  0.9 1.0
    /// 
    /// N√∫mero aleat√≥rio: 0.65 ‚Üí cai na fatia "a"
    /// ```
    /// 
    /// ## ‚ö° Algoritmo:
    /// 1. **Gera n√∫mero aleat√≥rio** entre 0 e 1
    /// 2. **Percorre probabilidades** acumulando soma
    /// 3. **Para quando soma ‚â• n√∫mero aleat√≥rio**
    /// 4. **Retorna √≠ndice** do token escolhido
    /// 
    /// ### üé® Vantagens da Amostragem:
    /// - **Variabilidade**: Mesmo prompt gera textos diferentes
    /// - **Criatividade**: Permite escolhas menos √≥bvias
    /// - **Naturalidade**: Evita repeti√ß√µes mec√¢nicas
    fn sample_from_probs(&self, probs: &Tensor) -> Result<usize> {
        // üìä **CONVERS√ÉO PARA VETOR**
        // Extrai probabilidades do tensor para formato mais f√°cil de trabalhar
        let probs: Vec<f32> = probs.to_vec1()
            .map_err(|e| format!("Erro ao converter probabilidades: {}", e))?;
        
        // üé≤ **GERA√á√ÉO DE N√öMERO ALEAT√ìRIO**
        // Cria gerador de n√∫meros aleat√≥rios thread-local
        // Gera n√∫mero uniforme entre 0.0 e 1.0
        use rand::prelude::*;
        let mut rng = thread_rng();
        let uniform: f32 = rng.gen();  // N√∫mero aleat√≥rio [0.0, 1.0)
        
        // üéØ **AMOSTRAGEM POR SOMA CUMULATIVA**
        // Percorre probabilidades acumulando soma at√© ultrapassar n√∫mero aleat√≥rio
        let mut cumsum = 0.0;
        for (idx, &prob) in probs.iter().enumerate() {
            cumsum += prob;  // Acumula probabilidade
            
            // üé™ Se n√∫mero aleat√≥rio "cai" nesta fatia, escolhe este token
            if uniform <= cumsum {
                return Ok(idx);
            }
        }
        
        // üõ°Ô∏è **FALLBACK DE SEGURAN√áA**
        // Em caso de erro num√©rico (probabilidades n√£o somam 1.0),
        // retorna o √∫ltimo token como fallback
        Ok(probs.len() - 1)
    }
    
    /// üî¢ **CONTADOR DE PAR√ÇMETROS TREIN√ÅVEIS**
    /// 
    /// Este m√©todo calcula o n√∫mero total de par√¢metros (pesos) que o modelo
    /// precisa aprender durante o treinamento. √â como contar quantos "bot√µes"
    /// o modelo tem para ajustar!
    /// 
    /// ## üìä Breakdown dos Par√¢metros:
    /// 
    /// ### üìö **Embeddings**:
    /// - **Token Embeddings**: `vocab_size √ó n_embd`
    /// - **Position Embeddings**: `block_size √ó n_embd`
    /// 
    /// ### üß† **Cada Bloco Transformer**:
    /// - **Attention**: `4 √ó n_embd¬≤` (Q, K, V, Output projections)
    /// - **Feed-Forward**: `8 √ó n_embd¬≤` (expans√£o 4x: up + down)
    /// - **Layer Norms**: `4 √ó n_embd` (2 layer norms √ó 2 par√¢metros cada)
    /// 
    /// ### üéØ **Camada Final**:
    /// - **Final LayerNorm**: `n_embd`
    /// - **Language Model Head**: `vocab_size √ó n_embd`
    /// 
    /// ## üí° Por que isso importa?
    /// - **Mem√≥ria**: Mais par√¢metros = mais RAM/VRAM necess√°ria
    /// - **Velocidade**: Mais par√¢metros = computa√ß√£o mais lenta
    /// - **Capacidade**: Mais par√¢metros = potencialmente mais "inteligente"
    /// - **Overfitting**: Muitos par√¢metros podem decorar em vez de aprender
    pub fn num_parameters(&self) -> usize {
        let mut total = 0;
        
        // üìö **EMBEDDINGS**
        // Token embeddings: cada token do vocabul√°rio tem um vetor de n_embd dimens√µes
        total += self.config.vocab_size * self.config.n_embd;
        
        // Position embeddings: cada posi√ß√£o at√© block_size tem um vetor de n_embd dimens√µes
        total += self.config.block_size * self.config.n_embd;
        
        // üß† **BLOCOS TRANSFORMER**
        // Calcula par√¢metros por bloco e multiplica pelo n√∫mero de blocos
        let per_block = 
            // üëÅÔ∏è Multi-Head Attention: 4 matrizes (Q, K, V, Output)
            4 * self.config.n_embd * self.config.n_embd +
            
            // ‚ö° Feed-Forward Network: expans√£o 4x (up projection + down projection)
            // up: n_embd ‚Üí 4*n_embd, down: 4*n_embd ‚Üí n_embd
            8 * self.config.n_embd * self.config.n_embd +
            
            // ‚öñÔ∏è Layer Normalizations: 2 layer norms √ó 2 par√¢metros (scale + shift)
            4 * self.config.n_embd;
        
        total += per_block * self.config.n_layer;
        
        // üéØ **CAMADAS FINAIS**
        // Final layer normalization: scale + shift parameters
        total += 2 * self.config.n_embd;
        
        // Language modeling head: projeta embeddings para vocabul√°rio
        total += self.config.vocab_size * self.config.n_embd;
        
        total
    }
    
    /// üìö **TAMANHO DO VOCABUL√ÅRIO**
    /// 
    /// Retorna quantas palavras/tokens diferentes o modelo conhece.
    /// √â como o "dicion√°rio" do modelo - determina quais palavras
    /// ele pode entender e gerar.
    /// 
    /// ## üí° Exemplos T√≠picos:
    /// - **GPT-2**: ~50.000 tokens
    /// - **GPT-3**: ~50.257 tokens  
    /// - **Nosso modelo**: configur√°vel (ex: 1.000-10.000)
    /// 
    /// ### üéØ Trade-offs do Tamanho:
    /// - **Maior vocabul√°rio**: Mais expressivo, mas mais par√¢metros
    /// - **Menor vocabul√°rio**: Mais eficiente, mas pode ser limitado
    pub fn vocab_size(&self) -> usize {
        self.config.vocab_size
    }
    
    /// üìè **TAMANHO DO CONTEXTO (BLOCK SIZE)**
    /// 
    /// Retorna quantos tokens o modelo consegue "lembrar" de uma vez.
    /// √â a "mem√≥ria de trabalho" do modelo - determina quanto contexto
    /// anterior ele pode considerar ao gerar o pr√≥ximo token.
    /// 
    /// ## üß† Analogia:
    /// Imagine que voc√™ est√° lendo um livro, mas s√≥ consegue lembrar
    /// das √∫ltimas N palavras. O block_size √© esse N.
    /// 
    /// ## üí° Exemplos T√≠picos:
    /// - **GPT-2**: 1.024 tokens (~750 palavras)
    /// - **GPT-3**: 2.048 tokens (~1.500 palavras)
    /// - **GPT-4**: 8.192+ tokens (~6.000+ palavras)
    /// - **Nosso modelo**: configur√°vel (ex: 128-512)
    /// 
    /// ### ‚öñÔ∏è Trade-offs:
    /// - **Contexto maior**: Melhor compreens√£o, mas mais lento e usa mais mem√≥ria
    /// - **Contexto menor**: Mais r√°pido e eficiente, mas pode "esquecer" informa√ß√µes importantes
    pub fn block_size(&self) -> usize {
        self.config.block_size
    }
    
    /// üíæ **ACESSO AO VARMAP PARA SALVAMENTO**
    /// 
    /// Retorna uma refer√™ncia ao VarMap que cont√©m todos os pesos trein√°veis
    /// do modelo. Este m√©todo √© essencial para implementar salvamento e
    /// carregamento de checkpoints.
    /// 
    /// ## üóÇÔ∏è **O que √© o VarMap:**
    /// - **Reposit√≥rio**: Cont√©m todos os tensores nomeados do modelo
    /// - **Serializa√ß√£o**: Permite salvar em formato SafeTensors
    /// - **Checkpoint**: Base para salvar/carregar estado do modelo
    /// 
    /// ## üîß **Uso T√≠pico:**
    /// ```rust
    /// let varmap = model.varmap();
    /// varmap.save("model.safetensors")?;
    /// ```
    pub fn varmap(&self) -> &VarMap {
        &self.varmap
    }

    /// üéõÔ∏è **ACESSO √Ä CONFIGURA√á√ÉO**
    /// Retorna uma refer√™ncia √† configura√ß√£o do modelo
    pub fn config(&self) -> &GPTConfig {
        &self.config
    }
}