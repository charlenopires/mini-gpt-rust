//! # Mini-GPT: Arquitetura Transformer Completa
//! 
//! ## ğŸ§  O que Ã© um Modelo Transformer?
//! 
//! O Transformer Ã© uma arquitetura revolucionÃ¡ria de deep learning que mudou
//! completamente o campo de processamento de linguagem natural. Diferente de
//! redes recorrentes (RNNs), o Transformer processa sequÃªncias inteiras em
//! paralelo usando o mecanismo de "atenÃ§Ã£o".
//! 
//! ### ğŸ—ï¸ Componentes Principais:
//! 
//! 1. **Token Embeddings** ğŸ“š
//!    - Converte palavras/tokens em vetores densos de nÃºmeros reais
//!    - Cada palavra vira um ponto no espaÃ§o multidimensional
//!    - Palavras similares ficam prÃ³ximas no espaÃ§o vetorial
//! 
//! 2. **Position Embeddings** ğŸ“
//!    - Adiciona informaÃ§Ã£o sobre a posiÃ§Ã£o da palavra na sequÃªncia
//!    - Crucial porque Transformers nÃ£o tÃªm noÃ§Ã£o natural de ordem
//!    - Permite distinguir "JoÃ£o ama Maria" de "Maria ama JoÃ£o"
//! 
//! 3. **Multi-Head Attention** ğŸ‘ï¸
//!    - Permite ao modelo "focar" em diferentes partes da entrada
//!    - MÃºltiplas "cabeÃ§as" capturam diferentes tipos de relaÃ§Ãµes
//!    - Como ter vÃ¡rios "focos de atenÃ§Ã£o" simultÃ¢neos
//! 
//! 4. **Feed-Forward Networks** âš¡
//!    - Redes neurais que processam cada posiÃ§Ã£o independentemente
//!    - Aplicam transformaÃ§Ãµes nÃ£o-lineares aos dados
//!    - Aumentam a capacidade expressiva do modelo
//! 
//! 5. **Layer Normalization** âš–ï¸
//!    - Estabiliza o treinamento normalizando ativaÃ§Ãµes
//!    - Acelera convergÃªncia e melhora performance
//! 
//! ### ğŸ”„ Processo Autoregressivo:
//! 
//! O modelo GPT Ã© "autoregressivo" - gera texto token por token:
//! 1. Recebe sequÃªncia de tokens como entrada
//! 2. Prediz probabilidades para o prÃ³ximo token
//! 3. Amostra um token baseado nessas probabilidades
//! 4. Adiciona o token Ã  sequÃªncia e repete
//! 
//! Este Ã© nosso "cÃ©rebro artificial" completo que implementa
//! toda essa arquitetura sofisticada em Rust!

use candle_core::{DType, Device, Tensor, IndexOp};
use candle_nn::{embedding, layer_norm, linear, Embedding, LayerNorm, Linear, Module, VarBuilder, VarMap};
use crate::transformer::TransformerBlock;
use crate::tokenizer::BPETokenizer;

type Result<T> = std::result::Result<T, Box<dyn std::error::Error + Send + Sync>>;

/// ğŸ›ï¸ **CONFIGURAÃ‡ÃƒO DO MODELO GPT**
/// 
/// Esta estrutura define todos os hiperparÃ¢metros que controlam
/// a arquitetura e comportamento do modelo. Ã‰ como o "DNA" do modelo!
/// 
/// ## ğŸ“Š ParÃ¢metros Explicados:
/// 
/// ### `vocab_size` ğŸ“š
/// - Quantas palavras/tokens diferentes o modelo conhece
/// - Determina o tamanho da camada de saÃ­da
/// - Exemplo: 50.000 = modelo conhece 50 mil palavras diferentes
/// 
/// ### `n_embd` ğŸ§®
/// - DimensÃ£o dos vetores de embedding (largura do modelo)
/// - Maior = mais capacidade, mas mais lento
/// - GPT-3: 12.288, nosso modelo educacional: 128
/// 
/// ### `n_head` ğŸ‘ï¸
/// - NÃºmero de "cabeÃ§as" de atenÃ§Ã£o paralelas
/// - Cada cabeÃ§a foca em aspectos diferentes do texto
/// - Deve dividir `n_embd` igualmente
/// 
/// ### `n_layer` ğŸ—ï¸
/// - Profundidade do modelo (quantas camadas Transformer)
/// - Mais camadas = mais capacidade de abstraÃ§Ã£o
/// - GPT-3: 96 camadas, nosso: 4 camadas
/// 
/// ### `block_size` ğŸ“
/// - Tamanho mÃ¡ximo da sequÃªncia de entrada (contexto)
/// - Quantas palavras o modelo "lembra" de uma vez
/// - Maior contexto = melhor compreensÃ£o, mas mais memÃ³ria
/// 
/// ### `dropout` ğŸ²
/// - Taxa de regularizaÃ§Ã£o para evitar overfitting
/// - 0.1 = desliga 10% dos neurÃ´nios aleatoriamente
/// - Usado apenas durante treinamento, nÃ£o na inferÃªncia
#[derive(Debug, Clone)]
pub struct GPTConfig {
    pub vocab_size: usize,   // ğŸ“š Tamanho do vocabulÃ¡rio (quantas palavras o modelo conhece)
    pub n_embd: usize,       // ğŸ§® DimensÃ£o dos embeddings (largura do modelo)
    pub n_head: usize,       // ğŸ‘ï¸ NÃºmero de cabeÃ§as de atenÃ§Ã£o paralelas
    pub n_layer: usize,      // ğŸ—ï¸ NÃºmero de blocos transformer (profundidade)
    pub block_size: usize,   // ğŸ“ Tamanho mÃ¡ximo do contexto (memÃ³ria do modelo)
    pub dropout: f32,        // ğŸ² Taxa de dropout para regularizaÃ§Ã£o
}

/// ğŸ¤– **MINI-GPT: MODELO TRANSFORMER COMPLETO**
/// 
/// Esta Ã© a implementaÃ§Ã£o principal do nosso modelo de linguagem.
/// Funciona como um "cÃ©rebro artificial" que aprendeu padrÃµes de texto
/// e pode gerar novas sequÃªncias baseadas no que aprendeu.
/// 
/// ## ğŸ§© Arquitetura Detalhada:
/// 
/// ```text
/// Input Tokens â†’ Token Embeddings â†’ Position Embeddings
///                      â†“
///              [Transformer Block 1]
///                      â†“
///              [Transformer Block 2]
///                      â†“
///                    ...
///                      â†“
///              [Transformer Block N]
///                      â†“
///               Layer Normalization
///                      â†“
///              Linear Projection (lm_head)
///                      â†“
///              Output Probabilities
/// ```
/// 
/// ### ğŸ”„ Processo Autoregressivo:
/// O modelo Ã© "autoregressivo" - gera texto token por token:
/// 1. ğŸ“ Recebe sequÃªncia de tokens como entrada
/// 2. ğŸ§® Calcula probabilidades para o prÃ³ximo token
/// 3. ğŸ² Amostra um token baseado nessas probabilidades
/// 4. â• Adiciona o token Ã  sequÃªncia e repete
/// 
/// Ã‰ como completar uma frase palavra por palavra, sempre
/// considerando todo o contexto anterior!
pub struct MiniGPT {
    config: GPTConfig,              // ğŸ›ï¸ ConfiguraÃ§Ã£o do modelo
    
    // ğŸ“š **CAMADAS DE EMBEDDING**
    // Convertem tokens discretos em representaÃ§Ãµes vetoriais contÃ­nuas
    token_embedding: Embedding,     // ğŸ”¤ Converte IDs de tokens em vetores densos
    position_embedding: Embedding,  // ğŸ“ Adiciona informaÃ§Ã£o posicional aos tokens
    
    // ğŸ—ï¸ **BLOCOS TRANSFORMER EMPILHADOS**
    // Cada bloco contÃ©m atenÃ§Ã£o multi-cabeÃ§a + feed-forward + normalizaÃ§Ãµes
    blocks: Vec<TransformerBlock>,  // ğŸ§  Stack de camadas que processam sequÃªncias
    
    // ğŸ¯ **CAMADAS DE SAÃDA**
    ln_final: LayerNorm,           // âš–ï¸ NormalizaÃ§Ã£o final para estabilidade
    lm_head: Linear,               // ğŸª Projeta embeddings para vocabulÃ¡rio
    
    device: Device,                // ğŸ’» Dispositivo de computaÃ§Ã£o (CPU/GPU)
    
    // ğŸ’¾ **VARMAP PARA SALVAMENTO**
    // ContÃ©m todos os pesos treinÃ¡veis do modelo para serializaÃ§Ã£o
    varmap: VarMap,                // ğŸ—‚ï¸ Mapa de variÃ¡veis para salvamento/carregamento
}

impl MiniGPT {
    /// ğŸ—ï¸ **CONSTRUTOR DO MODELO MINI-GPT**
    /// 
    /// Este mÃ©todo inicializa toda a arquitetura Transformer do zero.
    /// Ã‰ como "construir o cÃ©rebro" do modelo, criando todas as conexÃµes
    /// neurais que serÃ£o ajustadas durante o treinamento.
    /// 
    /// ## ğŸ§© Processo de InicializaÃ§Ã£o:
    /// 
    /// ### 1. **InicializaÃ§Ã£o de Pesos** âš–ï¸
    /// - Todos os pesos comeÃ§am com valores aleatÃ³rios pequenos
    /// - InicializaÃ§Ã£o adequada Ã© crucial para convergÃªncia
    /// - Usamos distribuiÃ§Ã£o normal com variÃ¢ncia controlada
    /// 
    /// ### 2. **Camadas de Embedding** ğŸ“š
    /// - Token embeddings: mapeiam IDs â†’ vetores densos
    /// - Position embeddings: codificam posiÃ§Ã£o na sequÃªncia
    /// - Ambos sÃ£o "lookup tables" aprendÃ­veis
    /// 
    /// ### 3. **Stack de Transformers** ğŸ—ï¸
    /// - MÃºltiplas camadas idÃªnticas empilhadas
    /// - Cada camada processa e refina representaÃ§Ãµes
    /// - Profundidade permite abstraÃ§Ãµes complexas
    /// 
    /// ### 4. **CabeÃ§a de Linguagem** ğŸ¯
    /// - Projeta embeddings finais para vocabulÃ¡rio
    /// - Produz distribuiÃ§Ã£o de probabilidades sobre tokens
    /// - Ã‰ onde acontece a "prediÃ§Ã£o" da prÃ³xima palavra
    pub fn new(config: GPTConfig, device: &Device) -> Result<Self> {
        // ğŸš€ **OTIMIZAÃ‡Ã•ES ESPECÃFICAS PARA METAL GPU ARM APPLE**
        match device {
            Device::Metal(_) => {
                println!("ğŸ”¥ Inicializando modelo para Metal GPU:");
                println!("   ğŸ’¾ Usando precisÃ£o F32 otimizada para Metal");
                println!("   âš¡ ConfiguraÃ§Ãµes de memÃ³ria otimizadas para 18GB");
                println!("   ğŸ¯ ParÃ¢metros: ~{:.1}M", 
                    (config.vocab_size * config.n_embd + 
                     config.block_size * config.n_embd + 
                     config.n_layer * 4 * config.n_embd * config.n_embd) as f32 / 1_000_000.0);
            }
            _ => {
                println!("ğŸ–¥ï¸  Inicializando modelo para CPU (modo compatibilidade)");
            }
        }
        
        // ğŸ² **INICIALIZADOR DE VARIÃVEIS COM XAVIER INITIALIZATION**
        // 
        // A inicializaÃ§Ã£o adequada Ã© crucial para o sucesso do treinamento:
        // - Pesos muito pequenos â†’ gradientes desaparecem
        // - Pesos muito grandes â†’ gradientes explodem
        // - Xavier/Glorot: variÃ¢ncia baseada no nÃºmero de conexÃµes
        // 
        // ## ğŸ“Š **FÃ³rmula Xavier:**
        // ```
        // std = sqrt(2.0 / (fan_in + fan_out))
        // ```
        // Onde fan_in/fan_out sÃ£o dimensÃµes de entrada/saÃ­da
        let varmap = candle_nn::VarMap::new();
        let vb = VarBuilder::from_varmap(&varmap, DType::F32, device);
        
        // ğŸ“š **TOKEN EMBEDDINGS: TRANSFORMANDO SÃMBOLOS EM SIGNIFICADO**
        // 
        // ## ğŸ”¤ **Token Embeddings:**
        // - Converte IDs discretos (0, 1, 2...) em vetores contÃ­nuos
        // - Cada token vira um ponto no espaÃ§o n_embd-dimensional
        // - Palavras similares ficam prÃ³ximas no espaÃ§o vetorial
        // - Exemplo: "gato" e "felino" terÃ£o embeddings similares
        // 
        // Cada token (palavra/subpalavra) Ã© mapeado para um vetor denso
        // Usando VarBuilder para inicializaÃ§Ã£o adequada
        let token_embedding = embedding(config.vocab_size, config.n_embd, vb.pp("token_emb"))?;
        
        // ğŸ“ **POSITION EMBEDDINGS: ONDE ESTÃ A PALAVRA?**
        // 
        // ## ğŸ“ **Position Embeddings:**
        // - Adiciona informaÃ§Ã£o sobre ONDE a palavra aparece
        // - Crucial porque Transformers nÃ£o tÃªm noÃ§Ã£o natural de ordem
        // - Permite distinguir "JoÃ£o ama Maria" de "Maria ama JoÃ£o"
        // - Cada posiÃ§Ã£o (0, 1, 2...) tem seu prÃ³prio embedding aprendÃ­vel
        // 
        // Adiciona informaÃ§Ã£o sobre posiÃ§Ã£o na sequÃªncia
        // Usando VarBuilder para inicializaÃ§Ã£o adequada
        let position_embedding = embedding(config.block_size, config.n_embd, vb.pp("pos_emb"))?;
        
        // ğŸ—ï¸ **STACK DE BLOCOS TRANSFORMER: O CORAÃ‡ÃƒO DO MODELO**
        // 
        // Cada bloco Ã© uma unidade de processamento completa que contÃ©m:
        // 
        // ### ğŸ‘ï¸ **Multi-Head Attention:**
        // - Permite ao modelo "focar" em diferentes partes da entrada
        // - MÃºltiplas cabeÃ§as capturam diferentes tipos de relaÃ§Ãµes
        // - Como ter vÃ¡rios "focos de atenÃ§Ã£o" simultÃ¢neos
        // 
        // ### âš¡ **Feed-Forward Network:**
        // - Rede neural densa que processa cada posiÃ§Ã£o
        // - Aplica transformaÃ§Ãµes nÃ£o-lineares complexas
        // - Aumenta capacidade expressiva do modelo
        // 
        // ### âš–ï¸ **Layer Normalizations:**
        // - Estabilizam treinamento normalizando ativaÃ§Ãµes
        // - Aceleram convergÃªncia e melhoram performance
        // 
        // ## ğŸ”„ **Processamento em Camadas:**
        // Cada camada refina e abstrai mais as representaÃ§Ãµes:
        // - Camada 1: PadrÃµes locais (bigramas, trigramas)
        // - Camada 2: Sintaxe (sujeito-verbo-objeto)
        // - Camada 3: SemÃ¢ntica (significado, contexto)
        // - Camada 4: PragmÃ¡tica (intenÃ§Ã£o, estilo)
        // 
        // Cada bloco contÃ©m:
        // - Multi-Head Self-Attention (foco em diferentes partes)
        // - Feed-Forward Network (transformaÃ§Ãµes nÃ£o-lineares)
        // - Layer Normalizations (estabilizaÃ§Ã£o)
        // - ConexÃµes residuais (facilita treinamento profundo)
        let mut blocks = Vec::new();
        for i in 0..config.n_layer {
            blocks.push(TransformerBlock::new(
                config.n_embd,     // ğŸ§® DimensÃ£o dos embeddings
                config.n_head,     // ğŸ‘ï¸ NÃºmero de cabeÃ§as de atenÃ§Ã£o
                config.dropout,    // ğŸ² Taxa de dropout para regularizaÃ§Ã£o
                vb.pp(format!("block_{}", i)),  // ğŸ·ï¸ Nome Ãºnico para cada bloco
            )?);
        }
        
        // ğŸ¯ **CAMADAS FINAIS DE PROCESSAMENTO**
        
        // ğŸ¯ **CAMADAS FINAIS: TRANSFORMANDO REPRESENTAÃ‡Ã•ES EM PREDIÃ‡Ã•ES**
        // 
        // ## âš–ï¸ **Layer Normalization Final:**
        // - Ãšltima normalizaÃ§Ã£o antes da prediÃ§Ã£o
        // - Garante que as ativaÃ§Ãµes estejam em escala adequada
        // - Melhora estabilidade numÃ©rica da camada de saÃ­da
        // - Epsilon (1e-5) previne divisÃ£o por zero
        // 
        // ## ğŸª **Language Modeling Head (lm_head):**
        // - Camada linear que projeta embeddings â†’ vocabulÃ¡rio
        // - Transforma vetor de n_embd dimensÃµes â†’ vocab_size logits
        // - Cada logit representa "confianÃ§a" para um token especÃ­fico
        // - Softmax converte logits em probabilidades
        // 
        // ### ğŸ“Š **Exemplo de SaÃ­da:**
        // ```
        // Embeddings [128 dims] â†’ Linear â†’ Logits [vocab_size]
        // [0.1, -0.3, 0.8, ...] â†’ [...] â†’ [2.1, 0.5, -1.2, 3.4, ...]
        //                                    â†“ softmax
        //                                 [0.15, 0.03, 0.01, 0.81, ...]
        // ```
        // 
        // âš–ï¸ **LAYER NORMALIZATION FINAL**
        // Normaliza as ativaÃ§Ãµes antes da projeÃ§Ã£o final
        // Usando VarBuilder para inicializaÃ§Ã£o adequada
        let ln_final = layer_norm(config.n_embd, 1e-5, vb.pp("ln_final"))?;
        
        // ğŸª **CABEÃ‡A DE LINGUAGEM (LANGUAGE MODELING HEAD)**
        // Projeta embeddings finais para espaÃ§o do vocabulÃ¡rio
        // Usando VarBuilder para inicializaÃ§Ã£o adequada
        let lm_head = linear(config.n_embd, config.vocab_size, vb.pp("lm_head"))?;
        
        // ğŸ‰ **MONTAGEM FINAL DO MODELO**
        // Combina todos os componentes em uma estrutura coesa
        Ok(Self {
            config: config.clone(),    // ğŸ›ï¸ MantÃ©m configuraÃ§Ã£o para referÃªncia
            token_embedding,           // ğŸ“š Lookup table de tokens
            position_embedding,        // ğŸ“ Lookup table de posiÃ§Ãµes
            blocks,                    // ğŸ§  Stack de processamento principal
            ln_final,                  // âš–ï¸ NormalizaÃ§Ã£o final
            lm_head,                   // ğŸ¯ ProjeÃ§Ã£o para vocabulÃ¡rio
            device: device.clone(),    // ğŸ’» Dispositivo de computaÃ§Ã£o
            varmap,                    // ğŸ’¾ Mapa de variÃ¡veis para salvamento
        })
    }
    
    /// ğŸš€ **FORWARD PASS: O CORAÃ‡ÃƒO DO MODELO**
    /// 
    /// Este mÃ©todo implementa a passagem direta (forward pass) dos dados
    /// atravÃ©s de toda a arquitetura Transformer. Ã‰ aqui que a "mÃ¡gica" acontece!
    /// 
    /// ## ğŸ”„ Fluxo de Dados Detalhado:
    /// 
    /// ```text
    /// Input IDs â†’ Token Embeddings â†’ + Position Embeddings
    ///                                        â†“
    ///                              [Transformer Block 1]
    ///                                        â†“
    ///                              [Transformer Block 2]
    ///                                        â†“
    ///                                      ...
    ///                                        â†“
    ///                              [Transformer Block N]
    ///                                        â†“
    ///                               Layer Normalization
    ///                                        â†“
    ///                              Linear Projection (lm_head)
    ///                                        â†“
    ///                               Logits (probabilidades)
    /// ```
    /// 
    /// ### ğŸ“Š DimensÃµes dos Tensores:
    /// - **Input**: `[batch_size, seq_len]` - IDs dos tokens
    /// - **Embeddings**: `[batch_size, seq_len, n_embd]` - RepresentaÃ§Ãµes vetoriais
    /// - **Logits**: `[batch_size, seq_len, vocab_size]` - Probabilidades por posiÃ§Ã£o
    /// 
    /// ### ğŸ¯ ParÃ¢metros:
    /// - `idx`: Tensor com IDs dos tokens de entrada
    /// - `targets`: Tokens alvo para cÃ¡lculo de loss (opcional, usado no treino)
    /// 
    /// ### ğŸ“¤ Retorno:
    /// - `logits`: Probabilidades para prÃ³ximo token em cada posiÃ§Ã£o
    /// - `loss`: FunÃ§Ã£o de perda (apenas se targets fornecidos)
    pub fn forward(&self, idx: &Tensor, targets: Option<&Tensor>) -> Result<(Tensor, Option<Tensor>)> {
        // ğŸ“ **EXTRAÃ‡ÃƒO DAS DIMENSÃ•ES**
        // batch_size: quantas sequÃªncias processamos simultaneamente
        // seq_len: comprimento de cada sequÃªncia (nÃºmero de tokens)
        let (batch_size, seq_len) = idx.dims2()?;
        
        // ğŸš¨ **VALIDAÃ‡ÃƒO DE CONTEXTO**
        // Garante que nÃ£o excedemos o tamanho mÃ¡ximo de contexto
        // Modelos tÃªm limite de memÃ³ria - nÃ£o podem "lembrar" infinitamente
        assert!(seq_len <= self.config.block_size, 
                "SequÃªncia muito longa! Max: {}, Atual: {}", 
                self.config.block_size, seq_len);
        
        // 1ï¸âƒ£ **TOKEN EMBEDDINGS: IDs â†’ VETORES**
        let tok_emb = self.token_embedding.forward(idx)?;
        
        // ğŸ” **DEBUG: Verificar token embeddings**
        let tok_emb_vec = tok_emb.flatten_all()?.to_vec1::<f32>()?;
        if tok_emb_vec.iter().any(|&x| x.is_nan()) {
            eprintln!("âš ï¸ DEBUG: Token embeddings contÃ©m NaN!");
            return Err("Token embeddings contÃ©m NaN".into());
        }
        
        // 2ï¸âƒ£ **POSITION EMBEDDINGS: ONDE ESTÃ CADA TOKEN?**
        let pos = Tensor::arange(0, seq_len as i64, &self.device)?;
        let pos_emb = self.position_embedding.forward(&pos)?;
        
        // ğŸ” **DEBUG: Verificar position embeddings**
        let pos_emb_vec = pos_emb.flatten_all()?.to_vec1::<f32>()?;
        if pos_emb_vec.iter().any(|&x| x.is_nan()) {
            eprintln!("âš ï¸ DEBUG: Position embeddings contÃ©m NaN!");
            return Err("Position embeddings contÃ©m NaN".into());
        }
        
        // 3ï¸âƒ£ **COMBINAÃ‡ÃƒO: TOKENS + POSIÃ‡Ã•ES = REPRESENTAÃ‡ÃƒO COMPLETA**
        // 
        // ## â• **Soma de Embeddings:**
        // Esta Ã© uma das operaÃ§Ãµes mais importantes do Transformer!
        // 
        // ```text
        // Token "gato" na posiÃ§Ã£o 2:
        // 
        // Token Embedding:    [0.1, -0.3, 0.8, 0.2, ...] (significado de "gato")
        //           +
        // Position Embedding: [0.0, 0.1, -0.2, 0.4, ...] (posiÃ§Ã£o 2)
        //           =
        // Combined:           [0.1, -0.2, 0.6, 0.6, ...] ("gato" na posiÃ§Ã£o 2)
        // ```
        // 
        // ### ğŸ¯ **Por que somar?**
        // - Preserva informaÃ§Ã£o de ambos (significado + posiÃ§Ã£o)
        // - Permite que atenÃ§Ã£o considere tanto semÃ¢ntica quanto sintaxe
        // - Mais eficiente que concatenaÃ§Ã£o (mantÃ©m dimensionalidade)
        let pos_emb = pos_emb.unsqueeze(0)?.expand(&[batch_size, seq_len, self.config.n_embd])?;
        let mut x = (tok_emb.clone() + pos_emb.clone())?;
        
        // ğŸ” **DEBUG: VERIFICAÃ‡ÃƒO DE INTEGRIDADE NUMÃ‰RICA**
        // Detecta problemas numÃ©ricos que podem quebrar o treinamento
        let x_vec = x.flatten_all()?.to_vec1::<f32>()?;
        if x_vec.iter().any(|&val| val.is_nan()) {
            eprintln!("âš ï¸ DEBUG: CombinaÃ§Ã£o de embeddings contÃ©m NaN!");
            eprintln!("   Token shape: {:?}", tok_emb.shape());
            eprintln!("   Position shape: {:?}", pos_emb.shape());
            return Err("CombinaÃ§Ã£o de embeddings contÃ©m NaN".into());
        }
        
        // 4ï¸âƒ£ **MÃSCARA CAUSAL: IMPEDINDO "VISÃƒO DO FUTURO"**
        // Cria mÃ¡scara triangular que impede tokens de "verem" tokens futuros
        // Crucial para treinamento autoregressivo - modelo sÃ³ pode usar contexto passado
        // Exemplo para seq_len=4:
        // [[0, -âˆ, -âˆ, -âˆ],
        //  [0,  0, -âˆ, -âˆ],
        //  [0,  0,  0, -âˆ],
        //  [0,  0,  0,  0]]
        let mask = self.create_causal_mask(seq_len)?;
        
        // 5ï¸âƒ£ **PROCESSAMENTO ATRAVÃ‰S DOS BLOCOS TRANSFORMER**
        // 
        // ## ğŸ—ï¸ **Stack de Processamento Sequencial:**
        // Cada bloco refina progressivamente as representaÃ§Ãµes:
        // 
        // ### ğŸ“Š **EvoluÃ§Ã£o das RepresentaÃ§Ãµes:**
        // ```text
        // Entrada:    ["O", "gato", "subiu", "no", "telhado"]
        //             â†“ (embeddings iniciais)
        // Bloco 1:    [sintaxe bÃ¡sica, bigramas]
        // Bloco 2:    [relaÃ§Ãµes gramaticais, trigramas]
        // Bloco 3:    [semÃ¢ntica, contexto local]
        // Bloco 4:    [pragmÃ¡tica, contexto global]
        //             â†“
        // SaÃ­da:      [representaÃ§Ãµes ricas e contextuais]
        // ```
        // 
        // ### ğŸ”„ **Processamento Residual:**
        // Cada bloco usa conexÃµes residuais (skip connections):
        // `output = block(input) + input`
        // Isso permite gradientes fluÃ­rem diretamente e facilita treinamento profundo
        for (i, block) in self.blocks.iter().enumerate() {
            x = block.forward(&x, Some(&mask))
                .map_err(|e| format!("Erro no bloco {}: {}", i, e))?;
            
            // ğŸ” **DEBUG: MONITORAMENTO POR CAMADA**
            // Detecta em qual camada problemas numÃ©ricos aparecem
            let x_vec = x.flatten_all()?.to_vec1::<f32>()?;
            if x_vec.iter().any(|&val| val.is_nan()) {
                eprintln!("âš ï¸ DEBUG: Bloco {} produziu NaN!", i);
                eprintln!("   DimensÃµes de entrada: {:?}", x.shape());
                eprintln!("   Bloco: {}/{}", i + 1, self.blocks.len());
                return Err(format!("Bloco {} produziu NaN", i).into());
            }
        }
        
        // 6ï¸âƒ£ **NORMALIZAÃ‡ÃƒO FINAL: PREPARAÃ‡ÃƒO PARA PREDIÃ‡ÃƒO**
        // 
        // ## âš–ï¸ **Layer Normalization Final:**
        // - Ãšltima oportunidade de estabilizar ativaÃ§Ãµes
        // - Garante que inputs para lm_head estejam bem condicionados
        // - Melhora estabilidade numÃ©rica da prediÃ§Ã£o
        // 
        // ### ğŸ“Š **FÃ³rmula da NormalizaÃ§Ã£o:**
        // ```
        // normalized = (x - mean) / sqrt(variance + epsilon)
        // output = normalized * gamma + beta
        // ```
        // Onde gamma e beta sÃ£o parÃ¢metros aprendÃ­veis
        let x = self.ln_final.forward(&x)?;
        
        // ğŸ” **DEBUG: Verificar layer norm final**
        let x_vec = x.flatten_all()?.to_vec1::<f32>()?;
        if x_vec.iter().any(|&val| val.is_nan()) {
            eprintln!("âš ï¸ DEBUG: Layer norm final produziu NaN!");
            return Err("Layer norm final produziu NaN".into());
        }
        
        // ğŸ¯ **PROJEÃ‡ÃƒO PARA VOCABULÃRIO: TRANSFORMANDO PENSAMENTOS EM PALAVRAS**
        // 
        // ## ğŸª **Language Modeling Head:**
        // Esta Ã© a camada que "traduz" as representaÃ§Ãµes internas
        // do modelo em probabilidades sobre palavras do vocabulÃ¡rio.
        // 
        // ### ğŸ”„ **TransformaÃ§Ã£o Dimensional:**
        // ```text
        // Input:  [batch_size, seq_len, n_embd]     (representaÃ§Ãµes ricas)
        //           â†“ (linear transformation)
        // Output: [batch_size, seq_len, vocab_size] (logits por token)
        // ```
        // 
        // ### ğŸ“Š **InterpretaÃ§Ã£o dos Logits:**
        // - Cada posiÃ§Ã£o na sequÃªncia produz vocab_size logits
        // - Logit alto = modelo "confia" nesse token
        // - Logit baixo = modelo "nÃ£o acredita" nesse token
        // - Softmax converte logits em probabilidades vÃ¡lidas
        let logits = self.lm_head.forward(&x)?;
        
        // ğŸ” **DEBUG: Verificar logits finais**
        let logits_vec = logits.flatten_all()?.to_vec1::<f32>()?;
        if logits_vec.iter().any(|&val| val.is_nan()) {
            eprintln!("âš ï¸ DEBUG: CabeÃ§a de linguagem (lm_head) produziu NaN!");
            return Err("CabeÃ§a de linguagem produziu NaN".into());
        }
        
        // 7ï¸âƒ£ **CÃLCULO DE LOSS (APENAS DURANTE TREINAMENTO)**
        // Se temos targets, calcula quÃ£o "erradas" foram nossas prediÃ§Ãµes
        // Cross-entropy loss: penaliza prediÃ§Ãµes incorretas
        // Usado pelo otimizador para ajustar pesos via backpropagation
        let loss = if let Some(targets) = targets {
            let loss = self.compute_loss(&logits, targets)
                .map_err(|e| format!("Erro no cÃ¡lculo de loss: {}", e))?;
            Some(loss)
        } else {
            None  // Modo inferÃªncia - sem loss
        };
        
        // ğŸ“¤ **RETORNO DOS RESULTADOS**
        // logits: probabilidades brutas (antes de softmax)
        // loss: funÃ§Ã£o de perda para otimizaÃ§Ã£o (opcional)
        Ok((logits, loss))
    }
    
    /// ğŸ”’ **CRIAÃ‡ÃƒO DA MÃSCARA CAUSAL**
    /// 
    /// Este mÃ©todo cria uma mÃ¡scara triangular que implementa a "causalidade"
    /// no modelo - garantindo que cada token sÃ³ pode "ver" tokens anteriores.
    /// 
    /// ## ğŸ¯ Por que precisamos disso?
    /// 
    /// Em modelos autoregressivos como GPT, queremos que o modelo aprenda a
    /// predizer o prÃ³ximo token baseado APENAS no contexto passado, nunca
    /// no futuro. Durante o treinamento, temos acesso a toda a sequÃªncia,
    /// mas precisamos "mascarar" o futuro para simular a geraÃ§Ã£o real.
    /// 
    /// ## ğŸ“Š Exemplo Visual (seq_len=4):
    /// 
    /// ```text
    /// PosiÃ§Ãµes:  0    1    2    3
    /// Token 0: [ 0,  -âˆ,  -âˆ,  -âˆ]  â† sÃ³ vÃª a si mesmo
    /// Token 1: [ 0,   0,  -âˆ,  -âˆ]  â† vÃª posiÃ§Ãµes 0,1
    /// Token 2: [ 0,   0,   0,  -âˆ]  â† vÃª posiÃ§Ãµes 0,1,2
    /// Token 3: [ 0,   0,   0,   0]  â† vÃª todas as posiÃ§Ãµes
    /// ```
    /// 
    /// ## âš¡ ImplementaÃ§Ã£o:
    /// - **0**: AtenÃ§Ã£o permitida (sem penalidade)
    /// - **-âˆ**: AtenÃ§Ã£o bloqueada (apÃ³s softmax â†’ probabilidade 0)
    /// 
    /// ### ğŸ”¢ DimensÃµes:
    /// - **Input**: `seq_len` (comprimento da sequÃªncia)
    /// - **Output**: `[seq_len, seq_len]` (matriz quadrada)
    fn create_causal_mask(&self, seq_len: usize) -> Result<Tensor> {
        // ğŸ—ï¸ **CONSTRUÃ‡ÃƒO DA MATRIZ TRIANGULAR**
        // Inicializa vetor 1D que representa matriz seq_len x seq_len
        // Usamos indexaÃ§Ã£o linear: posiÃ§Ã£o [i,j] = i * seq_len + j
        let mut mask_data = vec![0.0f32; seq_len * seq_len];
        
        // ğŸ”„ **PREENCHIMENTO DA MÃSCARA**
        // Para cada linha i (token atual)
        for i in 0..seq_len {
            // Para cada coluna j (token de referÃªncia)
            for j in 0..seq_len {
                // ğŸš« Se j > i, entÃ£o j estÃ¡ no "futuro" relativo a i
                if j > i {
                    // Bloqueia atenÃ§Ã£o para tokens futuros
                    // Usa valor finito grande negativo em vez de infinito para evitar NaN
                    mask_data[i * seq_len + j] = 1.0; // 1 indica posiÃ§Ã£o a ser mascarada
                }
                // Se j <= i, mantÃ©m 0 (atenÃ§Ã£o permitida)
            }
        }
        
        // ğŸ¯ **CRIAÃ‡ÃƒO DO TENSOR FINAL**
        // Converte vetor 1D em tensor 2D com dimensÃµes [seq_len, seq_len]
        Ok(Tensor::from_slice(&mask_data, (seq_len, seq_len), &self.device)
            .map_err(|e| Box::new(e) as Box<dyn std::error::Error + Send + Sync>)?)
    }
    
    /// ğŸ“Š **CÃLCULO DA FUNÃ‡ÃƒO DE PERDA (LOSS)**
    /// 
    /// Este mÃ©todo implementa a Cross-Entropy Loss, que mede quÃ£o "surpreso"
    /// o modelo fica com a resposta correta. Ã‰ a funÃ§Ã£o que o modelo tenta
    /// minimizar durante o treinamento.
    /// 
    /// ## ğŸ¯ O que Ã© Cross-Entropy Loss?
    /// 
    /// Imagine que o modelo Ã© um estudante fazendo uma prova de mÃºltipla escolha.
    /// Para cada pergunta (posiÃ§Ã£o na sequÃªncia), ele dÃ¡ uma "confianÃ§a" para
    /// cada resposta possÃ­vel (token do vocabulÃ¡rio). A cross-entropy mede:
    /// 
    /// - **Alta confianÃ§a na resposta certa** â†’ Loss baixo (bom!)
    /// - **Baixa confianÃ§a na resposta certa** â†’ Loss alto (ruim!)
    /// - **Alta confianÃ§a na resposta errada** â†’ Loss muito alto (muito ruim!)
    /// 
    /// ## ğŸ“ FÃ³rmula MatemÃ¡tica:
    /// 
    /// ```text
    /// Loss = -log(P(token_correto))
    /// 
    /// Onde P(token_correto) Ã© a probabilidade que o modelo
    /// atribuiu ao token correto naquela posiÃ§Ã£o.
    /// ```
    /// 
    /// ## ğŸ”¢ DimensÃµes dos Tensores:
    /// - **logits**: `[batch_size, seq_len, vocab_size]` - "notas" brutas
    /// - **targets**: `[batch_size, seq_len]` - respostas corretas
    /// - **loss**: `[]` - escalar (nÃºmero Ãºnico)
    /// 
    /// ## âš¡ Processo de CÃ¡lculo:
    /// 1. **Reshape**: Achata tensores para facilitar cÃ¡lculo
    /// 2. **Softmax**: Converte logits em probabilidades (implÃ­cito na cross-entropy)
    /// 3. **Loss**: Calcula -log(probabilidade_correta) para cada posiÃ§Ã£o
    /// 4. **MÃ©dia**: Retorna loss mÃ©dio sobre todas as posiÃ§Ãµes
    fn compute_loss(&self, logits: &Tensor, targets: &Tensor) -> Result<Tensor> {
        // ğŸ“ **EXTRAÃ‡ÃƒO DAS DIMENSÃ•ES**
        let (batch_size, seq_len, vocab_size) = logits.dims3()?;
        
        // ğŸ” **DEBUG: Verificar se logits contÃ©m valores invÃ¡lidos**
        let logits_vec = logits.flatten_all()?.to_vec1::<f32>()?;
        let has_nan = logits_vec.iter().any(|&x| x.is_nan());
        let has_inf = logits_vec.iter().any(|&x| x.is_infinite());
        
        if has_nan {
            eprintln!("âš ï¸ DEBUG: Logits contÃ©m NaN!");
            eprintln!("   DimensÃµes: [{}, {}, {}]", batch_size, seq_len, vocab_size);
            return Err("Logits contÃ©m valores NaN".into());
        }
        
        if has_inf {
            eprintln!("âš ï¸ DEBUG: Logits contÃ©m valores infinitos!");
            eprintln!("   DimensÃµes: [{}, {}, {}]", batch_size, seq_len, vocab_size);
            return Err("Logits contÃ©m valores infinitos".into());
        }
        
        // ğŸ”„ **RESHAPE PARA CÃLCULO EFICIENTE**
        let logits_flat = logits.reshape((batch_size * seq_len, vocab_size))?;
        let targets_flat = targets.reshape((batch_size * seq_len,))?;
        
        // ğŸ” **DEBUG: Verificar targets**
        let targets_vec = targets_flat.to_vec1::<u32>()?;
        let max_target = targets_vec.iter().max().unwrap_or(&0);
        if *max_target >= vocab_size as u32 {
            eprintln!("âš ï¸ DEBUG: Target invÃ¡lido! Max target: {}, vocab_size: {}", max_target, vocab_size);
            return Err("Target fora do range do vocabulÃ¡rio".into());
        }
        
        // ğŸ¯ **CÃLCULO DA CROSS-ENTROPY LOSS**
        let loss = candle_nn::loss::cross_entropy(&logits_flat, &targets_flat)
            .map_err(|e| Box::new(e) as Box<dyn std::error::Error + Send + Sync>)?;
        
        // ğŸ” **DEBUG: Verificar se o loss Ã© vÃ¡lido**
        let loss_value = loss.to_scalar::<f32>()?;
        if loss_value.is_nan() {
            eprintln!("âš ï¸ DEBUG: Loss calculado Ã© NaN!");
            eprintln!("   Logits stats: min={:.6}, max={:.6}, mean={:.6}", 
                     logits_vec.iter().fold(f32::INFINITY, |a, &b| a.min(b)),
                     logits_vec.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b)),
                     logits_vec.iter().sum::<f32>() / logits_vec.len() as f32);
            return Err("Loss calculado Ã© NaN".into());
        }
        
        Ok(loss)
    }
    
    /// ğŸ­ **GERAÃ‡ÃƒO AUTOREGRESSIVA DE TEXTO**
    /// 
    /// Este Ã© o mÃ©todo que faz a "mÃ¡gica" acontecer! Ele implementa a geraÃ§Ã£o
    /// autoregressiva, onde o modelo "conversa consigo mesmo" para criar texto.
    /// 
    /// ## ğŸ”„ Como Funciona a GeraÃ§Ã£o Autoregressiva?
    /// 
    /// Imagine que vocÃª estÃ¡ escrevendo uma histÃ³ria, mas sÃ³ pode ver uma palavra
    /// por vez. A cada palavra, vocÃª precisa decidir qual serÃ¡ a prÃ³xima baseado
    /// apenas no que jÃ¡ escreveu:
    /// 
    /// ```text
    /// Prompt: "Era uma vez"
    /// 
    /// Passo 1: "Era uma vez" â†’ modelo prediz â†’ "uma"
    /// Passo 2: "Era uma vez uma" â†’ modelo prediz â†’ "princesa"
    /// Passo 3: "Era uma vez uma princesa" â†’ modelo prediz â†’ "que"
    /// ...
    /// ```
    /// 
    /// ## ğŸŒ¡ï¸ Temperatura: Controlando a Criatividade
    /// 
    /// - **Temperatura = 0**: Sempre escolhe a palavra mais provÃ¡vel (determinÃ­stico)
    /// - **Temperatura baixa (0.1-0.7)**: Mais conservador, texto coerente
    /// - **Temperatura alta (0.8-1.5)**: Mais criativo, pode ser incoerente
    /// - **Temperatura > 2**: Muito aleatÃ³rio, geralmente incoerente
    /// 
    /// ## âš¡ Processo Detalhado:
    /// 1. **TokenizaÃ§Ã£o**: Converte prompt em nÃºmeros
    /// 2. **Loop de GeraÃ§Ã£o**: Para cada novo token:
    ///    - Limita contexto ao tamanho mÃ¡ximo
    ///    - Executa forward pass
    ///    - Aplica temperatura
    ///    - Amostra prÃ³ximo token
    ///    - Adiciona ao contexto
    /// 3. **DecodificaÃ§Ã£o**: Converte nÃºmeros de volta em texto
    /// 
    /// ### ğŸ¯ ParÃ¢metros:
    /// - `prompt`: Texto inicial para comeÃ§ar a geraÃ§Ã£o
    /// - `max_tokens`: MÃ¡ximo de tokens novos a gerar
    /// - `tokenizer`: Conversor texto â†” nÃºmeros
    /// - `temperature`: Controle de criatividade (0.0 = determinÃ­stico)
    pub fn generate(
        &self,
        prompt: &str,
        max_tokens: usize,
        tokenizer: &BPETokenizer,
        temperature: f32,
    ) -> Result<String> {
        // ğŸ”¤ **TOKENIZAÃ‡ÃƒO INICIAL**
        // Converte o prompt de texto para sequÃªncia de IDs de tokens
        // Exemplo: "OlÃ¡ mundo" â†’ [156, 2134] (nÃºmeros dependem do vocabulÃ¡rio)
        let mut tokens = tokenizer.encode(prompt)
            .map_err(|e| format!("Erro na tokenizaÃ§Ã£o do prompt: {}", e))?;
        let mut generated_tokens = Vec::new();
        
        // ğŸ”„ **LOOP DE GERAÃ‡ÃƒO AUTOREGRESSIVA**
        // Gera um token por vez, sempre usando o contexto completo
        for step in 0..max_tokens {
            // ğŸ“ **LIMITAÃ‡ÃƒO DO CONTEXTO**
            // Modelos tÃªm limite de memÃ³ria - se o contexto ficar muito longo,
            // mantemos apenas os Ãºltimos N tokens (sliding window)
            let context = if tokens.len() > self.config.block_size {
                &tokens[tokens.len() - self.config.block_size..]
            } else {
                &tokens[..]
            };
            
            // ğŸ”¢ **CONVERSÃƒO PARA TENSOR**
            // Transforma vetor de tokens em tensor que o modelo entende
            // DimensÃµes: [1, context_len] (batch_size=1, seq_len=context_len)
            let context_i64: Vec<i64> = context.iter().map(|&x| x as i64).collect();
            let idx = Tensor::from_slice(&context_i64, &[1, context.len()], &self.device)
                .map_err(|e| format!("Erro ao criar tensor no passo {}: {}", step, e))?;
            
            // ğŸš€ **FORWARD PASS: PREDIÃ‡ÃƒO**
            // Executa o modelo para obter probabilidades do prÃ³ximo token
            // targets=None indica modo inferÃªncia (nÃ£o treinamento)
            let (logits, _) = self.forward(&idx, None)
                .map_err(|e| format!("Erro no forward pass no passo {}: {}", step, e))?;
            
            // ğŸ¯ **EXTRAÃ‡ÃƒO DOS LOGITS DA ÃšLTIMA POSIÃ‡ÃƒO**
            // logits tem dimensÃµes [1, seq_len, vocab_size]
            // Queremos apenas a Ãºltima posiÃ§Ã£o: [vocab_size]
            // Esta Ã© a "opiniÃ£o" do modelo sobre qual token vem a seguir
            let logits = logits.i((0, context.len() - 1, ..))
                .map_err(|e| format!("Erro ao extrair logits no passo {}: {}", step, e))?;
            
            // ğŸŒ¡ï¸ **APLICAÃ‡ÃƒO DA TEMPERATURA**
            // Temperatura controla o quÃ£o "ousado" o modelo serÃ¡:
            // - Divide logits pela temperatura
            // - Temperatura baixa â†’ distribuiÃ§Ã£o mais "afiada" (conservador)
            // - Temperatura alta â†’ distribuiÃ§Ã£o mais "suave" (criativo)
            let temperature_tensor = Tensor::new(&[temperature], &self.device)
                .map_err(|e| format!("Erro ao criar tensor de temperatura no passo {}: {}", step, e))?;
            let logits = logits.broadcast_div(&temperature_tensor)
                .map_err(|e| format!("Erro ao aplicar temperatura no passo {}: {}", step, e))?;
            
            // ğŸ“Š **CONVERSÃƒO PARA PROBABILIDADES**
            // Softmax transforma "notas" brutas em probabilidades vÃ¡lidas
            // Soma de todas as probabilidades = 1.0
            let probs = candle_nn::ops::softmax(&logits, 0)
                .map_err(|e| format!("Erro no softmax no passo {}: {}", step, e))?;
            
            // ğŸ² **AMOSTRAGEM DO PRÃ“XIMO TOKEN**
            // Escolhe um token baseado nas probabilidades calculadas
            // NÃ£o sempre o mais provÃ¡vel - introduz variabilidade
            let next_token = self.sample_from_probs(&probs)
                .map_err(|e| format!("Erro na amostragem no passo {}: {}", step, e))?;
            
            // â• **ADIÃ‡ÃƒO AO CONTEXTO**
            // O token gerado vira parte do contexto para a prÃ³xima iteraÃ§Ã£o
            // Este Ã© o "autoregressivo" - cada saÃ­da alimenta a prÃ³xima entrada
            tokens.push(next_token);
            generated_tokens.push(next_token);
            
            // ğŸ›‘ **VERIFICAÃ‡ÃƒO DE PARADA ANTECIPADA**
            // Alguns tokenizers tÃªm tokens especiais de fim de sequÃªncia
            // Se encontrarmos um, podemos parar a geraÃ§Ã£o
            if tokenizer.is_eos_token(next_token) {
                break;  // Para a geraÃ§Ã£o se encontrar token de fim
            }
        }
        
        // ğŸ”¤ **DECODIFICAÃ‡ÃƒO FINAL**
        // Converte a sequÃªncia completa de tokens de volta para texto legÃ­vel
        // Exemplo: [156, 2134, 891] â†’ "OlÃ¡ mundo!"
        Ok(tokenizer.decode(&generated_tokens)
            .map_err(|e| format!("Erro na decodificaÃ§Ã£o final: {}", e))?)
    }
    
    /// ğŸ² **AMOSTRAGEM PROBABILÃSTICA**
    /// 
    /// Este mÃ©todo implementa amostragem baseada em probabilidades,
    /// escolhendo tokens de forma nÃ£o-determinÃ­stica para gerar texto variado.
    /// 
    /// ## ğŸ¯ Como Funciona?
    /// 
    /// Imagine uma roleta onde cada fatia representa um token possÃ­vel,
    /// e o tamanho da fatia Ã© proporcional Ã  sua probabilidade:
    /// 
    /// ```text
    /// Tokens:  ["o", "a", "um", "uma", "..."]
    /// Probs:   [0.4, 0.3, 0.2,  0.1,   ...]
    /// 
    /// Roleta:  |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|â–ˆâ–ˆâ–ˆâ–ˆ|â–ˆâ–ˆ|...
    ///          0      0.4    0.7  0.9 1.0
    /// 
    /// NÃºmero aleatÃ³rio: 0.65 â†’ cai na fatia "a"
    /// ```
    /// 
    /// ## âš¡ Algoritmo:
    /// 1. **Gera nÃºmero aleatÃ³rio** entre 0 e 1
    /// 2. **Percorre probabilidades** acumulando soma
    /// 3. **Para quando soma â‰¥ nÃºmero aleatÃ³rio**
    /// 4. **Retorna Ã­ndice** do token escolhido
    /// 
    /// ### ğŸ¨ Vantagens da Amostragem:
    /// - **Variabilidade**: Mesmo prompt gera textos diferentes
    /// - **Criatividade**: Permite escolhas menos Ã³bvias
    /// - **Naturalidade**: Evita repetiÃ§Ãµes mecÃ¢nicas
    fn sample_from_probs(&self, probs: &Tensor) -> Result<usize> {
        // ğŸ“Š **CONVERSÃƒO PARA VETOR**
        // Extrai probabilidades do tensor para formato mais fÃ¡cil de trabalhar
        let probs: Vec<f32> = probs.to_vec1()
            .map_err(|e| format!("Erro ao converter probabilidades: {}", e))?;
        
        // ğŸ² **GERAÃ‡ÃƒO DE NÃšMERO ALEATÃ“RIO**
        // Cria gerador de nÃºmeros aleatÃ³rios thread-local
        // Gera nÃºmero uniforme entre 0.0 e 1.0
        use rand::prelude::*;
        let mut rng = thread_rng();
        let uniform: f32 = rng.gen();  // NÃºmero aleatÃ³rio [0.0, 1.0)
        
        // ğŸ¯ **AMOSTRAGEM POR SOMA CUMULATIVA**
        // Percorre probabilidades acumulando soma atÃ© ultrapassar nÃºmero aleatÃ³rio
        let mut cumsum = 0.0;
        for (idx, &prob) in probs.iter().enumerate() {
            cumsum += prob;  // Acumula probabilidade
            
            // ğŸª Se nÃºmero aleatÃ³rio "cai" nesta fatia, escolhe este token
            if uniform <= cumsum {
                return Ok(idx);
            }
        }
        
        // ğŸ›¡ï¸ **FALLBACK DE SEGURANÃ‡A**
        // Em caso de erro numÃ©rico (probabilidades nÃ£o somam 1.0),
        // retorna o Ãºltimo token como fallback
        Ok(probs.len() - 1)
    }
    
    /// ğŸ”¢ **CONTADOR DE PARÃ‚METROS TREINÃVEIS**
    /// 
    /// Este mÃ©todo calcula o nÃºmero total de parÃ¢metros (pesos) que o modelo
    /// precisa aprender durante o treinamento. Ã‰ como contar quantos "botÃµes"
    /// o modelo tem para ajustar!
    /// 
    /// ## ğŸ“Š Breakdown dos ParÃ¢metros:
    /// 
    /// ### ğŸ“š **Embeddings**:
    /// - **Token Embeddings**: `vocab_size Ã— n_embd`
    /// - **Position Embeddings**: `block_size Ã— n_embd`
    /// 
    /// ### ğŸ§  **Cada Bloco Transformer**:
    /// - **Attention**: `4 Ã— n_embdÂ²` (Q, K, V, Output projections)
    /// - **Feed-Forward**: `8 Ã— n_embdÂ²` (expansÃ£o 4x: up + down)
    /// - **Layer Norms**: `4 Ã— n_embd` (2 layer norms Ã— 2 parÃ¢metros cada)
    /// 
    /// ### ğŸ¯ **Camada Final**:
    /// - **Final LayerNorm**: `n_embd`
    /// - **Language Model Head**: `vocab_size Ã— n_embd`
    /// 
    /// ## ğŸ’¡ Por que isso importa?
    /// - **MemÃ³ria**: Mais parÃ¢metros = mais RAM/VRAM necessÃ¡ria
    /// - **Velocidade**: Mais parÃ¢metros = computaÃ§Ã£o mais lenta
    /// - **Capacidade**: Mais parÃ¢metros = potencialmente mais "inteligente"
    /// - **Overfitting**: Muitos parÃ¢metros podem decorar em vez de aprender
    pub fn num_parameters(&self) -> usize {
        let mut total = 0;
        
        // ğŸ“š **EMBEDDINGS**
        // Token embeddings: cada token do vocabulÃ¡rio tem um vetor de n_embd dimensÃµes
        total += self.config.vocab_size * self.config.n_embd;
        
        // Position embeddings: cada posiÃ§Ã£o atÃ© block_size tem um vetor de n_embd dimensÃµes
        total += self.config.block_size * self.config.n_embd;
        
        // ğŸ§  **BLOCOS TRANSFORMER**
        // Calcula parÃ¢metros por bloco e multiplica pelo nÃºmero de blocos
        let per_block = 
            // ğŸ‘ï¸ Multi-Head Attention: 4 matrizes (Q, K, V, Output)
            4 * self.config.n_embd * self.config.n_embd +
            
            // âš¡ Feed-Forward Network: expansÃ£o 4x (up projection + down projection)
            // up: n_embd â†’ 4*n_embd, down: 4*n_embd â†’ n_embd
            8 * self.config.n_embd * self.config.n_embd +
            
            // âš–ï¸ Layer Normalizations: 2 layer norms Ã— 2 parÃ¢metros (scale + shift)
            4 * self.config.n_embd;
        
        total += per_block * self.config.n_layer;
        
        // ğŸ¯ **CAMADAS FINAIS**
        // Final layer normalization: scale + shift parameters
        total += 2 * self.config.n_embd;
        
        // Language modeling head: projeta embeddings para vocabulÃ¡rio
        total += self.config.vocab_size * self.config.n_embd;
        
        total
    }
    
    /// ğŸ“š **TAMANHO DO VOCABULÃRIO**
    /// 
    /// Retorna quantas palavras/tokens diferentes o modelo conhece.
    /// Ã‰ como o "dicionÃ¡rio" do modelo - determina quais palavras
    /// ele pode entender e gerar.
    /// 
    /// ## ğŸ’¡ Exemplos TÃ­picos:
    /// - **GPT-2**: ~50.000 tokens
    /// - **GPT-3**: ~50.257 tokens  
    /// - **Nosso modelo**: configurÃ¡vel (ex: 1.000-10.000)
    /// 
    /// ### ğŸ¯ Trade-offs do Tamanho:
    /// - **Maior vocabulÃ¡rio**: Mais expressivo, mas mais parÃ¢metros
    /// - **Menor vocabulÃ¡rio**: Mais eficiente, mas pode ser limitado
    pub fn vocab_size(&self) -> usize {
        self.config.vocab_size
    }
    
    /// ğŸ“ **TAMANHO DO CONTEXTO (BLOCK SIZE)**
    /// 
    /// Retorna quantos tokens o modelo consegue "lembrar" de uma vez.
    /// Ã‰ a "memÃ³ria de trabalho" do modelo - determina quanto contexto
    /// anterior ele pode considerar ao gerar o prÃ³ximo token.
    /// 
    /// ## ğŸ§  Analogia:
    /// Imagine que vocÃª estÃ¡ lendo um livro, mas sÃ³ consegue lembrar
    /// das Ãºltimas N palavras. O block_size Ã© esse N.
    /// 
    /// ## ğŸ’¡ Exemplos TÃ­picos:
    /// - **GPT-2**: 1.024 tokens (~750 palavras)
    /// - **GPT-3**: 2.048 tokens (~1.500 palavras)
    /// - **GPT-4**: 8.192+ tokens (~6.000+ palavras)
    /// - **Nosso modelo**: configurÃ¡vel (ex: 128-512)
    /// 
    /// ### âš–ï¸ Trade-offs:
    /// - **Contexto maior**: Melhor compreensÃ£o, mas mais lento e usa mais memÃ³ria
    /// - **Contexto menor**: Mais rÃ¡pido e eficiente, mas pode "esquecer" informaÃ§Ãµes importantes
    pub fn block_size(&self) -> usize {
        self.config.block_size
    }
    
    /// ğŸ’¾ **ACESSO AO VARMAP PARA SALVAMENTO**
    /// 
    /// Retorna uma referÃªncia ao VarMap que contÃ©m todos os pesos treinÃ¡veis
    /// do modelo. Este mÃ©todo Ã© essencial para implementar salvamento e
    /// carregamento de checkpoints.
    /// 
    /// ## ğŸ—‚ï¸ **O que Ã© o VarMap:**
    /// - **RepositÃ³rio**: ContÃ©m todos os tensores nomeados do modelo
    /// - **SerializaÃ§Ã£o**: Permite salvar em formato SafeTensors
    /// - **Checkpoint**: Base para salvar/carregar estado do modelo
    /// 
    /// ## ğŸ”§ **Uso TÃ­pico:**
    /// ```rust
    /// let varmap = model.varmap();
    /// varmap.save("model.safetensors")?;
    /// ```
    pub fn varmap(&self) -> &VarMap {
        &self.varmap
    }
}